# utils/metrics.py
import re
import string
from typing import List, Optional, Tuple
import logging

def normalize_answer(s: str) -> str:
    """ë‹µë³€ ì •ê·œí™” - ë” ê²¬ê³ í•˜ê²Œ"""
    if not s:
        return ""
    
    s = str(s).lower().strip()
    # ê´€ì‚¬ ì œê±°
    s = re.sub(r'\b(a|an|the)\b', ' ', s)
    # íŠ¹ìˆ˜ë¬¸ì ì œê±° (ì•ŒíŒŒë²³, ìˆ«ì, ê³µë°±ë§Œ ìœ ì§€)
    s = re.sub(r'[^\w\s]', '', s)
    # ì—°ì† ê³µë°± ì •ë¦¬
    return ' '.join(s.split())

def extract_final_answer(text: str, dataset_type: Optional[str] = None) -> str:
    """T5 ë‹µë³€ì—ì„œ ìµœì¢… ë‹µ ì¶”ì¶œ - ìƒˆ ë°ì´í„°ì…‹ ì§€ì›"""
    if not text:
        return ""
    
    text = str(text).strip()
    if not text:
        return ""
    
    # ì²« ë²ˆì§¸ ë‹¨ì–´ ì¶”ì¶œ
    words = text.split()
    first_word = words[0] if words else ""
    
    if dataset_type == "strategyqa":
        # Yes/No ì§ˆë¬¸ ì²˜ë¦¬
        first_lower = first_word.lower()
        
        # ëª…í™•í•œ Yes íŒ¨í„´
        if any(pattern in first_lower for pattern in ['yes', 'true', 'correct', 'right']):
            return "Yes"
        # ëª…í™•í•œ No íŒ¨í„´  
        elif any(pattern in first_lower for pattern in ['no', 'false', 'incorrect', 'wrong']):
            return "No"
        # ìˆ«ìë¡œ í‘œí˜„ëœ ê²½ìš°
        elif first_lower in ['1', 'one']:
            return "Yes"
        elif first_lower in ['0', 'zero']:
            return "No"
        
        # ê¸°ë³¸ê°’: ì²« ê¸€ì ê¸°ì¤€
        return "Yes" if first_lower.startswith('y') else "No"
    
    elif dataset_type == "gsm8k":
        # ìˆ˜í•™ ë¬¸ì œ - ìˆ«ì ì¶”ì¶œ
        # ì²« ë²ˆì§¸ ë‹¨ì–´ê°€ ìˆ«ìì¸ì§€ í™•ì¸
        first_numbers = re.findall(r'^-?\d+(?:\.\d+)?$', first_word)
        if first_numbers:
            try:
                # ì •ìˆ˜ë¡œ ë³€í™˜ ê°€ëŠ¥í•˜ë©´ ì •ìˆ˜ë¡œ, ì•„ë‹ˆë©´ ì‹¤ìˆ˜ë¡œ
                num = float(first_numbers[0])
                if num.is_integer():
                    return str(int(num))
                else:
                    return str(num)
            except ValueError:
                pass
        
        # ì²« ë²ˆì§¸ ë‹¨ì–´ê°€ ìˆ«ìê°€ ì•„ë‹ˆë©´ "error" ë°˜í™˜ (í‹€ë¦° ê²ƒìœ¼ë¡œ ê°„ì£¼)
        return "error"
    
    elif dataset_type == "logiqa":
        # ë‹¤ì¤‘ ì„ íƒ ë¬¸ì œ - A, B, C, D
        first_upper = first_word.upper()
        
        # ì§ì ‘ì ì¸ ì„ íƒì§€
        if first_upper in ['A', 'B', 'C', 'D']:
            return first_upper
        
        # ê´„í˜¸ë‚˜ ê¸°íƒ€ ë¬¸ì ì œê±° í›„ í™•ì¸
        clean = re.sub(r'[^\w]', '', first_upper)
        if clean in ['A', 'B', 'C', 'D']:
            return clean
        
        # ë‹¨ì–´ë¡œ ëœ ì„ íƒì§€ ì²˜ë¦¬
        choice_map = {
            'FIRST': 'A', 'ONE': 'A', '1': 'A',
            'SECOND': 'B', 'TWO': 'B', '2': 'B', 
            'THIRD': 'C', 'THREE': 'C', '3': 'C',
            'FOURTH': 'D', 'FOUR': 'D', '4': 'D'
        }
        return choice_map.get(first_upper, "A")  # ê¸°ë³¸ê°’ A
    
    elif dataset_type == "multinli":
        # ìì—°ì–´ ì¶”ë¡  - entailment, neutral, contradiction
        first_lower = first_word.lower()
        
        # ëª…í™•í•œ ë§¤ì¹­
        if first_lower.startswith('ent'):
            return "entailment"
        elif first_lower.startswith('neu'):
            return "neutral"  
        elif first_lower.startswith('con'):
            return "contradiction"
        
        # ë™ì˜ì–´ ì²˜ë¦¬
        entailment_words = ['yes', 'true', 'follows', 'implies', 'supports']
        neutral_words = ['maybe', 'unclear', 'unknown', 'possible']
        contradiction_words = ['no', 'false', 'contradicts', 'opposes']
        
        if any(word in first_lower for word in entailment_words):
            return "entailment"
        elif any(word in first_lower for word in neutral_words):
            return "neutral"
        elif any(word in first_lower for word in contradiction_words):
            return "contradiction"
        
        return "neutral"  # ê¸°ë³¸ê°’
    
    elif dataset_type == "eli5":
        # ELI5 - ê¸´ ì„¤ëª… ìƒì„± (ì²« ë¬¸ì¥ ì¶”ì¶œ)
        # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• í•˜ì—¬ ì²« ë¬¸ì¥ ë°˜í™˜
        sentences = re.split(r'[.!?]+', text)
        first_sentence = sentences[0].strip() if sentences else text.strip()
        
        # ë„ˆë¬´ ì§§ìœ¼ë©´ ì „ì²´ í…ìŠ¤íŠ¸ì˜ ì²˜ìŒ ì¼ë¶€ ë°˜í™˜
        if len(first_sentence.split()) < 3:
            words = text.split()[:20]  # ì²˜ìŒ 20 ë‹¨ì–´
            return ' '.join(words)
        
        return first_sentence
    
    elif dataset_type == "commongen":
        # CommonGen - ê°œë… ì—°ê²° ë¬¸ì¥ (ì „ì²´ í…ìŠ¤íŠ¸ ë°˜í™˜, ë‹¨ ì ì ˆíˆ ì •ë¦¬)
        # ë¶ˆí•„ìš”í•œ ì•ë’¤ ê³µë°± ì œê±° ë° ì—°ì† ê³µë°± ì •ë¦¬
        cleaned = ' '.join(text.split())
        
        # ë„ˆë¬´ ê¸¸ë©´ ì²« ë¬¸ì¥ë§Œ
        sentences = re.split(r'[.!?]+', cleaned)
        if sentences and len(sentences[0].split()) <= 30:
            return sentences[0].strip()
        
        # ì²« ë¬¸ì¥ì´ ë„ˆë¬´ ê¸¸ê±°ë‚˜ ì—†ìœ¼ë©´ ì²˜ìŒ 30 ë‹¨ì–´ë§Œ
        words = cleaned.split()[:30]
        result = ' '.join(words)
        
        # ë¬¸ì¥ì´ ì™„ì„±ë˜ì§€ ì•Šì•˜ìœ¼ë©´ ë§ˆì¹¨í‘œ ì¶”ê°€
        if result and not result.endswith(('.', '!', '?')):
            result += '.'
        
        return result
    
    # ì•Œ ìˆ˜ ì—†ëŠ” ë°ì´í„°ì…‹ íƒ€ì…ì¸ ê²½ìš° ì²« ë‹¨ì–´ ë°˜í™˜
    return first_word

def exact_match_score(prediction: str, ground_truth: str, dataset_type: Optional[str] = None) -> bool:
    """ì •í™•í•œ ì¼ì¹˜ ì—¬ë¶€ - ìƒˆ ë°ì´í„°ì…‹ ì§€ì›"""
    try:
        pred = extract_final_answer(prediction, dataset_type)
        true = extract_final_answer(ground_truth, dataset_type)
        
        pred_norm = normalize_answer(pred)
        true_norm = normalize_answer(true)
        
        # ìˆ˜í•™ ë¬¸ì œëŠ” ìˆ«ì ë¹„êµ
        if dataset_type == "gsm8k":
            try:
                pred_num = float(pred_norm) if pred_norm else 0.0
                true_num = float(true_norm) if true_norm else 0.0
                return abs(pred_num - true_num) < 1e-6  # ë¶€ë™ì†Œìˆ˜ì  ì˜¤ì°¨ ê³ ë ¤
            except ValueError:
                # ìˆ«ì ë³€í™˜ ì‹¤íŒ¨ ì‹œ ë¬¸ìì—´ ë¹„êµ
                pass
        
        # ELI5ì™€ CommonGenì€ ì˜ë¯¸ì  ìœ ì‚¬ì„±ì„ ê³ ë ¤í•œ í‰ê°€
        elif dataset_type in ["eli5", "commongen"]:
            return calculate_semantic_similarity(pred_norm, true_norm)
        
        # ê¸°ë³¸ì ìœ¼ë¡œ ì •ê·œí™”ëœ ë¬¸ìì—´ ë¹„êµ
        return pred_norm == true_norm
        
    except Exception as e:
        # ì˜ˆì™¸ ë°œìƒ ì‹œ False ë°˜í™˜ (ë¡œê¹…ì€ ì„ íƒì )
        logging.debug(f"Exact match error: {e}")
        return False

def calculate_semantic_similarity(pred: str, target: str) -> bool:
    """ELI5, CommonGenìš© ì˜ë¯¸ì  ìœ ì‚¬ì„± ê³„ì‚°"""
    if not pred or not target:
        return False
    
    # í† í° ê¸°ë°˜ ìœ ì‚¬ì„± (Jaccard similarity)
    pred_tokens = set(pred.split())
    target_tokens = set(target.split())
    
    if not pred_tokens or not target_tokens:
        return False
    
    intersection = len(pred_tokens & target_tokens)
    union = len(pred_tokens | target_tokens)
    
    jaccard_similarity = intersection / union if union > 0 else 0.0
    
    # 30% ì´ìƒ ìœ ì‚¬í•˜ë©´ ì •ë‹µìœ¼ë¡œ ê°„ì£¼ (ìƒì„± íƒœìŠ¤í¬ëŠ” ê´€ëŒ€í•˜ê²Œ)
    return jaccard_similarity >= 0.3

def calculate_accuracy(predictions: List[str], targets: List[str], 
                      dataset_type: Optional[str] = None, 
                      verbose: bool = True) -> float:
    """ì •í™•ë„ ê³„ì‚° - ìƒˆ ë°ì´í„°ì…‹ ì§€ì›"""
    if not predictions or not targets:
        return 0.0
    
    if len(predictions) != len(targets):
        if verbose:
            print(f"âš ï¸ Length mismatch: predictions={len(predictions)}, targets={len(targets)}")
        # ê¸¸ì´ê°€ ë‹¤ë¥´ë©´ ì§§ì€ ìª½ì— ë§ì¶¤
        min_len = min(len(predictions), len(targets))
        predictions = predictions[:min_len]
        targets = targets[:min_len]
    
    # ë°ì´í„°ì…‹ íƒ€ì… ìë™ ê°ì§€
    if dataset_type is None:
        dataset_type = detect_dataset_type(targets)
        if verbose and dataset_type:
            print(f"ğŸ” Dataset type auto-detected: {dataset_type}")
    
    # ì •í™•ë„ ê³„ì‚°
    correct_count = 0
    total_count = len(predictions)
    
    for i, (pred, target) in enumerate(zip(predictions, targets)):
        if exact_match_score(pred, target, dataset_type):
            correct_count += 1
    
    accuracy = correct_count / total_count if total_count > 0 else 0.0
    
    if verbose:
        print(f"ğŸ¯ Accuracy: {correct_count}/{total_count} = {accuracy:.4f} (type: {dataset_type or 'unknown'})")
        
        # ìƒˆ ë°ì´í„°ì…‹ì˜ ê²½ìš° í‰ê°€ ë°©ì‹ ì„¤ëª…
        if dataset_type in ["eli5", "commongen"]:
            print(f"   ğŸ“ Note: Using semantic similarity (30%+ token overlap) for {dataset_type}")
    
    return accuracy

def detect_dataset_type(targets: List[str]) -> Optional[str]:
    """íƒ€ê²Ÿìœ¼ë¡œë¶€í„° ë°ì´í„°ì…‹ íƒ€ì… ìë™ ê°ì§€ - ìƒˆ ë°ì´í„°ì…‹ ì§€ì›"""
    if not targets:
        return None
    
    # ìƒ˜í”Œ í¬ê¸° ì¡°ì • (ìµœëŒ€ 20ê°œ, ìµœì†Œ 5ê°œ)
    sample_size = min(max(len(targets) // 10, 5), 20)
    sample = targets[:sample_size]
    
    # ê° íƒ€ì…ë³„ íŒ¨í„´ ì¹´ìš´íŠ¸
    counters = {
        'strategyqa': 0,
        'gsm8k': 0, 
        'logiqa': 0,
        'multinli': 0,
        'eli5': 0,
        'commongen': 0
    }
    
    for target in sample:
        target_str = str(target).lower().strip()
        word_count = len(target_str.split())
        
        # StrategyQA: Yes/No ë‹µë³€
        if target_str in ['yes', 'no', 'true', 'false', '1', '0']:
            counters['strategyqa'] += 1
        
        # GSM8K: ìˆ«ì ë‹µë³€
        elif re.match(r'^-?\d+(\.\d+)?$', target_str):
            counters['gsm8k'] += 1
            
        # LogiQA: A, B, C, D ì„ íƒì§€
        elif target_str.upper() in ['A', 'B', 'C', 'D']:
            counters['logiqa'] += 1
            
        # MultiNLI: entailment, neutral, contradiction
        elif target_str in ['entailment', 'neutral', 'contradiction']:
            counters['multinli'] += 1
        
        # ELI5: ê¸´ ì„¤ëª… (ë³´í†µ 50+ ë‹¨ì–´)
        elif word_count >= 20 and any(word in target_str for word in ['because', 'when', 'how', 'why', 'what', 'explanation']):
            counters['eli5'] += 1
        
        # CommonGen: ì¤‘ê°„ ê¸¸ì´ ë¬¸ì¥ (5-30 ë‹¨ì–´, ì™„ì „í•œ ë¬¸ì¥)
        elif 5 <= word_count <= 30 and target_str.endswith('.'):
            counters['commongen'] += 1
    
    # ê°€ì¥ ë§ì´ ë§¤ì¹­ëœ íƒ€ì… ë°˜í™˜ (40% ì´ìƒ ë§¤ì¹­ ì‹œ)
    threshold = sample_size * 0.4  # ìƒˆ ë°ì´í„°ì…‹ì€ íŒ¨í„´ì´ ëœ ëª…í™•í•  ìˆ˜ ìˆì–´ì„œ ì„ê³„ê°’ ë‚®ì¶¤
    
    for dataset_type, count in counters.items():
        if count >= threshold:
            return dataset_type
    
    # íŠ¹ë³„ ì¼€ì´ìŠ¤: í‰ê·  ê¸¸ì´ë¡œ ì¶”ê°€ íŒë‹¨
    avg_length = sum(len(str(target).split()) for target in sample) / len(sample)
    
    if avg_length >= 30:
        return 'eli5'
    elif 10 <= avg_length < 30:
        return 'commongen'
    
    return None

def get_accuracy_breakdown(predictions: List[str], targets: List[str], 
                          dataset_type: Optional[str] = None) -> dict:
    """ì •í™•ë„ ì„¸ë¶€ ë¶„ì„ ì •ë³´ ë°˜í™˜ - ìƒˆ ë°ì´í„°ì…‹ ì§€ì›"""
    if not predictions or not targets:
        return {'accuracy': 0.0, 'correct': 0, 'total': 0, 'details': []}
    
    # ê¸¸ì´ ë§ì¶¤
    min_len = min(len(predictions), len(targets))
    predictions = predictions[:min_len]
    targets = targets[:min_len]
    
    # ë°ì´í„°ì…‹ íƒ€ì… ê°ì§€
    if dataset_type is None:
        dataset_type = detect_dataset_type(targets)
    
    # ì„¸ë¶€ ë¶„ì„
    details = []
    correct_count = 0
    
    for i, (pred, target) in enumerate(zip(predictions, targets)):
        is_correct = exact_match_score(pred, target, dataset_type)
        if is_correct:
            correct_count += 1
            
        # ìƒˆ ë°ì´í„°ì…‹ì˜ ê²½ìš° ì¶”ê°€ ì •ë³´
        extra_info = {}
        if dataset_type in ["eli5", "commongen"]:
            pred_norm = normalize_answer(extract_final_answer(pred, dataset_type))
            target_norm = normalize_answer(extract_final_answer(target, dataset_type))
            
            if pred_norm and target_norm:
                pred_tokens = set(pred_norm.split())
                target_tokens = set(target_norm.split())
                intersection = len(pred_tokens & target_tokens)
                union = len(pred_tokens | target_tokens)
                similarity = intersection / union if union > 0 else 0.0
                extra_info['token_similarity'] = similarity
            
        details.append({
            'index': i,
            'prediction': pred,
            'target': target,
            'correct': is_correct,
            'extracted_pred': extract_final_answer(pred, dataset_type),
            'extracted_target': extract_final_answer(target, dataset_type),
            **extra_info
        })
    
    return {
        'accuracy': correct_count / len(predictions),
        'correct': correct_count,
        'total': len(predictions),
        'dataset_type': dataset_type,
        'details': details
    }