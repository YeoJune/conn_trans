# utils/metrics.py
import re
import string
import torch
import numpy as np
from collections import Counter
from typing import List, Dict, Any, Optional, Tuple

def normalize_answer(s: str) -> str:
    """ë‹µë³€ì„ ì •ê·œí™” (ê³µë°±, êµ¬ë‘ì , ëŒ€ì†Œë¬¸ì ì²˜ë¦¬)"""
    def remove_articles(text):
        return re.sub(r'\b(a|an|the)\b', ' ', text)
    
    def white_space_fix(text):
        return ' '.join(text.split())
    
    def remove_punc(text):
        exclude = set(string.punctuation)
        return ''.join(ch for ch in text if ch not in exclude)
    
    def lower(text):
        return text.lower()
    
    return white_space_fix(remove_articles(remove_punc(lower(s))))

def extract_final_answer(text: str, dataset_type: Optional[str] = None) -> str:
    """
    âœ… T5 Text2Text Generationì— ìµœì í™”ëœ ë‹µë³€ ì¶”ì¶œ
    
    T5ëŠ” ì§§ê³  ì§ì ‘ì ì¸ ë‹µë³€ì„ ìƒì„±í•˜ë¯€ë¡œ ë³µì¡í•œ íŒŒì‹±ë³´ë‹¤ëŠ”
    ì²« ë²ˆì§¸ ì˜ë¯¸ìˆëŠ” í† í°ì„ ì¶”ì¶œí•˜ëŠ” ê²ƒì´ íš¨ê³¼ì 
    """
    if not text or text.strip() == "":
        return ""
    
    text = str(text).strip()
    
    # T5ëŠ” ë³´í†µ ì²« ë²ˆì§¸ ë‹¨ì–´ê°€ ë‹µì´ë¯€ë¡œ ë‹¨ìˆœí•˜ê²Œ ì²˜ë¦¬
    words = text.split()
    if not words:
        return ""
    
    first_word = words[0].strip()
    
    # ë°ì´í„°ì…‹ë³„ íŠ¹í™” ì²˜ë¦¬
    if dataset_type == "strategyqa":
        # Yes/No íŒ¨í„´
        first_lower = first_word.lower()
        if first_lower in ['yes', 'y', 'true', '1']:
            return "Yes"
        elif first_lower in ['no', 'n', 'false', '0']:
            return "No"
        # ì²« ê¸€ìë¡œ íŒë‹¨
        elif first_lower.startswith('y'):
            return "Yes"
        elif first_lower.startswith('n'):
            return "No"
        return first_word.capitalize()
    
    elif dataset_type == "gsm8k":
        # ìˆ«ì ë‹µë³€ - T5ëŠ” ë³´í†µ ìˆ«ìë§Œ ì¶œë ¥
        # ì²« ë²ˆì§¸ ìˆ«ì ì°¾ê¸°
        numbers = re.findall(r'-?\d+(?:\.\d+)?', text)
        if numbers:
            return numbers[0]
        return "0"
    
    elif dataset_type == "logiqa":
        # A, B, C, D ì„ íƒì§€
        first_upper = first_word.upper()
        if first_upper in ['A', 'B', 'C', 'D']:
            return first_upper
        # ê´„í˜¸ ì œê±°
        clean_first = re.sub(r'[^\w]', '', first_upper)
        if clean_first in ['A', 'B', 'C', 'D']:
            return clean_first
        return "A"  # ê¸°ë³¸ê°’
    
    elif dataset_type == "multinli":
        # entailment, neutral, contradiction
        first_lower = first_word.lower()
        if first_lower.startswith('ent'):
            return "entailment"
        elif first_lower.startswith('neu'):
            return "neutral"
        elif first_lower.startswith('con'):
            return "contradiction"
        return first_word.lower()
    
    # ê¸°ë³¸: ì²« ë²ˆì§¸ ë‹¨ì–´ ë°˜í™˜
    return first_word

def exact_match_score(prediction: str, ground_truth: str, dataset_type: Optional[str] = None) -> bool:
    """âœ… T5ì— ìµœì í™”ëœ ì •í™•í•œ ì¼ì¹˜ ì ìˆ˜"""
    pred_answer = extract_final_answer(prediction, dataset_type)
    true_answer = extract_final_answer(ground_truth, dataset_type)
    
    # ì •ê·œí™” í›„ ë¹„êµ
    pred_norm = normalize_answer(pred_answer)
    true_norm = normalize_answer(true_answer)
    
    # GSM8K íŠ¹ë³„ ì²˜ë¦¬ (ìˆ«ì ë¹„êµ)
    if dataset_type == "gsm8k":
        try:
            pred_num = float(pred_norm) if pred_norm else None
            true_num = float(true_norm) if true_norm else None
            if pred_num is not None and true_num is not None:
                return abs(pred_num - true_num) < 1e-6
        except ValueError:
            pass
    
    return pred_norm == true_norm

def calculate_accuracy(predictions: List[str], targets: List[str], 
                      dataset_type: Optional[str] = None) -> float:
    """âœ… T5 Text2Text Generationì— ìµœì í™”ëœ ì •í™•ë„ ê³„ì‚°"""
    if len(predictions) != len(targets):
        raise ValueError(f"Predictions and targets must have same length: {len(predictions)} vs {len(targets)}")
    
    if len(predictions) == 0:
        return 0.0
    
    # ë°ì´í„°ì…‹ íƒ€ì… ìë™ ê°ì§€
    if dataset_type is None:
        dataset_type = detect_dataset_type(targets)
    
    correct = 0
    total = len(predictions)
    
    print(f"\nğŸ” T5 Accuracy Debug (dataset_type: {dataset_type}):")
    
    # ê° ì˜ˆì¸¡ ë¶„ì„
    for i, (pred, target) in enumerate(zip(predictions, targets)):
        extracted_pred = extract_final_answer(str(pred), dataset_type)
        extracted_target = extract_final_answer(str(target), dataset_type)
        
        is_correct = exact_match_score(str(pred), str(target), dataset_type)
        if is_correct:
            correct += 1
        
        # ì²˜ìŒ 5ê°œ ìƒ˜í”Œë§Œ ìƒì„¸ ì¶œë ¥
        if i < 5:
            status = "âœ…" if is_correct else "âŒ"
            print(f"  {status} Pred: '{pred}' -> '{extracted_pred}' | Target: '{target}' -> '{extracted_target}'")
    
    accuracy = correct / total
    print(f"ğŸ¯ Final T5 accuracy: {correct}/{total} = {accuracy:.4f}")
    
    return accuracy

def calculate_token_level_accuracy(predictions: List[str], targets: List[str],
                                 tokenizer, dataset_type: Optional[str] = None) -> Dict[str, float]:
    """âœ… T5 í† í° ë ˆë²¨ ì •í™•ë„ (ë” ì„¸ë°€í•œ ë¶„ì„)"""
    if len(predictions) != len(targets):
        raise ValueError(f"Length mismatch: {len(predictions)} vs {len(targets)}")
    
    total_tokens = 0
    correct_tokens = 0
    
    for pred, target in zip(predictions, targets):
        # í† í¬ë‚˜ì´ì§•
        pred_tokens = tokenizer.encode(str(pred), add_special_tokens=False)
        target_tokens = tokenizer.encode(str(target), add_special_tokens=False)
        
        # ê¸¸ì´ ë§ì¶”ê¸° (ë” ì§§ì€ ê²ƒì— ë§ì¶¤)
        min_len = min(len(pred_tokens), len(target_tokens))
        
        for i in range(min_len):
            total_tokens += 1
            if pred_tokens[i] == target_tokens[i]:
                correct_tokens += 1
    
    token_accuracy = correct_tokens / total_tokens if total_tokens > 0 else 0.0
    
    return {
        'token_accuracy': token_accuracy,
        'total_tokens': total_tokens,
        'correct_tokens': correct_tokens
    }

def calculate_sequence_level_metrics(predictions: List[str], targets: List[str],
                                   dataset_type: Optional[str] = None) -> Dict[str, float]:
    """âœ… T5ì— ì í•©í•œ ì‹œí€€ìŠ¤ ë ˆë²¨ ë©”íŠ¸ë¦­"""
    exact_matches = []
    f1_scores = []
    
    for pred, target in zip(predictions, targets):
        # Exact Match
        em = exact_match_score(pred, target, dataset_type)
        exact_matches.append(em)
        
        # F1 Score (í† í° ë ˆë²¨)
        pred_tokens = normalize_answer(extract_final_answer(pred, dataset_type)).split()
        target_tokens = normalize_answer(extract_final_answer(target, dataset_type)).split()
        
        if len(pred_tokens) == 0 and len(target_tokens) == 0:
            f1 = 1.0
        elif len(pred_tokens) == 0 or len(target_tokens) == 0:
            f1 = 0.0
        else:
            common = Counter(pred_tokens) & Counter(target_tokens)
            num_same = sum(common.values())
            
            if num_same == 0:
                f1 = 0.0
            else:
                precision = num_same / len(pred_tokens)
                recall = num_same / len(target_tokens)
                f1 = 2 * precision * recall / (precision + recall)
        
        f1_scores.append(f1)
    
    return {
        'exact_match': np.mean(exact_matches),
        'f1_score': np.mean(f1_scores),
        'total_samples': len(predictions)
    }

def calculate_generation_quality_metrics(predictions: List[str]) -> Dict[str, float]:
    """âœ… T5 ìƒì„± í’ˆì§ˆ ë©”íŠ¸ë¦­"""
    if not predictions:
        return {}
    
    # ê¸¸ì´ í†µê³„
    lengths = [len(pred.split()) for pred in predictions]
    
    # ë‹¤ì–‘ì„± ì¸¡ì •
    unique_predictions = len(set(predictions))
    diversity = unique_predictions / len(predictions)
    
    # ë¹ˆ ì˜ˆì¸¡ ë¹„ìœ¨
    empty_predictions = sum(1 for pred in predictions if not pred.strip())
    empty_ratio = empty_predictions / len(predictions)
    
    # ë°˜ë³µ íŒ¨í„´ ê°ì§€
    repetitive_predictions = 0
    for pred in predictions:
        words = pred.split()
        if len(words) > 1:
            # ê°™ì€ ë‹¨ì–´ê°€ ì—°ì†ìœ¼ë¡œ ë°˜ë³µë˜ëŠ”ì§€ í™•ì¸
            for i in range(len(words) - 1):
                if words[i] == words[i + 1]:
                    repetitive_predictions += 1
                    break
    
    repetition_ratio = repetitive_predictions / len(predictions)
    
    return {
        'avg_length': np.mean(lengths),
        'length_std': np.std(lengths),
        'diversity': diversity,
        'empty_ratio': empty_ratio,
        'repetition_ratio': repetition_ratio,
        'min_length': min(lengths),
        'max_length': max(lengths)
    }

def detect_dataset_type(targets: List[str]) -> Optional[str]:
    """íƒ€ê²Ÿ ë°ì´í„°ë¡œë¶€í„° ë°ì´í„°ì…‹ íƒ€ì… ìë™ ê°ì§€ (ê°œì„ ë¨)"""
    if not targets:
        return None
    
    # ë” ë§ì€ ìƒ˜í”Œë¡œ ì •í™•ë„ í–¥ìƒ
    sample_targets = targets[:min(20, len(targets))]
    
    yes_no_count = 0
    number_count = 0
    choice_count = 0
    entail_count = 0
    
    for target in sample_targets:
        target_str = str(target).lower().strip()
        
        # Yes/No íŒ¨í„´
        if target_str in ['yes', 'no', 'true', 'false']:
            yes_no_count += 1
        
        # ìˆ«ì íŒ¨í„´ (ë” ìœ ì—°í•˜ê²Œ)
        if re.search(r'^\d+(\.\d+)?$', target_str) or target_str.isdigit():
            number_count += 1
        
        # ì„ íƒì§€ íŒ¨í„´
        if re.search(r'^[abcd]$', target_str):
            choice_count += 1
        
        # Entailment íŒ¨í„´
        if target_str in ['entailment', 'neutral', 'contradiction']:
            entail_count += 1
    
    # ê°€ì¥ ë§ì€ íŒ¨í„´ìœ¼ë¡œ ê²°ì • (ë” ì—„ê²©í•œ ê¸°ì¤€)
    total_samples = len(sample_targets)
    threshold = total_samples * 0.5  # 50% ì´ìƒì´ì–´ì•¼ í™•ì‹ 
    
    if yes_no_count >= threshold:
        return 'strategyqa'
    elif number_count >= threshold:
        return 'gsm8k'
    elif choice_count >= threshold:
        return 'logiqa'
    elif entail_count >= threshold:
        return 'multinli'
    
    return None  # í™•ì‹ í•  ìˆ˜ ì—†ìŒ

def analyze_error_cases(predictions: List[str], targets: List[str],
                       dataset_type: Optional[str] = None, 
                       max_examples: int = 10) -> Dict[str, Any]:
    """âœ… T5 ì˜¤ë¥˜ ì‚¬ë¡€ ë¶„ì„ (ê°œì„ ë¨)"""
    if dataset_type is None:
        dataset_type = detect_dataset_type(targets)
    
    errors = []
    error_types = {
        'format_errors': 0,      # í˜•ì‹ ì˜¤ë¥˜ (ì˜ˆ: Yes/No ëŒ€ì‹  ë‹¤ë¥¸ ë‹µ)
        'partial_matches': 0,    # ë¶€ë¶„ ì¼ì¹˜ (ì˜¬ë°”ë¥¸ ë‚´ìš©ì´ì§€ë§Œ í˜•ì‹ì´ ë‹¤ë¦„)
        'content_errors': 0,     # ë‚´ìš© ì˜¤ë¥˜ (ì™„ì „íˆ í‹€ë¦¼)
        'empty_predictions': 0   # ë¹ˆ ì˜ˆì¸¡
    }
    
    for i, (pred, target) in enumerate(zip(predictions, targets)):
        extracted_pred = extract_final_answer(str(pred), dataset_type)
        extracted_target = extract_final_answer(str(target), dataset_type)
        
        if not exact_match_score(pred, target, dataset_type):
            # ì˜¤ë¥˜ íƒ€ì… ë¶„ë¥˜
            error_type = 'content_errors'  # ê¸°ë³¸ê°’
            
            if not pred.strip():
                error_type = 'empty_predictions'
            elif dataset_type == "strategyqa":
                if extracted_pred not in ['Yes', 'No']:
                    error_type = 'format_errors'
                else:
                    error_type = 'content_errors'
            elif dataset_type == "logiqa":
                if extracted_pred not in ['A', 'B', 'C', 'D']:
                    error_type = 'format_errors'
                else:
                    error_type = 'content_errors'
            elif dataset_type == "multinli":
                if extracted_pred not in ['entailment', 'neutral', 'contradiction']:
                    error_type = 'format_errors'
                else:
                    error_type = 'content_errors'
            
            error_types[error_type] += 1
            
            if len(errors) < max_examples:
                errors.append({
                    'index': i,
                    'prediction': pred,
                    'target': target,
                    'extracted_prediction': extracted_pred,
                    'extracted_target': extracted_target,
                    'error_type': error_type,
                    'dataset_type': dataset_type
                })
    
    return {
        'errors': errors,
        'error_types': error_types,
        'total_errors': len([p for p, t in zip(predictions, targets) 
                           if not exact_match_score(p, t, dataset_type)]),
        'total_samples': len(predictions)
    }

def calculate_comprehensive_metrics(predictions: List[str], targets: List[str],
                                  tokenizer=None, dataset_type: Optional[str] = None) -> Dict[str, Any]:
    """âœ… T5ì— ìµœì í™”ëœ ì¢…í•© ë©”íŠ¸ë¦­"""
    if not predictions or not targets:
        return {}
    
    metrics = {}
    
    # ê¸°ë³¸ ì •í™•ë„
    metrics['accuracy'] = calculate_accuracy(predictions, targets, dataset_type)
    
    # ì‹œí€€ìŠ¤ ë ˆë²¨ ë©”íŠ¸ë¦­
    seq_metrics = calculate_sequence_level_metrics(predictions, targets, dataset_type)
    metrics.update(seq_metrics)
    
    # ìƒì„± í’ˆì§ˆ ë©”íŠ¸ë¦­
    quality_metrics = calculate_generation_quality_metrics(predictions)
    metrics.update(quality_metrics)
    
    # í† í° ë ˆë²¨ ì •í™•ë„ (í† í¬ë‚˜ì´ì €ê°€ ìˆëŠ” ê²½ìš°)
    if tokenizer is not None:
        token_metrics = calculate_token_level_accuracy(predictions, targets, tokenizer, dataset_type)
        metrics.update(token_metrics)
    
    # ì˜¤ë¥˜ ë¶„ì„
    error_analysis = analyze_error_cases(predictions, targets, dataset_type)
    metrics['error_analysis'] = error_analysis
    
    # ë°ì´í„°ì…‹ íŠ¹í™” ë©”íŠ¸ë¦­
    if dataset_type:
        metrics['dataset_type'] = dataset_type
        metrics['detected_format_compliance'] = _check_format_compliance(predictions, dataset_type)
    
    return metrics

def _check_format_compliance(predictions: List[str], dataset_type: str) -> float:
    """ì˜ˆì¸¡ì´ ì˜ˆìƒ í˜•ì‹ì— ë§ëŠ”ì§€ í™•ì¸"""
    compliant = 0
    
    for pred in predictions:
        extracted = extract_final_answer(pred, dataset_type)
        
        if dataset_type == "strategyqa":
            if extracted in ['Yes', 'No']:
                compliant += 1
        elif dataset_type == "logiqa":
            if extracted in ['A', 'B', 'C', 'D']:
                compliant += 1
        elif dataset_type == "multinli":
            if extracted in ['entailment', 'neutral', 'contradiction']:
                compliant += 1
        elif dataset_type == "gsm8k":
            try:
                float(extracted)
                compliant += 1
            except ValueError:
                pass
    
    return compliant / len(predictions) if predictions else 0.0