# utils/comparison_analyzer.py
import json
import time
from pathlib import Path
from typing import Dict, List, Any
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

class ComparisonAnalyzer:
    """ì‹¤í—˜ ê°„ êµì°¨ ë¹„êµ ë¶„ì„ê¸°"""
    
    def __init__(self, base_dir: str):
        self.base_dir = Path(base_dir)
        self.comparison_dir = self.base_dir / "comparisons" / f"{time.strftime('%Y%m%d_%H%M')}_comparison"
        self.comparison_dir.mkdir(parents=True, exist_ok=True)
        
        print(f"ğŸ“Š ë¹„êµ ë¶„ì„ ë””ë ‰í† ë¦¬: {self.comparison_dir}")
    
    def analyze_all_experiments(self) -> bool:
        """ëª¨ë“  ì‹¤í—˜ ê²°ê³¼ë¥¼ ë¶„ì„í•˜ê³  ë¹„êµ"""
        # 1. ì‹¤í—˜ ê²°ê³¼ ìˆ˜ì§‘
        experiments = self._collect_experiments()
        if len(experiments) < 2:
            print(f"âš ï¸ ë¹„êµí•  ì‹¤í—˜ì´ ë¶€ì¡±í•©ë‹ˆë‹¤ (í˜„ì¬: {len(experiments)}ê°œ)")
            return False
        
        print(f"ğŸ” {len(experiments)}ê°œ ì‹¤í—˜ ë°œê²¬")
        
        # 2. ë¹„êµ í…Œì´ë¸” ìƒì„±
        df = self._create_comparison_table(experiments)
        
        # 3. ê²°ê³¼ ì €ì¥
        self._save_comparison_results(df, experiments)
        
        # 4. ì‹œê°í™” ìƒì„±
        self._create_comparison_charts(df)
        
        # 5. ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„±
        self._generate_summary_report(df, experiments)
        
        print(f"âœ… ë¹„êµ ë¶„ì„ ì™„ë£Œ: {self.comparison_dir}")
        return True
    
    def _collect_experiments(self) -> List[Dict[str, Any]]:
        """experiments ë””ë ‰í† ë¦¬ì—ì„œ ëª¨ë“  ì‹¤í—˜ ìˆ˜ì§‘"""
        experiments = []
        exp_dir = self.base_dir / "experiments"
        
        if not exp_dir.exists():
            return experiments
        
        for exp_folder in exp_dir.iterdir():
            if not exp_folder.is_dir():
                continue
            
            try:
                # config.json ì½ê¸°
                config_path = exp_folder / "config.json"
                summary_path = self.base_dir / "analysis" / exp_folder.name / "summary.json"
                
                if config_path.exists() and summary_path.exists():
                    with open(config_path, 'r') as f:
                        config = json.load(f)
                    
                    with open(summary_path, 'r') as f:
                        summary = json.load(f)
                    
                    experiments.append({
                        'folder': exp_folder.name,
                        'config': config,
                        'summary': summary
                    })
                    
            except Exception as e:
                print(f"âš ï¸ {exp_folder.name} ì½ê¸° ì‹¤íŒ¨: {str(e)[:30]}...")
                continue
        
        return experiments
    
    def _create_comparison_table(self, experiments: List[Dict]) -> pd.DataFrame:
        """ë¹„êµ í…Œì´ë¸” ìƒì„±"""
        rows = []
        
        for exp in experiments:
            config = exp['config']
            summary = exp['summary']
            
            row = {
                'Experiment_ID': exp['folder'],
                'Model': config.get('model_type', 'unknown').title(),
                'Dataset': config.get('dataset', 'unknown').upper(),
                'Size': config.get('model_size', 'unknown'),
                'Accuracy': summary.get('best_accuracy', 0.0),
                'd_model': config.get('d_model', 'N/A'),
                'Batch_Size': config.get('batch_size', 'N/A'),
                'Learning_Rate': config.get('learning_rate', 'N/A'),
                'Epochs': summary.get('final_metrics', {}).get('num_epochs', 'N/A'),
                'Final_Train_Loss': summary.get('final_metrics', {}).get('train_loss', 'N/A'),
                'Timestamp': config.get('timestamp', 'unknown')
            }
            
            # Connection Transformer ì „ìš© ì •ë³´
            if config.get('model_type') == 'connection':
                row['Slots'] = config.get('num_slots', 'N/A')
                row['Bilinear_Rank'] = config.get('bilinear_rank', 'N/A')
                row['Max_Steps'] = config.get('max_reasoning_steps', 'N/A')
            else:
                row['Slots'] = 'N/A'
                row['Bilinear_Rank'] = 'N/A'
                row['Max_Steps'] = 'N/A'
            
            rows.append(row)
        
        return pd.DataFrame(rows)
    
    def _save_comparison_results(self, df: pd.DataFrame, experiments: List[Dict]):
        """ë¹„êµ ê²°ê³¼ ì €ì¥"""
        # CSV ì €ì¥
        csv_path = self.comparison_dir / "comparison_table.csv"
        df.to_csv(csv_path, index=False)
        print(f"ğŸ’¾ ë¹„êµ í…Œì´ë¸”: {csv_path.name}")
        
        # ìƒì„¸ ë°ì´í„° JSON ì €ì¥
        detailed_path = self.comparison_dir / "detailed_results.json"
        with open(detailed_path, 'w') as f:
            json.dump(experiments, f, indent=2, default=str)
    
    def _create_comparison_charts(self, df: pd.DataFrame):
        """ë¹„êµ ì°¨íŠ¸ ìƒì„±"""
        if len(df) == 0:
            return
        
        # 1. ë°ì´í„°ì…‹ë³„ ì„±ëŠ¥ ë¹„êµ
        self._plot_dataset_performance(df)
        
        # 2. ëª¨ë¸ íƒ€ì…ë³„ ë¹„êµ
        self._plot_model_comparison(df)
        
        # 3. ëª¨ë¸ í¬ê¸°ë³„ ì„±ëŠ¥
        self._plot_size_analysis(df)
    
    def _plot_dataset_performance(self, df: pd.DataFrame):
        """ë°ì´í„°ì…‹ë³„ ì„±ëŠ¥ ë¹„êµ ì°¨íŠ¸"""
        datasets = df['Dataset'].unique()
        models = df['Model'].unique()
        
        if len(datasets) < 2:
            return
        
        fig, ax = plt.subplots(figsize=(12, 6))
        
        x = range(len(datasets))
        width = 0.35
        
        for i, model in enumerate(models):
            model_data = []
            for dataset in datasets:
                subset = df[(df['Dataset'] == dataset) & (df['Model'] == model)]
                if len(subset) > 0:
                    # ê°™ì€ ë°ì´í„°ì…‹ì—ì„œ ìµœê³  ì„±ëŠ¥ ì„ íƒ
                    model_data.append(subset['Accuracy'].max())
                else:
                    model_data.append(0)
            
            color = '#3498db' if model == 'Connection' else '#e74c3c'
            bars = ax.bar([xi + i * width for xi in x], model_data, 
                         width, label=model, color=color, alpha=0.8)
            
            # ê°’ í‘œì‹œ
            for bar, value in zip(bars, model_data):
                if value > 0:
                    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005,
                           f'{value:.3f}', ha='center', va='bottom', fontweight='bold')
        
        ax.set_xlabel('Dataset')
        ax.set_ylabel('Accuracy')
        ax.set_title('Performance Comparison by Dataset')
        ax.set_xticks([xi + width/2 for xi in x])
        ax.set_xticklabels(datasets)
        ax.legend()
        ax.grid(True, alpha=0.3, axis='y')
        
        plt.tight_layout()
        plt.savefig(self.comparison_dir / 'dataset_performance.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"ğŸ“Š ë°ì´í„°ì…‹ ì„±ëŠ¥ ì°¨íŠ¸ ì €ì¥")
    
    def _plot_model_comparison(self, df: pd.DataFrame):
        """ëª¨ë¸ íƒ€ì…ë³„ ì „ì²´ ë¹„êµ"""
        if len(df['Model'].unique()) < 2:
            return
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
        
        # 1. í‰ê·  ì„±ëŠ¥ ë¹„êµ
        model_avg = df.groupby('Model')['Accuracy'].agg(['mean', 'std', 'count']).reset_index()
        
        bars = ax1.bar(model_avg['Model'], model_avg['mean'], 
                      yerr=model_avg['std'], capsize=5,
                      color=['#3498db', '#e74c3c'], alpha=0.8)
        
        # ê°’ê³¼ ê°œìˆ˜ í‘œì‹œ
        for bar, mean, count in zip(bars, model_avg['mean'], model_avg['count']):
            ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                    f'{mean:.3f}\n(n={count})', ha='center', va='bottom', fontweight='bold')
        
        ax1.set_ylabel('Average Accuracy')
        ax1.set_title('Average Performance by Model Type')
        ax1.grid(True, alpha=0.3, axis='y')
        
        # 2. ì„±ëŠ¥ ë¶„í¬ ë°•ìŠ¤í”Œë¡¯
        models = df['Model'].unique()
        data = [df[df['Model'] == model]['Accuracy'].values for model in models]
        
        box_plot = ax2.boxplot(data, labels=models, patch_artist=True)
        colors = ['#3498db', '#e74c3c']
        for patch, color in zip(box_plot['boxes'], colors[:len(models)]):
            patch.set_facecolor(color)
            patch.set_alpha(0.7)
        
        ax2.set_ylabel('Accuracy')
        ax2.set_title('Accuracy Distribution by Model Type')
        ax2.grid(True, alpha=0.3, axis='y')
        
        plt.tight_layout()
        plt.savefig(self.comparison_dir / 'model_comparison.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"ğŸ“ˆ ëª¨ë¸ ë¹„êµ ì°¨íŠ¸ ì €ì¥")
    
    def _plot_size_analysis(self, df: pd.DataFrame):
        """ëª¨ë¸ í¬ê¸°ë³„ ì„±ëŠ¥ ë¶„ì„"""
        if 'd_model' not in df.columns or df['d_model'].nunique() < 2:
            return
        
        # ìˆ«ì ë³€í™˜ ê°€ëŠ¥í•œ d_modelë§Œ í•„í„°ë§
        numeric_df = df[df['d_model'] != 'N/A'].copy()
        try:
            numeric_df['d_model_num'] = pd.to_numeric(numeric_df['d_model'])
        except:
            return
        
        if len(numeric_df) < 2:
            return
        
        fig, ax = plt.subplots(figsize=(10, 6))
        
        for model in numeric_df['Model'].unique():
            model_data = numeric_df[numeric_df['Model'] == model]
            color = '#3498db' if model == 'Connection' else '#e74c3c'
            
            ax.scatter(model_data['d_model_num'], model_data['Accuracy'], 
                      label=model, alpha=0.7, s=80, color=color)
        
        ax.set_xlabel('Model Dimension (d_model)')
        ax.set_ylabel('Accuracy')
        ax.set_title('Accuracy vs Model Size')
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(self.comparison_dir / 'size_analysis.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"ğŸ“ í¬ê¸° ë¶„ì„ ì°¨íŠ¸ ì €ì¥")
    
    def _generate_summary_report(self, df: pd.DataFrame, experiments: List[Dict]):
        """ì¢…í•© ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„±"""
        report_path = self.comparison_dir / "summary_report.md"
        
        with open(report_path, 'w') as f:
            f.write("# ì‹¤í—˜ ë¹„êµ ë¶„ì„ ë¦¬í¬íŠ¸\n\n")
            f.write(f"ìƒì„± ì‹œê°„: {time.strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            # ì „ì²´ ìš”ì•½
            f.write("## ğŸ“Š ì „ì²´ ìš”ì•½\n\n")
            f.write(f"- **ì´ ì‹¤í—˜ ìˆ˜**: {len(experiments)}\n")
            f.write(f"- **ëª¨ë¸ íƒ€ì…**: {', '.join(df['Model'].unique())}\n")
            f.write(f"- **ë°ì´í„°ì…‹**: {', '.join(df['Dataset'].unique())}\n")
            f.write(f"- **ìµœê³  ì •í™•ë„**: {df['Accuracy'].max():.4f}\n")
            f.write(f"- **í‰ê·  ì •í™•ë„**: {df['Accuracy'].mean():.4f}\n\n")
            
            # ë°ì´í„°ì…‹ë³„ ìµœê³  ì„±ëŠ¥
            f.write("## ğŸ† ë°ì´í„°ì…‹ë³„ ìµœê³  ì„±ëŠ¥\n\n")
            for dataset in df['Dataset'].unique():
                dataset_df = df[df['Dataset'] == dataset]
                if len(dataset_df) > 0:
                    best_row = dataset_df.loc[dataset_df['Accuracy'].idxmax()]
                    f.write(f"### {dataset}\n")
                    f.write(f"- **ìµœê³  ì„±ëŠ¥**: {best_row['Accuracy']:.4f}\n")
                    f.write(f"- **ëª¨ë¸**: {best_row['Model']}\n")
                    f.write(f"- **ì‹¤í—˜ ID**: `{best_row['Experiment_ID']}`\n\n")
            
            # Connection vs Baseline ë¹„êµ
            f.write("## âš”ï¸ Connection vs Baseline ë¹„êµ\n\n")
            
            total_comparisons = 0
            connection_wins = 0
            improvements = []
            
            for dataset in df['Dataset'].unique():
                dataset_df = df[df['Dataset'] == dataset]
                conn_rows = dataset_df[dataset_df['Model'] == 'Connection']
                base_rows = dataset_df[dataset_df['Model'] == 'Baseline']
                
                if len(conn_rows) > 0 and len(base_rows) > 0:
                    conn_best = conn_rows['Accuracy'].max()
                    base_best = base_rows['Accuracy'].max()
                    improvement = (conn_best - base_best) * 100
                    
                    f.write(f"### {dataset}\n")
                    f.write(f"- **Connection**: {conn_best:.4f}\n")
                    f.write(f"- **Baseline**: {base_best:.4f}\n")
                    f.write(f"- **ê°œì„ **: {improvement:+.2f}%p\n")
                    
                    total_comparisons += 1
                    improvements.append(improvement)
                    
                    if conn_best > base_best:
                        connection_wins += 1
                        f.write(f"- **ê²°ê³¼**: âœ… Connection ìŠ¹ë¦¬\n\n")
                    else:
                        f.write(f"- **ê²°ê³¼**: âŒ Baseline ìŠ¹ë¦¬\n\n")
            
            # ì „ì²´ ê²°ë¡ 
            f.write("## ğŸ¯ ì¢…í•© ê²°ë¡ \n\n")
            
            if total_comparisons > 0:
                win_rate = connection_wins / total_comparisons * 100
                avg_improvement = sum(improvements) / len(improvements)
                
                f.write(f"- **ì§ì ‘ ë¹„êµ ê°€ëŠ¥í•œ ë°ì´í„°ì…‹**: {total_comparisons}ê°œ\n")
                f.write(f"- **Connection Transformer ìŠ¹ë¥ **: {connection_wins}/{total_comparisons} ({win_rate:.1f}%)\n")
                f.write(f"- **í‰ê·  ì„±ëŠ¥ ê°œì„ **: {avg_improvement:+.2f}%p\n\n")
                
                if win_rate >= 70:
                    f.write("**ğŸ‰ Connection Transformerê°€ ëŒ€ë¶€ë¶„ì˜ íƒœìŠ¤í¬ì—ì„œ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤.**\n")
                elif win_rate >= 50:
                    f.write("**âœ… Connection Transformerê°€ ì „ë°˜ì ìœ¼ë¡œ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤.**\n")
                else:
                    f.write("**âš ï¸ í˜¼ì¬ëœ ê²°ê³¼ë¡œ ì¶”ê°€ ë¶„ì„ì´ í•„ìš”í•©ë‹ˆë‹¤.**\n")
            else:
                f.write("- **ì§ì ‘ ë¹„êµ ë¶ˆê°€**: ê°™ì€ ë°ì´í„°ì…‹ì—ì„œ ë‘ ëª¨ë¸ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.\n")
            
            # ê¶Œì¥ì‚¬í•­
            f.write("\n## ğŸ’¡ ê¶Œì¥ì‚¬í•­\n\n")
            f.write("1. **ì„±ëŠ¥ì´ ì¢‹ì€ ì„¤ì •**ì„ ë‹¤ë¥¸ ë°ì´í„°ì…‹ì— ì ìš©í•´ë³´ì„¸ìš”\n")
            f.write("2. **ë¶€ì¡±í•œ ë°ì´í„°ì…‹**ì—ì„œ ì¶”ê°€ ì‹¤í—˜ì„ ì§„í–‰í•˜ì„¸ìš”\n")
            f.write("3. **í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹**ìœ¼ë¡œ ì„±ëŠ¥ì„ ë” ê°œì„ í•´ë³´ì„¸ìš”\n")
            
            # íŒŒì¼ ìœ„ì¹˜ ì•ˆë‚´
            f.write(f"\n## ğŸ“ ìƒì„¸ ê²°ê³¼\n\n")
            f.write(f"- **ë¹„êµ í…Œì´ë¸”**: `{self.comparison_dir.name}/comparison_table.csv`\n")
            f.write(f"- **ì°¨íŠ¸ë“¤**: `{self.comparison_dir.name}/`\n")
            f.write(f"- **ê°œë³„ ì‹¤í—˜ ê²°ê³¼**: `analysis/` ë””ë ‰í† ë¦¬\n")
        
        print(f"ğŸ“‹ ìš”ì•½ ë¦¬í¬íŠ¸ ì €ì¥: {report_path.name}")
    
    def get_comparison_summary(self) -> Dict[str, Any]:
        """í”„ë¡œê·¸ë˜ë° ë°©ì‹ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ìš”ì•½ ë°˜í™˜"""
        experiments = self._collect_experiments()
        if len(experiments) < 2:
            return {'status': 'insufficient_data', 'count': len(experiments)}
        
        df = self._create_comparison_table(experiments)
        
        return {
            'status': 'success',
            'total_experiments': len(experiments),
            'datasets': list(df['Dataset'].unique()),
            'models': list(df['Model'].unique()),
            'best_accuracy': float(df['Accuracy'].max()),
            'average_accuracy': float(df['Accuracy'].mean()),
            'comparison_dir': str(self.comparison_dir)
        }