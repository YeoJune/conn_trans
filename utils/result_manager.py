# utils/result_manager.py
import os
import json
import time
from pathlib import Path
from typing import Dict, List, Optional, Any
import torch

class ResultManager:
    """í†µí•© ê²°ê³¼ ê´€ë¦¬ì - í•™ìŠµ ì¤‘/í›„ íŒŒì¼ì„ ì²´ê³„ì ìœ¼ë¡œ ë¶„ë¦¬"""
    
    def __init__(self, base_dir: str, model_type: str, dataset: str, model_size: str):
        self.base_dir = Path(base_dir)
        self.model_type = model_type
        self.dataset = dataset
        self.model_size = model_size
        
        # ì‹¤í—˜ ì‹ë³„ì ìƒì„±
        self.timestamp = time.strftime("%Y%m%d_%H%M")
        self.exp_id = f"{self.timestamp}_{model_type}_{dataset}_{model_size}"
        
        # ë””ë ‰í† ë¦¬ êµ¬ì¡° ì„¤ì •
        self.exp_dir = self.base_dir / "experiments" / self.exp_id
        self.analysis_dir = self.base_dir / "analysis" / self.exp_id
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        self.exp_dir.mkdir(parents=True, exist_ok=True)
        self.analysis_dir.mkdir(parents=True, exist_ok=True)
        
        # ì‹¤ì‹œê°„ ë©”íŠ¸ë¦­ ì¶”ì 
        self.metrics = {
            'train_losses': [],
            'eval_losses': [],
            'eval_accuracies': [],
            'reasoning_steps': []
        }
        
        print(f"ğŸ“ ì‹¤í—˜ ë””ë ‰í† ë¦¬: {self.exp_dir}")
        print(f"ğŸ“Š ë¶„ì„ ë””ë ‰í† ë¦¬: {self.analysis_dir}")
    
    def save_config(self, config):
        """ì‹¤í—˜ ì„¤ì • ì €ì¥ (í•™ìŠµ ì‹œì‘ ì‹œ)"""
        config_path = self.exp_dir / "config.json"
        config_dict = vars(config) if hasattr(config, '__dict__') else config
        
        # ë©”íƒ€ë°ì´í„° ì¶”ê°€
        config_dict.update({
            'experiment_id': self.exp_id,
            'timestamp': self.timestamp,
            'model_type': self.model_type,
            'dataset': self.dataset,
            'model_size': self.model_size
        })
        
        with open(config_path, 'w') as f:
            json.dump(config_dict, f, indent=2, default=str)
        
        print(f"ğŸ’¾ ì„¤ì • ì €ì¥: {config_path.name}")
    
    def update_metrics(self, epoch: int, train_loss: float, eval_loss: float, 
                      accuracy: float, reasoning_steps: Optional[float] = None):
        """ì‹¤ì‹œê°„ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸"""
        self.metrics['train_losses'].append(train_loss)
        self.metrics['eval_losses'].append(eval_loss)
        self.metrics['eval_accuracies'].append(accuracy)
        
        if reasoning_steps is not None:
            self.metrics['reasoning_steps'].append(reasoning_steps)
        
        # ì‹¤ì‹œê°„ ë©”íŠ¸ë¦­ ì €ì¥
        metrics_path = self.exp_dir / "metrics.json"
        with open(metrics_path, 'w') as f:
            json.dump(self.metrics, f, indent=2)
    
    def log_training(self, message: str):
        """ì‹¤ì‹œê°„ í›ˆë ¨ ë¡œê·¸"""
        log_path = self.exp_dir / "training_log.txt"
        timestamp = time.strftime("%H:%M:%S")
        
        with open(log_path, 'a') as f:
            f.write(f"[{timestamp}] {message}\n")
    
    def save_checkpoint(self, model, optimizer, epoch: int, accuracy: float, is_best: bool = False):
        """ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
        if is_best:
            checkpoint_path = self.exp_dir / "model_best.pt"
        else:
            checkpoint_path = self.exp_dir / f"model_epoch_{epoch}.pt"
        
        # íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚°
        total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'accuracy': accuracy,
            'total_parameters': total_params,
            'experiment_id': self.exp_id
        }
        
        torch.save(checkpoint, checkpoint_path)
        
        if is_best:
            print(f"ğŸ† ìµœê³  ëª¨ë¸ ì €ì¥: {checkpoint_path.name}")
    
    def finalize_training(self, best_accuracy: float, model, predictions: List[str], targets: List[str]):
        """í›ˆë ¨ ì™„ë£Œ í›„ ìµœì¢… ë¶„ì„ ìˆ˜í–‰"""
        print(f"\nğŸ“Š ìµœì¢… ë¶„ì„ ì‹œì‘...")
        
        # 1. ìµœì¢… ë¦¬í¬íŠ¸ ìƒì„±
        self._generate_final_report(best_accuracy, model)
        
        # 2. ìµœì¢… ì‹œê°í™” ìƒì„±
        self._generate_final_visualizations(model, predictions, targets)
        
        # 3. ê²°ê³¼ ìš”ì•½ ì €ì¥ (íŒŒë¼ë¯¸í„° ìˆ˜ í¬í•¨)
        self._save_experiment_summary(best_accuracy, model, predictions, targets)
        
        print(f"âœ… ë¶„ì„ ì™„ë£Œ: {self.analysis_dir}")
        return self.analysis_dir
    
    def _generate_final_report(self, best_accuracy: float, model):
        """ì¢…í•© ë§ˆí¬ë‹¤ìš´ ë¦¬í¬íŠ¸ ìƒì„±"""
        report_path = self.analysis_dir / "report.md"
        
        # íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚°
        total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        
        with open(report_path, 'w') as f:
            f.write(f"# ì‹¤í—˜ ê²°ê³¼: {self.exp_id}\n\n")
            
            # ê¸°ë³¸ ì •ë³´
            f.write("## ğŸ“‹ ì‹¤í—˜ ì •ë³´\n")
            f.write(f"- **ëª¨ë¸**: {self.model_type.title()}\n")
            f.write(f"- **ë°ì´í„°ì…‹**: {self.dataset.upper()}\n")
            f.write(f"- **í¬ê¸°**: {self.model_size}\n")
            f.write(f"- **ì‹¤í–‰ ì‹œê°„**: {self.timestamp}\n")
            f.write(f"- **ìµœê³  ì •í™•ë„**: {best_accuracy:.4f}\n")
            f.write(f"- **ì´ íŒŒë¼ë¯¸í„°**: {total_params:,}\n\n")
            
            # í›ˆë ¨ ì§„í–‰
            f.write("## ğŸ“ˆ í›ˆë ¨ ì§„í–‰\n")
            if self.metrics['train_losses']:
                f.write(f"- **ìµœì¢… í›ˆë ¨ ì†ì‹¤**: {self.metrics['train_losses'][-1]:.4f}\n")
                f.write(f"- **ìµœì¢… í‰ê°€ ì†ì‹¤**: {self.metrics['eval_losses'][-1]:.4f}\n")
                f.write(f"- **ì´ ì—í¬í¬**: {len(self.metrics['train_losses'])}\n")
            
            if self.metrics['reasoning_steps']:
                avg_steps = sum(self.metrics['reasoning_steps']) / len(self.metrics['reasoning_steps'])
                f.write(f"- **í‰ê·  ì¶”ë¡  ë‹¨ê³„**: {avg_steps:.2f}\n")
            
            # ëª¨ë¸ ë¶„ì„
            f.write("\n## ğŸ” ëª¨ë¸ ë¶„ì„\n")
            if hasattr(model, 'get_connection_analysis'):
                analysis = model.get_connection_analysis()
                f.write(f"- **ì—°ê²° í¬ì†Œì„±**: {analysis.get('sparsity_ratio', 0):.4f}\n")
                f.write(f"- **ìµœëŒ€ ì—°ê²° ê°•ë„**: {analysis.get('max_connection', 0):.4f}\n")
                if 'orthogonality_quality' in analysis:
                    f.write(f"- **ì§êµì„± í’ˆì§ˆ**: {analysis['orthogonality_quality']:.4f}\n")
            
            f.write(f"- **ì´ íŒŒë¼ë¯¸í„°**: {total_params:,}\n")
            
            f.write(f"\n## ğŸ“ íŒŒì¼ ìœ„ì¹˜\n")
            f.write(f"- **ì‹¤í—˜ ë°ì´í„°**: `experiments/{self.exp_id}/`\n")
            f.write(f"- **ë¶„ì„ ê²°ê³¼**: `analysis/{self.exp_id}/`\n")
        
        print(f"ğŸ“‹ ë¦¬í¬íŠ¸ ìƒì„±: {report_path.name}")
    
    def _generate_final_visualizations(self, model, predictions: List[str], targets: List[str]):
        """ìµœì¢… ì‹œê°í™” ìƒì„± (í•™ìŠµ í›„ í•œ ë²ˆë§Œ)"""
        try:
            from .visualization_manager import VisualizationManager
            
            vis_manager = VisualizationManager(self.analysis_dir)
            
            # 1. ìµœì¢… í•™ìŠµ ê³¡ì„ 
            vis_manager.plot_final_training_curves(
                self.metrics['train_losses'],
                self.metrics['eval_accuracies'],
                self.metrics.get('reasoning_steps')
            )
            
            # 2. ì—°ê²° í–‰ë ¬ (Connection Transformerë§Œ)
            if self.model_type == "connection" and hasattr(model, 'get_connection_analysis'):
                vis_manager.plot_connection_matrix(model)
            
            # 3. ì •í™•ë„ ë¶„ì„
            if predictions and targets:
                vis_manager.plot_accuracy_breakdown(predictions, targets, self.dataset)
            
            print(f"ğŸ“Š ì‹œê°í™” ì™„ë£Œ")
            
        except Exception as e:
            print(f"âš ï¸ ì‹œê°í™” ì˜¤ë¥˜: {str(e)[:50]}...")
    
    def _save_experiment_summary(self, best_accuracy: float, model, predictions: List[str], targets: List[str]):
        """ì‹¤í—˜ ìš”ì•½ ì €ì¥ (íŒŒë¼ë¯¸í„° ìˆ˜ í¬í•¨)"""
        # íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚°
        total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        
        summary = {
            'experiment_id': self.exp_id,
            'model_type': self.model_type,
            'dataset': self.dataset,
            'model_size': self.model_size,
            'timestamp': self.timestamp,
            'best_accuracy': best_accuracy,
            'total_parameters': total_params,
            'final_metrics': {
                'train_loss': self.metrics['train_losses'][-1] if self.metrics['train_losses'] else 0,
                'eval_loss': self.metrics['eval_losses'][-1] if self.metrics['eval_losses'] else 0,
                'num_epochs': len(self.metrics['train_losses'])
            },
            'sample_predictions': predictions[:3],
            'sample_targets': targets[:3]
        }
        
        summary_path = self.analysis_dir / "summary.json"
        with open(summary_path, 'w') as f:
            json.dump(summary, f, indent=2, default=str)
        
        print(f"ğŸ’¾ ìš”ì•½ ì €ì¥: {summary_path.name}")