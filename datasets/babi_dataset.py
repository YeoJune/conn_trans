# datasets/babi_dataset.py
import torch
from torch.utils.data import Dataset
from datasets import load_dataset
import re
from typing import Dict, List  # typing ì¶”ê°€


class BabiDataset(Dataset):
    """bAbI Task Dataset - (ì‚¬ìš©ì ì œê³µ ì›ë³¸ ì½”ë“œ ê¸°ë°˜, load_dataset ìˆ˜ì •)"""

    def __init__(self, task_id=1, babi_hf_config_name_prefix="en-10k-qa",  # configì—ì„œ prefix ë°›ê³  task_id ê²°í•©
                 split='train', max_seq_len=128):
        self.max_seq_len = max_seq_len
        self.task_id = task_id

        # task_idì— ë”°ë¼ hf_name_param, hf_task_no_param ê²°ì •
        # "en-10k-qa1" ê°™ì€ í˜•ì‹ì„ nameìœ¼ë¡œ ì‚¬ìš©
        hf_name_param = f"{babi_hf_config_name_prefix}{self.task_id}"
        # ì´ ê²½ìš° task_noëŠ” ëª…ì‹œì ìœ¼ë¡œ í•„ìš” ì—†ì„ ìˆ˜ ìˆìœ¼ë‚˜, babi_qa ë¡œë”ê°€ ìš”êµ¬í•  ìˆ˜ ìˆìŒ
        # ê°€ì¥ ì•ˆì „í•œ ê²ƒì€ nameì— ëª¨ë“  ì •ë³´ë¥¼ ë‹´ê±°ë‚˜, name="en-10k", task_no="qaX" í˜•íƒœ ì‚¬ìš©
        # ì—¬ê¸°ì„œëŠ” nameì— ëª¨ë“  ì •ë³´ë¥¼ ë‹´ëŠ” ê²ƒìœ¼ë¡œ ê°€ì • (ì‚¬ìš©ì ìš”ì²­)
        # ë§Œì•½ ì˜¤ë¥˜ ì‹œ, name="en-10k", task_no=f"qa{self.task_id}" ì‹œë„

        print(f"Loading bAbI task (name: {hf_name_param}, split: {split})...")

        try:
            dataset = load_dataset("facebook/babi_qa", name=hf_name_param)

            split_mapping = {
                'train': 'train',
                'validation': 'test',
                'test': 'test'
            }
            actual_split = split_mapping.get(split, 'train')

            if actual_split not in dataset:
                available_splits_msg = list(dataset.keys()) if isinstance(dataset, dict) else "N/A"
                raise ValueError(
                    f"âŒ Split '{actual_split}' not found in dataset for {hf_name_param}. Available: {available_splits_msg}.")
            self.raw_data = dataset[actual_split]

        except Exception as e:
            print(f"âŒ HuggingFace ë¡œë”© ì‹¤íŒ¨ (name: {hf_name_param}): {e}")
            print("ğŸ”„ ëŒ€ì²´ ë°©ë²• ì‹œë„ ì¤‘...")
            try:
                fallback_dataset_name = "habanoz/babi_qa_en_valid_10k_qa1"  # task_id 1ì— ëŒ€í•œ ì˜ˆì‹œ
                if self.task_id != 1:
                    print(
                        f"âš ï¸ Fallback dataset {fallback_dataset_name} might not match requested task_id {self.task_id}.")
                dataset_fallback = load_dataset(fallback_dataset_name)
                actual_split_fallback = split_mapping.get(split, 'train')
                self.raw_data = dataset_fallback[
                    actual_split_fallback] if actual_split_fallback in dataset_fallback else dataset_fallback['train']
                print("âœ… ëŒ€ì²´ ë°ì´í„°ì…‹ ë¡œë”© ì„±ê³µ")
            except:
                print("âŒ ëª¨ë“  ì˜¨ë¼ì¸ ì†ŒìŠ¤ ì‹¤íŒ¨")
                print("ğŸ’¡ í•´ê²°ë°©ë²•: ... (ìƒëµ, ì›ë³¸ ì½”ë“œ ì°¸ì¡°)")
                raise Exception("bAbI ë°ì´í„°ì…‹ ë¡œë”© ì‹¤íŒ¨.")

        self.data = self._convert_format()
        print(f"Loaded {len(self.data)} examples")

        self.vocab = self._build_vocab()
        self.word_to_id = {word: i for i, word in enumerate(self.vocab)}
        self.vocab_size = len(self.vocab)
        print(f"Vocabulary size: {self.vocab_size}")

    def _convert_format(self):
        converted_data = []
        for example in self.raw_data:
            story_from_raw = example.get('story', [])
            processed_story_lines = []
            if isinstance(story_from_raw, list):
                for item in story_from_raw:
                    if isinstance(item, dict) and 'text' in item:
                        processed_story_lines.append(item['text'])
                    elif isinstance(item, str):
                        processed_story_lines.append(item)
            elif isinstance(story_from_raw, str):
                processed_story_lines.append(story_from_raw)

            converted_example = {
                'story': processed_story_lines,
                'question': example.get('question', ''),
                'answer': example.get('answer', ''),
                'task': self.task_id
            }
            converted_data.append(converted_example)
        return converted_data

    def _build_vocab(self):
        vocab = set(['<PAD>', '<UNK>', '<SEP>'])
        for example in self.data:
            story_words = ' '.join(example['story']).lower().split()
            question_words = example['question'].lower().split()
            answer_words = example['answer'].lower().split()
            for word in story_words + question_words + answer_words:
                clean_word = re.sub(r'[^\w]', '', word)
                if clean_word:
                    vocab.add(clean_word)
        return ['<PAD>', '<UNK>', '<SEP>'] + sorted(list(vocab - {'<PAD>', '<UNK>', '<SEP>'}))

    def _tokenize(self, text):
        words = re.findall(r'\w+', text.lower())
        token_ids = []
        for word in words:
            token_ids.append(self.word_to_id.get(word, self.word_to_id['<UNK>']))
        return token_ids

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        example = self.data[idx]
        story_text = ' '.join(example['story'])
        question_text = example['question']
        input_text = f"{story_text} <SEP> {question_text}"
        answer_text = example['answer']

        input_ids = self._tokenize(input_text)
        tokenized_answer = self._tokenize(answer_text)
        if not tokenized_answer:
            tokenized_answer = [self.word_to_id['<UNK>']]
        answer_ids = tokenized_answer

        if len(input_ids) > self.max_seq_len - 1:
            input_ids = input_ids[:self.max_seq_len - 1]

        input_length = len(input_ids)
        input_ids += [self.word_to_id['<PAD>']] * (self.max_seq_len - input_length)
        attention_mask = [1] * input_length + [0] * (self.max_seq_len - input_length)

        return {
            'input_ids': torch.tensor(input_ids, dtype=torch.long),
            'attention_mask': torch.tensor(attention_mask, dtype=torch.bool),
            'answer_ids': torch.tensor(answer_ids, dtype=torch.long),  # train_modelì—ì„œ ì²«ë²ˆì§¸ í† í° ì‚¬ìš©
            'answer_text': answer_text
        }