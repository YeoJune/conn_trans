# configs/base_config.py
import math

class BaseConfig:
    """RTX 4090 ìµœì í™” ê¸°ë³¸ ì„¤ì • í´ë˜ìŠ¤"""
    
    # ğŸš€ RTX 4090 ìµœì í™” ëª¨ë¸ ì•„í‚¤í…ì²˜
    d_model = 256           # ìœ ì§€ (ì ë‹¹í•œ í¬ê¸°)
    num_slots = 64          # 128 â†’ 64 (4ë°° ë©”ëª¨ë¦¬ ì ˆì•½)
    bilinear_rank = 16      # 32 â†’ 16 (4ë°° ë©”ëª¨ë¦¬ ì ˆì•½)
    max_reasoning_steps = 4 # 6 â†’ 4 (ë¹ ë¥¸ ìˆ˜ë ´)
    convergence_threshold = 0.01
    
    # ğŸ¯ í›ˆë ¨ ì„¤ì • ìµœì í™”
    learning_rate = 1e-4
    weight_decay = 0.01
    num_epochs = 12         # 20 â†’ 12 (ë¹ ë¥¸ ì‹¤í—˜)
    warmup_ratio = 0.1
    batch_size = 16         # 32 â†’ 16 (ë©”ëª¨ë¦¬ ì ˆì•½)
    gradient_clip = 1.0
    reasoning_cost_weight = 0.001
    gradient_accumulation_steps = 2  # ì‹¤ì§ˆì  batch_size = 32
    
    # í† í¬ë‚˜ì´ì € ì„¤ì •
    tokenizer_name = "t5-base"
    max_seq_len = 256       # 512 â†’ 256 (ë©”ëª¨ë¦¬ ì ˆì•½)
    
    # T5 íŠ¹í™” ì„¤ì •
    decoder_start_token_id = 0
    
    # ë°ì´í„°ì…‹ë³„ task prefix
    task_prefixes = {
        "logiqa": "reason",
        "gsm8k": "solve", 
        "strategyqa": "strategy"
    }
    
    # ğŸ”§ RTX 4090 ìµœì í™” í•˜ë“œì›¨ì–´ ì„¤ì •
    fp16 = True             # Mixed precision í•„ìˆ˜
    gradient_checkpointing = True  # ë©”ëª¨ë¦¬ ì ˆì•½
    num_workers = 8         # 4090ì˜ ë†’ì€ ì²˜ë¦¬ ì†ë„ í™œìš©
    pin_memory = True
    empty_cache_every = 50  # ìì£¼ ìºì‹œ ì •ë¦¬
    
    # ë¡œê¹… ì„¤ì •
    save_every = 500
    eval_every = 200        # ë” ìì£¼ í‰ê°€
    log_every = 20          # ë” ìì£¼ ë¡œê¹…
    
    def update(self, **kwargs):
        """ì„¤ì • ê°’ë“¤ì„ ì—…ë°ì´íŠ¸"""
        for k, v in kwargs.items():
            if hasattr(self, k):
                setattr(self, k, v)
            else:
                print(f"Warning: Unknown config parameter {k}")
        return self
    
    def to_dict(self):
        """ì„¤ì •ì„ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
        return {k: v for k, v in self.__dict__.items() 
                if not k.startswith('_') and not callable(v)}
    
    def get_task_prefix(self, dataset_name):
        """ë°ì´í„°ì…‹ì— ë”°ë¥¸ task prefix ë°˜í™˜"""
        return self.task_prefixes.get(dataset_name, "answer")
    
    def get_estimated_params(self):
        """ì˜ˆìƒ íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚°"""
        N = self.num_slots
        D = self.d_model
        r = self.bilinear_rank
        V = 32128  # T5-base vocab size
        S = self.max_seq_len
        
        # Connection Transformer íŒŒë¼ë¯¸í„°
        bilinear_params = 2 * N * N * D * r  # W_source + W_target
        cross_attn_params = 6 * D * D        # 6ê°œ projection matrices
        embedding_params = (V + S) * D       # token + pos embeddings
        output_params = D * V                # output projection
        layer_norm_params = self.max_reasoning_steps * 2 * D
        
        total = bilinear_params + cross_attn_params + embedding_params + output_params + layer_norm_params
        
        return {
            'bilinear': bilinear_params,
            'cross_attention': cross_attn_params,
            'embeddings': embedding_params,
            'output': output_params,
            'layer_norms': layer_norm_params,
            'total': total
        }
    
    def print_model_info(self):
        """ëª¨ë¸ ì •ë³´ ì¶œë ¥"""
        params = self.get_estimated_params()
        print(f"\nğŸ“Š Model Configuration:")
        print(f"   d_model: {self.d_model}")
        print(f"   num_slots: {self.num_slots}")
        print(f"   bilinear_rank: {self.bilinear_rank}")
        print(f"   max_reasoning_steps: {self.max_reasoning_steps}")
        print(f"\nğŸ”¢ Estimated Parameters:")
        print(f"   Bilinear connections: {params['bilinear']:,}")
        print(f"   Cross-attention: {params['cross_attention']:,}")
        print(f"   Embeddings: {params['embeddings']:,}")
        print(f"   Output projection: {params['output']:,}")
        print(f"   Layer norms: {params['layer_norms']:,}")
        print(f"   Total: {params['total']:,} ({params['total']/1e6:.1f}M)")
        
        # ë©”ëª¨ë¦¬ ì˜ˆìƒ
        memory_gb = params['total'] * 4 / 1e9 * 3  # FP32 ê¸°ì¤€ x3 (grads + optimizer)
        print(f"\nğŸ’¾ Estimated GPU Memory: ~{memory_gb:.1f}GB")
