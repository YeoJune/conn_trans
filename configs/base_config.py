# configs/base_config.py
class BaseConfig:
    """
    Simpleí•œ ê¸°ë³¸ ì„¤ì •
    """
    
    def __init__(self):
        # ëª¨ë¸ ì•„í‚¤í…ì²˜ (Attention Is All You Need ê¸°ë°˜ ì¡°ì •)
        self.d_model = 256          # íš¨ìœ¨ì„±ì„ ìœ„í•´ 256
        self.num_slots = 32         # d_model/8
        self.bilinear_rank = 16     # d_model/16
        self.max_reasoning_steps = 4
        self.num_decoder_layers = 4  # auto_balance()ì—ì„œ ì¡°ì •ë¨
        self.num_heads = 8          # ì›ë…¼ë¬¸ê³¼ ë™ì¼
        self.convergence_threshold = 0.01
        
        # í† í¬ë‚˜ì´ì € (ê¸°ì¡´ ìœ ì§€)
        self.tokenizer_name = "google-t5/t5-base"
        self.vocab_size = None
        self.pad_token_id = None
        
        # ì‹œí€€ìŠ¤ ê¸¸ì´ (ê¸°ì¡´ë³´ë‹¤ ì¦ê°€)
        self.max_seq_len = 256      # ê¸°ì¡´ 128 â†’ 256
        self.answer_max_length = 64 # ê¸°ì¡´ 32 â†’ 64
        
        # í›ˆë ¨ ì„¤ì • (Attention Is All You Need ê¸°ë°˜)
        self.learning_rate = 1e-4
        self.batch_size = 32        # ê¸°ì¡´ 8 â†’ 32
        self.num_epochs = 3
        self.dropout = 0.1          # ê¸°ì¡´ 0.3 â†’ 0.1 (ì›ë…¼ë¬¸)
        self.weight_decay = 0.01    # ê¸°ì¡´ 0.1 â†’ 0.01
        
        # ì •ê·œí™” (ì›ë…¼ë¬¸ ê¸°ë°˜)
        self.orthogonal_weight = 0.01   # ê¸°ì¡´ 0.1 â†’ 0.01
        self.label_smoothing = 0.1      # ì›ë…¼ë¬¸ê³¼ ë™ì¼
        self.gradient_clip = 1.0
        
        # ê¸°íƒ€ (ê¸°ì¡´ ìœ ì§€)
        self.bf16 = True
        self.early_stopping_patience = 3
        self.eval_every = 100       # ê¸°ì¡´ 50 â†’ 100
        
        # ë°ì´í„°ì…‹ë³„ ì„¤ì • (ê¸°ì¡´ ìœ ì§€)
        self.dataset_name = "unknown"
        self.task_prefix = "answer"

    def set_size(self, size):
        """ëª¨ë¸ í¬ê¸° ì„¤ì • (ê¸°ì¡´ ë©”ì„œë“œ + ê°’ ê°œì„ )"""
        sizes = {
            "micro": {
                "d_model": 128, "num_slots": 16, "bilinear_rank": 8,
                "max_reasoning_steps": 3, "num_decoder_layers": 3, "num_heads": 4,
                "max_seq_len": 128, "batch_size": 64, "learning_rate": 3e-4
            },
            "small": {
                "d_model": 192, "num_slots": 24, "bilinear_rank": 12,
                "max_reasoning_steps": 3, "num_decoder_layers": 4, "num_heads": 6,
                "max_seq_len": 256, "batch_size": 48, "learning_rate": 2e-4
            },
            "base": {
                "d_model": 256, "num_slots": 32, "bilinear_rank": 16,
                "max_reasoning_steps": 4, "num_decoder_layers": 4, "num_heads": 8,
                "max_seq_len": 256, "batch_size": 32, "learning_rate": 1e-4
            },
            "large": {
                "d_model": 384, "num_slots": 48, "bilinear_rank": 24,
                "max_reasoning_steps": 5, "num_decoder_layers": 6, "num_heads": 12,
                "max_seq_len": 384, "batch_size": 24, "learning_rate": 5e-5
            }
        }
        
        if size in sizes:
            for key, value in sizes[size].items():
                setattr(self, key, value)
        
        return self

    def set_dataset(self, dataset_name, **kwargs):
        """ë°ì´í„°ì…‹ë³„ ì„¤ì • (ê¸°ì¡´ ë©”ì„œë“œ + ë°ì´í„°ì…‹ íŠ¹ì„± ë°˜ì˜)"""
        self.dataset_name = dataset_name
        
        # ë°ì´í„°ì…‹ë³„ ìµœì í™”ëœ ê¸°ë³¸ê°’
        dataset_defaults = {
            "strategyqa": {
                "task_prefix": "strategy", 
                "answer_max_length": 8, 
                "num_epochs": 5,           # ì‘ì€ ë°ì´í„°ì…‹(2.7K)
                "batch_size": 16,
                "learning_rate": 2e-4      # ì‘ì€ ë°ì´í„°ì…‹ì—” ë†’ì€ lr
            },
            "logiqa": {
                "task_prefix": "reason", 
                "answer_max_length": 16, 
                "num_epochs": 3,
                "max_seq_len": 384,        # ë…¼ë¦¬ ì¶”ë¡ ì—” ê¸´ ì»¨í…ìŠ¤íŠ¸
                "learning_rate": 1.5e-4
            },
            "gsm8k": {
                "task_prefix": "solve", 
                "answer_max_length": 128,   # ìˆ˜í•™ í’€ì´ëŠ” ê¸¸ì–´ì§ˆ ìˆ˜ ìˆìŒ
                "num_epochs": 3,
                "max_seq_len": 512,        # ìˆ˜í•™ ë¬¸ì œ ì„¤ëª…ì´ ê¸¸ ìˆ˜ ìˆìŒ
                "max_reasoning_steps": 6   # ìˆ˜í•™ì€ ë” ë§ì€ ì¶”ë¡  ë‹¨ê³„
            },
            "multinli": {
                "task_prefix": "infer", 
                "answer_max_length": 16, 
                "num_epochs": 2,           # í° ë°ì´í„°ì…‹(433K)
                "batch_size": 64,
                "learning_rate": 5e-5,     # í° ë°ì´í„°ì…‹ì—” ë‚®ì€ lr
                "early_stopping_patience": 2
            }
        }
        
        # ê¸°ë³¸ê°’ ì ìš©
        if dataset_name in dataset_defaults:
            for key, value in dataset_defaults[dataset_name].items():
                setattr(self, key, value)
        
        # ì‚¬ìš©ì ì •ì˜ ì„¤ì • ì ìš© (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
        for key, value in kwargs.items():
            setattr(self, key, value)
        
        return self

    def update(self, **kwargs):
        """ì„¤ì • ì—…ë°ì´íŠ¸ (ê¸°ì¡´ ë©”ì„œë“œ ìœ ì§€)"""
        for key, value in kwargs.items():
            setattr(self, key, value)
        return self

    def auto_balance(self):
        """Connectionê³¼ Baseline íŒŒë¼ë¯¸í„° ìë™ ê· í˜•í™” ğŸ”¥"""
        vocab_size = self.vocab_size or 32000
        
        # Connection Transformer íŒŒë¼ë¯¸í„° ì¶”ì •
        conn_bilinear = 2 * self.num_slots**2 * self.d_model * self.bilinear_rank
        conn_other = 6 * self.d_model**2 + 2 * vocab_size * self.d_model
        conn_total = conn_bilinear + conn_other
        
        # Baseline Transformer íŒŒë¼ë¯¸í„° ì¶”ì •  
        baseline_layers = self.num_decoder_layers * 2  # encoder + decoder
        baseline_attn = baseline_layers * 4 * self.d_model**2
        baseline_ffn = baseline_layers * 2 * self.d_model * (self.d_model * 4)
        baseline_other = 2 * vocab_size * self.d_model
        baseline_total = baseline_attn + baseline_ffn + baseline_other
        
        # íŒŒë¼ë¯¸í„° ì°¨ì´ê°€ 10% ì´ìƒì´ë©´ ì¡°ì •
        diff_ratio = abs(conn_total - baseline_total) / max(conn_total, baseline_total)
        
        if diff_ratio > 0.1:
            if baseline_total > conn_total:
                # Baselineì´ í¬ë©´ ë ˆì´ì–´ ì¤„ì´ê¸°
                target_ratio = conn_total / baseline_total
                self.num_decoder_layers = max(2, int(self.num_decoder_layers * target_ratio**0.5))
            else:
                # Connectionì´ í¬ë©´ bilinear_rank ì¤„ì´ê¸°
                target_ratio = baseline_total / conn_total
                self.bilinear_rank = max(4, int(self.bilinear_rank * target_ratio**0.5))
        
        return self