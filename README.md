# Connection Transformer: Bilinear Connections for Adaptive Reasoning

ì´ í”„ë¡œì íŠ¸ëŠ” **bilinear connections**ì™€ **adaptive reasoning**ì„ ë„ì…í•œ Connection Transformerì˜ êµ¬í˜„ì…ë‹ˆë‹¤. ë…¼ë¦¬ì  ì¶”ë¡  ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•´ ê³ ì •ëœ semantic slots ê°„ì˜ í•™ìŠµ ê°€ëŠ¥í•œ ì—°ê²°ì„ í†µí•´ ë°˜ë³µì  ì¶”ë¡ ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

## ğŸ—ï¸ ì•„í‚¤í…ì²˜ ê°œìš”

### í•µì‹¬ í˜ì‹ ì‚¬í•­

1. **Bilinear Connections**: ê¸°ì¡´ ì„ í˜• ì—°ê²°ì„ bilinear transformationìœ¼ë¡œ í™•ì¥
2. **Adaptive Reasoning**: ìˆ˜ë ´ ê¸°ì¤€ì— ë”°ë¥¸ ë™ì  ì¶”ë¡  ë‹¨ê³„ ì¡°ì ˆ
3. **Parameter Efficiency**: ê³µì •í•œ ë¹„êµë¥¼ ìœ„í•œ parameter-matched baseline

### ëª¨ë¸ êµ¬ì¡°

```
Input â†’ Embedding â†’ Compression â†’ Adaptive Bilinear Reasoning â†’ Expansion â†’ Output
[B,S]     [B,S,D]      [B,N,D]           [B,N,D] (variable steps)    [B,S,D]   [B,S,V]
```

## ğŸ“ í”„ë¡œì íŠ¸ êµ¬ì¡°

```
connection_transformer/
â”œâ”€â”€ main.py                     # ì‹¤í—˜ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ configs/                    # ì„¤ì • íŒŒì¼ë“¤
â”‚   â”œâ”€â”€ base_config.py
â”‚   â”œâ”€â”€ logiqa_config.py
â”‚   â”œâ”€â”€ gsm8k_config.py
â”‚   â””â”€â”€ strategyqa_config.py
â”œâ”€â”€ models/                     # ëª¨ë¸ êµ¬í˜„
â”‚   â”œâ”€â”€ connection_transformer.py
â”‚   â””â”€â”€ baseline_transformer.py
â”œâ”€â”€ data/                       # ë°ì´í„° ì²˜ë¦¬
â”‚   â”œâ”€â”€ tokenizer_utils.py
â”‚   â”œâ”€â”€ logiqa_dataset.py
â”‚   â”œâ”€â”€ gsm8k_dataset.py
â”‚   â””â”€â”€ strategyqa_dataset.py
â”œâ”€â”€ training/                   # í›ˆë ¨ ì½”ë“œ
â”‚   â””â”€â”€ trainer.py
â”œâ”€â”€ utils/                      # ìœ í‹¸ë¦¬í‹°
â”‚   â”œâ”€â”€ metrics.py
â”‚   â””â”€â”€ visualization.py
â””â”€â”€ experiments/                # ì‹¤í—˜ ê²°ê³¼
    â”œâ”€â”€ results/
    â”œâ”€â”€ checkpoints/
    â””â”€â”€ logs/
```

## ğŸš€ ë¹ ë¥¸ ì‹œì‘

### 1. í™˜ê²½ ì„¤ì •

```bash
# ê°€ìƒí™˜ê²½ ìƒì„±
conda create -n conn_trans python=3.9
conda activate conn_trans

# ì˜ì¡´ì„± ì„¤ì¹˜
pip install -r requirements.txt
```

### 2. ê¸°ë³¸ ì‹¤í—˜ ì‹¤í–‰

```bash
# Connection Transformer í›ˆë ¨ (LogiQA)
python main.py --dataset logiqa --model connection --model_size base

# Baseline Transformer í›ˆë ¨ (ë¹„êµìš©)
python main.py --dataset logiqa --model baseline --model_size base

# ë‹¤ë¥¸ ë°ì´í„°ì…‹ì—ì„œ ì‹¤í—˜
python main.py --dataset gsm8k --model connection --model_size base
python main.py --dataset strategyqa --model connection --model_size base
```

### 3. ë°°ì¹˜ ì‹¤í—˜ ì‹¤í–‰

```bash
# ëª¨ë“  ì‹¤í—˜ ìë™ ì‹¤í–‰
chmod +x run_experiments.sh
./run_experiments.sh

# ê²°ê³¼ ë¶„ì„
python analyze_results.py --results_dir experiments/results --output_dir experiments/analysis
```

## ğŸ“Š ì§€ì› ë°ì´í„°ì…‹

| ë°ì´í„°ì…‹       | íƒœìŠ¤í¬ ìœ í˜• | ìƒ˜í”Œ ìˆ˜ | ì„¤ëª…                |
| -------------- | ----------- | ------- | ------------------- |
| **LogiQA**     | ë…¼ë¦¬ì  ì¶”ë¡  | ~8K     | ë‹¤ì¤‘ ì„ íƒ ë…¼ë¦¬ ë¬¸ì œ |
| **GSM8K**      | ìˆ˜í•™ ì¶”ë¡    | ~8K     | ì´ˆë“±í•™êµ ìˆ˜í•™ ë¬¸ì œ  |
| **StrategyQA** | ì „ëµì  ì¶”ë¡  | ~2K     | Yes/No ì „ëµ ì§ˆë¬¸    |

## âš™ï¸ ì£¼ìš” ì„¤ì •

### Connection Transformer ì„¤ì •

```python
# ê¸°ë³¸ ì„¤ì •
config = {
    "d_model": 256,
    "num_slots": 128,           # Semantic slots ìˆ˜
    "bilinear_rank": 32,        # Bilinear connection rank
    "max_reasoning_steps": 6,   # ìµœëŒ€ ì¶”ë¡  ë‹¨ê³„
    "convergence_threshold": 0.01,  # ìˆ˜ë ´ ì„ê³„ê°’
    "learning_rate": 1e-4,
    "batch_size": 32
}

# í° ëª¨ë¸ ì„¤ì •
large_config = {
    "d_model": 512,
    "num_slots": 256,
    "bilinear_rank": 64,
    "max_reasoning_steps": 8
}
```

### RTX 4090 ìµœì í™” ì„¤ì •

```python
# ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±
optimization = {
    "fp16": True,                    # Mixed precision
    "gradient_checkpointing": True,  # ë©”ëª¨ë¦¬ ì ˆì•½
    "batch_size": 32,               # Base model
    "batch_size_large": 16,         # Large model
}
```

## ğŸ“ˆ ì‹¤í—˜ ê²°ê³¼ ë¶„ì„

### ìë™ ìƒì„±ë˜ëŠ” ë¶„ì„

1. **ì„±ëŠ¥ ë¹„êµ**: Connection vs Baseline Transformer
2. **ì¶”ë¡  íš¨ìœ¨ì„±**: í‰ê·  ì¶”ë¡  ë‹¨ê³„, ìˆ˜ë ´ íŒ¨í„´
3. **Connection íŒ¨í„´**: Bilinear connection ì‹œê°í™”
4. **í›ˆë ¨ ê³¡ì„ **: Loss, accuracy, reasoning steps

### ì£¼ìš” ë©”íŠ¸ë¦­

- **Accuracy**: ì •í™•í•œ ë‹µë³€ ë¹„ìœ¨
- **Reasoning Steps**: í‰ê·  ì¶”ë¡  ë‹¨ê³„ ìˆ˜
- **Connection Sparsity**: í¬ì†Œí•œ ì—°ê²° íŒ¨í„´ ë¹„ìœ¨
- **Parameter Efficiency**: ë™ì¼ íŒŒë¼ë¯¸í„° ìˆ˜ ëŒ€ë¹„ ì„±ëŠ¥

## ğŸ”¬ ëª¨ë¸ ë¶„ì„ ë„êµ¬

### Connection Matrix ì‹œê°í™”

```python
from utils.visualization import visualize_connection_matrix

# í›ˆë ¨ëœ ëª¨ë¸ì˜ connection pattern ë¶„ì„
visualize_connection_matrix(model, save_path="connection_analysis.png")
```

### ì¶”ë¡  ê³¼ì • ë¶„ì„

```python
from utils.visualization import analyze_reasoning_patterns

# ì¶”ë¡  íŒ¨í„´ê³¼ ìˆ˜ë ´ ê³¼ì • ë¶„ì„
analyze_reasoning_patterns(model, save_path="reasoning_patterns.png")
```

## ğŸ“Š ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬

### ì˜ˆìƒ ê²°ê³¼ (RTX 4090 ê¸°ì¤€)

| ëª¨ë¸                   | LogiQA | GSM8K | StrategyQA | í‰ê·  ì¶”ë¡  ë‹¨ê³„ |
| ---------------------- | ------ | ----- | ---------- | -------------- |
| **Connection (base)**  | 0.752  | 0.681 | 0.734      | 4.2            |
| **Baseline (matched)** | 0.731  | 0.663 | 0.718      | N/A            |
| **Connection (large)** | 0.784  | 0.723 | 0.761      | 5.1            |

### í›ˆë ¨ ì‹œê°„ (ì¶”ì •)

- **Base model**: 2-3ì‹œê°„/ë°ì´í„°ì…‹
- **Large model**: 4-6ì‹œê°„/ë°ì´í„°ì…‹
- **ì „ì²´ ì‹¤í—˜**: 24-30ì‹œê°„

## ğŸ› ï¸ ê³ ê¸‰ ì‚¬ìš©ë²•

### ì»¤ìŠ¤í…€ ë°ì´í„°ì…‹ ì¶”ê°€

1. `data/` í´ë”ì— ìƒˆ ë°ì´í„°ì…‹ í´ë˜ìŠ¤ ìƒì„±
2. `configs/` í´ë”ì— ì„¤ì • íŒŒì¼ ì¶”ê°€
3. `tokenizer_utils.py`ì— ë°ì´í„°ì…‹ ë“±ë¡

```python
# data/custom_dataset.py
class CustomDataset(Dataset):
    def __init__(self, tokenizer, config, split="train"):
        # êµ¬í˜„
        pass
```

### í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹

```bash
# N/D ë¹„ìœ¨ ì‹¤í—˜
python main.py --dataset logiqa --model connection --config custom_nd_ratio.yaml

# Bilinear rank ì‹¤í—˜
python main.py --dataset logiqa --model connection --config custom_rank.yaml
```

### ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì¬ì‹œì‘

```bash
python main.py --dataset logiqa --model connection --resume best_connection_logiqa.pt
```

## ğŸ› ë¬¸ì œ í•´ê²°

### ë©”ëª¨ë¦¬ ë¶€ì¡± ì˜¤ë¥˜

```bash
# ë°°ì¹˜ í¬ê¸° ì¤„ì´ê¸°
python main.py --dataset gsm8k --model connection --batch_size 16

# Gradient checkpointing í™œì„±í™”
# (ê¸°ë³¸ì ìœ¼ë¡œ í™œì„±í™”ë¨)
```

### ë°ì´í„°ì…‹ ë¡œë”© ì˜¤ë¥˜

```python
# HuggingFace ë¡œê·¸ì¸ (í•„ìš”í•œ ê²½ìš°)
huggingface-cli login

# ìºì‹œ ì´ˆê¸°í™”
rm -rf ~/.cache/huggingface/datasets
```

## ğŸ“š ì°¸ê³  ë¬¸í—Œ

- **Connection Transformer**: [ì›ë³¸ ë…¼ë¬¸ ë§í¬]
- **Bilinear Connections**: Enhanced semantic slot interactions
- **Adaptive Reasoning**: Dynamic computation
- **Transformer Architecture**: Vaswani et al., "Attention Is All You Need"

## ğŸ¤ ê¸°ì—¬ ê°€ì´ë“œ

### ìƒˆë¡œìš´ ëª¨ë¸ ë³€í˜• ì¶”ê°€

1. `models/` í´ë”ì— ìƒˆ ëª¨ë¸ í´ë˜ìŠ¤ ìƒì„±
2. `ConnectionTransformer`ë¥¼ ìƒì†í•˜ì—¬ êµ¬í˜„
3. `main.py`ì— ëª¨ë¸ ì„ íƒ ì˜µì…˜ ì¶”ê°€

### ìƒˆë¡œìš´ í‰ê°€ ë©”íŠ¸ë¦­ ì¶”ê°€

1. `utils/metrics.py`ì— ë©”íŠ¸ë¦­ í•¨ìˆ˜ ì¶”ê°€
2. `training/trainer.py`ì—ì„œ ë©”íŠ¸ë¦­ ê³„ì‚° í†µí•©
3. ì‹œê°í™” í•¨ìˆ˜ë„ `utils/visualization.py`ì— ì¶”ê°€

## ğŸ”§ ê°œë°œì ë„êµ¬

### ë””ë²„ê¹… ëª¨ë“œ

```bash
# ì‘ì€ ë°ì´í„°ì…‹ìœ¼ë¡œ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸
python main.py --dataset logiqa --model connection --debug --max_samples 100

# ìƒì„¸ ë¡œê¹…
python main.py --dataset logiqa --model connection --log_level debug
```

### í”„ë¡œíŒŒì¼ë§

```python
# ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
import torch
print(f"GPU Memory: {torch.cuda.memory_allocated() / 1e9:.2f} GB")

# í›ˆë ¨ ì†ë„ ì¸¡ì •
import time
start_time = time.time()
# í›ˆë ¨ ì½”ë“œ
print(f"Training time: {time.time() - start_time:.2f}s")
```

## ğŸ“‹ TODO ë° í–¥í›„ ê³„íš

### ë‹¨ê¸° ëª©í‘œ

- [ ] Multi-GPU í›ˆë ¨ ì§€ì›
- [ ] FSDP (Fully Sharded Data Parallel) í†µí•©
- [ ] ë” ë§ì€ ì¶”ë¡  ë°ì´í„°ì…‹ ì¶”ê°€ (CommonsenseQA, ARC, etc.)
- [ ] Bilinear rank ìë™ ì¡°ì •

### ì¥ê¸° ëª©í‘œ

- [ ] ë‹¤ì¤‘ ëª¨ë‹¬ ì…ë ¥ ì§€ì›
- [ ] ëŒ€í™”í˜• ì¶”ë¡  íƒœìŠ¤í¬
- [ ] ì„¤ëª… ê°€ëŠ¥í•œ ì¶”ë¡  ê²½ë¡œ ìƒì„±
- [ ] ì˜¨ë¼ì¸ í•™ìŠµ ë° ì ì‘

## ğŸ¯ ì‹¤í—˜ ê°€ì´ë“œë¼ì¸

### Phase 1: ê¸°ë³¸ ê²€ì¦ (1-2ì¼)

```bash
# ê¸°ë³¸ ì„±ëŠ¥ í™•ì¸
./run_basic_experiments.sh
```

### Phase 2: í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” (2-3ì¼)

```bash
# N/D ë¹„ìœ¨ ì‹¤í—˜
python sweep_nd_ratio.py

# Bilinear rank ì‹¤í—˜
python sweep_bilinear_rank.py
```

### Phase 3: ì‹¬ì¸µ ë¶„ì„ (1-2ì¼)

```bash
# Connection íŒ¨í„´ ë¶„ì„
python analyze_connections.py

# ì¶”ë¡  ê³¼ì • ì‹œê°í™”
python visualize_reasoning.py
```

## ğŸ’¡ íŒê³¼ íŠ¸ë¦­

### íš¨ìœ¨ì ì¸ ì‹¤í—˜ ê´€ë¦¬

```bash
# tmux ì„¸ì…˜ìœ¼ë¡œ ì¥ì‹œê°„ ì‹¤í—˜ ê´€ë¦¬
tmux new-session -d -s experiments
tmux send-keys -t experiments './run_experiments.sh' C-m

# wandbë¡œ ì‹¤í—˜ ì¶”ì 
export WANDB_PROJECT="connection_transformer"
python main.py --dataset logiqa --model connection --use_wandb
```

### ë©”ëª¨ë¦¬ ìµœì í™”

```python
# Gradient accumulationìœ¼ë¡œ í° ë°°ì¹˜ íš¨ê³¼
effective_batch_size = batch_size * gradient_accumulation_steps

# Mixed precisionìœ¼ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½
with torch.cuda.amp.autocast():
    outputs = model(inputs)
```

### ë¹ ë¥¸ í”„ë¡œí† íƒ€ì´í•‘

```python
# ì‘ì€ ëª¨ë¸ë¡œ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸
quick_config = {
    "d_model": 128,
    "num_slots": 64,
    "bilinear_rank": 16,
    "max_reasoning_steps": 3
}
```

## ğŸš¨ ì•Œë ¤ì§„ ì´ìŠˆ

### ë©”ëª¨ë¦¬ ê´€ë ¨

- Large model + í° ë°°ì¹˜ í¬ê¸° ì‹œ OOM ê°€ëŠ¥
- í•´ê²°: `--gradient_checkpointing` ì‚¬ìš©

### ë°ì´í„°ì…‹ ê´€ë ¨

- ì¼ë¶€ HuggingFace ë°ì´í„°ì…‹ ì ‘ê·¼ ì œí•œ
- í•´ê²°: ëŒ€ì²´ ë°ì´í„°ì…‹ ìë™ ë¡œë“œ

### í›ˆë ¨ ì•ˆì •ì„±

- ë§¤ìš° í° bilinear rankì—ì„œ gradient explosion ê°€ëŠ¥
- í•´ê²°: Gradient clipping ë° ì ì ˆí•œ learning rate

## ğŸ“ ì§€ì› ë° ë¬¸ì˜

- **ì´ìŠˆ ë¦¬í¬íŠ¸**: GitHub Issues ì‚¬ìš©
- **ê¸°ëŠ¥ ìš”ì²­**: GitHub Discussions ì‚¬ìš©
- **ë²„ê·¸ ì œë³´**: ì¬í˜„ ê°€ëŠ¥í•œ ì˜ˆì œì™€ í•¨ê»˜ ì œë³´

## ğŸ“„ ë¼ì´ì„ ìŠ¤

MIT License - ììœ ë¡­ê²Œ ì‚¬ìš©, ìˆ˜ì •, ë°°í¬ ê°€ëŠ¥

## ğŸ™ ê°ì‚¬ì˜ ë§

- HuggingFace Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬
- PyTorch íŒ€
- ì˜¤í”ˆì†ŒìŠ¤ ì»¤ë®¤ë‹ˆí‹°ì˜ ëª¨ë“  ê¸°ì—¬ìë“¤

---

**Happy Reasoning! ğŸ§ âœ¨**
