# analyze_results.py - ì‹¤í—˜ ê²°ê³¼ ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸
import json
import os
import argparse
import glob
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from utils.visualization import compare_model_performance

def load_results(results_dir):
    """ê²°ê³¼ íŒŒì¼ë“¤ì„ ë¡œë“œ"""
    results = {}
    
    # JSON ê²°ê³¼ íŒŒì¼ë“¤ ì°¾ê¸°
    json_files = glob.glob(os.path.join(results_dir, "results_*.json"))
    
    for json_file in json_files:
        try:
            with open(json_file, 'r') as f:
                data = json.load(f)
            
            # íŒŒì¼ëª…ì—ì„œ ì •ë³´ ì¶”ì¶œ
            filename = os.path.basename(json_file)
            parts = filename.replace('results_', '').replace('.json', '').split('_')
            
            if len(parts) >= 2:
                model_type = parts[0]
                dataset = parts[1]
                timestamp = parts[2] if len(parts) > 2 else "unknown"
                
                key = f"{model_type}_{dataset}"
                results[key] = data
                
        except Exception as e:
            print(f"âš ï¸ Error loading {json_file}: {e}")
    
    return results

def create_comparison_table(results):
    """ë¹„êµ í…Œì´ë¸” ìƒì„±"""
    rows = []
    
    for key, data in results.items():
        model_type = data.get('model_type', 'unknown')
        dataset = data.get('dataset', 'unknown')
        accuracy = data.get('best_accuracy', 0.0)
        
        # Config ì •ë³´ ì¶”ì¶œ
        config = data.get('config', {})
        d_model = config.get('d_model', 'N/A')
        num_slots = config.get('num_slots', 'N/A')
        bilinear_rank = config.get('bilinear_rank', 'N/A')
        
        # Baselineì˜ ê²½ìš° ë ˆì´ì–´ ì •ë³´
        baseline_config = config.get('baseline_config', {})
        num_layers = baseline_config.get('num_layers', 'N/A')
        ffn_mult = baseline_config.get('ffn_multiplier', 'N/A')
        
        rows.append({
            'Model': model_type,
            'Dataset': dataset,
            'Accuracy': accuracy,
            'd_model': d_model,
            'num_slots': num_slots,
            'bilinear_rank': bilinear_rank,
            'num_layers': num_layers,
            'ffn_mult': ffn_mult
        })
    
    return pd.DataFrame(rows)

def analyze_reasoning_efficiency(results):
    """ì¶”ë¡  íš¨ìœ¨ì„± ë¶„ì„"""
    reasoning_data = {}
    
    for key, data in results.items():
        if data.get('model_type') == 'connection':
            dataset = data.get('dataset')
            steps_history = data.get('reasoning_steps_history', [])
            
            if steps_history:
                reasoning_data[dataset] = {
                    'mean_steps': sum(steps_history) / len(steps_history),
                    'final_steps': steps_history[-1] if steps_history else 0,
                    'convergence_trend': steps_history
                }
    
    return reasoning_data

def create_performance_plots(results, output_dir):
    """ì„±ëŠ¥ ë¹„êµ í”Œë¡¯ ìƒì„±"""
    datasets = set(data.get('dataset') for data in results.values())
    
    for dataset in datasets:
        dataset_results = {
            key: data for key, data in results.items() 
            if data.get('dataset') == dataset
        }
        
        if len(dataset_results) >= 2:  # ë¹„êµí•  ëª¨ë¸ì´ 2ê°œ ì´ìƒ
            # ì„±ëŠ¥ ë¹„êµ í”Œë¡¯
            model_accuracies = {
                data.get('model_type', key): {'best_accuracy': data.get('best_accuracy', 0.0)}
                for key, data in dataset_results.items()
            }
            
            try:
                from utils.visualization import compare_model_performance
                compare_model_performance(
                    model_accuracies,
                    save_path=os.path.join(output_dir, f'performance_comparison_{dataset}.png')
                )
            except ImportError:
                print(f"âš ï¸ Visualization not available for {dataset}")

def create_reasoning_analysis_plots(reasoning_data, output_dir):
    """ì¶”ë¡  ë¶„ì„ í”Œë¡¯ ìƒì„±"""
    if not reasoning_data:
        return
    
    try:
        from utils.visualization import plot_reasoning_efficiency
        
        # ëª¨ë“  ë°ì´í„°ì…‹ì˜ ì¶”ë¡  ìŠ¤í…ì„ í•˜ë‚˜ë¡œ í•©ì¹¨
        all_steps = []
        for dataset_data in reasoning_data.values():
            if 'convergence_trend' in dataset_data:
                all_steps.extend(dataset_data['convergence_trend'])
        
        if all_steps:
            plot_reasoning_efficiency(
                all_steps,
                save_path=os.path.join(output_dir, 'reasoning_efficiency.png')
            )
            print(f"ðŸ“Š Reasoning efficiency plot saved")
        
    except ImportError:
        print(f"âš ï¸ Reasoning visualization not available")

def generate_report(df, reasoning_data, output_dir):
    """ê²°ê³¼ ë¦¬í¬íŠ¸ ìƒì„±"""
    report_path = os.path.join(output_dir, 'experiment_report.md')
    
    with open(report_path, 'w') as f:
        f.write("# Connection Transformer Experiment Results\n\n")
        
        # ê°œìš”
        f.write("## ðŸ“‹ Overview\n\n")
        f.write(f"Total experiments: {len(df)}\n")
        f.write(f"Datasets: {', '.join(df['Dataset'].unique())}\n")
        f.write(f"Models: {', '.join(df['Model'].unique())}\n\n")
        
        # ì„±ëŠ¥ ìš”ì•½
        f.write("## ðŸŽ¯ Performance Summary\n\n")
        f.write("| Model | Dataset | Accuracy | Parameters | Notes |\n")
        f.write("|-------|---------|----------|------------|-------|\n")
        
        for _, row in df.iterrows():
            model = row['Model']
            dataset = row['Dataset']
            acc = row['Accuracy']
            
            if model == 'connection':
                params = f"d={row['d_model']}, N={row['num_slots']}, r={row['bilinear_rank']}"
            else:
                params = f"d={row['d_model']}, L={row['num_layers']}, FFN={row['ffn_mult']}x"
            
            f.write(f"| {model} | {dataset} | {acc:.4f} | {params} | |\n")
        
        f.write("\n")
        
        # ë°ì´í„°ì…‹ë³„ ë¹„êµ
        f.write("## ðŸ“Š Dataset-wise Comparison\n\n")
        for dataset in df['Dataset'].unique():
            dataset_df = df[df['Dataset'] == dataset]
            f.write(f"### {dataset}\n\n")
            
            conn_acc = dataset_df[dataset_df['Model'] == 'connection']['Accuracy'].values
            base_acc = dataset_df[dataset_df['Model'] == 'baseline']['Accuracy'].values
            
            if len(conn_acc) > 0 and len(base_acc) > 0:
                improvement = (conn_acc[0] - base_acc[0]) * 100
                f.write(f"- Connection Transformer: **{conn_acc[0]:.4f}**\n")
                f.write(f"- Baseline Transformer: **{base_acc[0]:.4f}**\n")
                f.write(f"- Improvement: **{improvement:+.2f}%**\n\n")
        
        # ì¶”ë¡  íš¨ìœ¨ì„±
        if reasoning_data:
            f.write("## âš¡ Reasoning Efficiency\n\n")
            for dataset, data in reasoning_data.items():
                f.write(f"### {dataset}\n")
                f.write(f"- Average reasoning steps: **{data['mean_steps']:.2f}**\n")
                f.write(f"- Final reasoning steps: **{data['final_steps']:.2f}**\n")
                f.write(f"- Convergence trend: {'Decreasing' if data['convergence_trend'][-1] < data['convergence_trend'][0] else 'Stable'}\n\n")
        
        # ê²°ë¡ 
        f.write("## ðŸ“ Key Findings\n\n")
        
        # ì „ì²´ í‰ê·  ì„±ëŠ¥ ê³„ì‚°
        conn_mean = df[df['Model'] == 'connection']['Accuracy'].mean()
        base_mean = df[df['Model'] == 'baseline']['Accuracy'].mean()
        
        f.write(f"1. **Overall Performance**: Connection Transformer achieves {conn_mean:.4f} vs Baseline {base_mean:.4f}\n")
        f.write(f"2. **Average Improvement**: {(conn_mean - base_mean) * 100:+.2f}%\n")
        
        if reasoning_data:
            avg_steps = sum(d['mean_steps'] for d in reasoning_data.values()) / len(reasoning_data)
            f.write(f"3. **Reasoning Efficiency**: Average {avg_steps:.2f} steps per sample\n")
        
        f.write("4. **Parameter Efficiency**: Fair comparison with matched parameter counts\n")
    
    print(f"ðŸ“‹ Report saved to {report_path}")

def main():
    parser = argparse.ArgumentParser(description="Analyze experiment results")
    parser.add_argument("--results_dir", type=str, required=True,
                       help="Directory containing result JSON files")
    parser.add_argument("--output_dir", type=str, required=True,
                       help="Directory to save analysis outputs")
    
    args = parser.parse_args()
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(args.output_dir, exist_ok=True)
    
    print("ðŸ” Loading experiment results...")
    results = load_results(args.results_dir)
    
    if not results:
        print("âŒ No results found!")
        return
    
    print(f"âœ… Loaded {len(results)} experiment results")
    
    # ë¹„êµ í…Œì´ë¸” ìƒì„±
    print("ðŸ“Š Creating comparison table...")
    df = create_comparison_table(results)
    
    # CSVë¡œ ì €ìž¥
    csv_path = os.path.join(args.output_dir, 'results_comparison.csv')
    df.to_csv(csv_path, index=False)
    print(f"ðŸ’¾ Comparison table saved to {csv_path}")
    
    # ì¶”ë¡  íš¨ìœ¨ì„± ë¶„ì„
    print("âš¡ Analyzing reasoning efficiency...")
    reasoning_data = analyze_reasoning_efficiency(results)
    
    # ì‹œê°í™”
    print("ðŸŽ¨ Creating visualizations...")
    create_performance_plots(results, args.output_dir)
    
    if reasoning_data:
        create_reasoning_analysis_plots(reasoning_data, args.output_dir)
    
    # ë¦¬í¬íŠ¸ ìƒì„±
    print("ðŸ“‹ Generating report...")
    generate_report(df, reasoning_data, args.output_dir)
    
    print("âœ… Analysis completed!")
    print(f"ðŸ“ Check {args.output_dir} for detailed results")

if __name__ == "__main__":
    main()