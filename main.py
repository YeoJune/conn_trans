# main.py
import torch
import argparse
import os
import warnings
from models.connection_transformer import ConnectionTransformer
from models.baseline_transformer import BaselineTransformer, calculate_matching_config
from training.trainer import Trainer
from dataset.tokenizer_utils import get_tokenizer_and_dataset
import configs.logiqa_config as logiqa_cfg
import configs.gsm8k_config as gsm8k_cfg
import configs.strategyqa_config as strategyqa_cfg
import configs.multinli_config as multinli_cfg  # âœ… MultiNLI ì¶”ê°€

def setup_environment():
    """í™˜ê²½ ì„¤ì •"""
    # ê²½ê³  ë¬´ì‹œ
    warnings.filterwarnings("ignore", category=UserWarning)
    
    # GPU ì„¤ì •
    if torch.cuda.is_available():
        torch.backends.cudnn.benchmark = True
        torch.backends.cudnn.deterministic = False
        print(f"ğŸš€ CUDA available: {torch.cuda.get_device_name()}")
        print(f"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
    else:
        print("âš ï¸ CUDA not available, using CPU")
    
    # ì‹œë“œ ì„¤ì •
    torch.manual_seed(42)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(42)

def main():
    parser = argparse.ArgumentParser(description="Connection Transformer Experiments")
    
    # âœ… ì—…ë°ì´íŠ¸ëœ ì„ íƒì§€ë“¤
    parser.add_argument("--dataset", 
                       choices=["logiqa", "gsm8k", "strategyqa", "multinli"], 
                       required=True,
                       help="Dataset to use for training")
    parser.add_argument("--model", 
                       choices=["connection", "baseline"], 
                       required=True,
                       help="Model type to train")
    parser.add_argument("--model_size", 
                       choices=["nano", "micro", "tiny", "small", "base"], 
                       default="micro",  # âœ… ì•ˆì „í•œ ê¸°ë³¸ê°’
                       help="Model size configuration")
    parser.add_argument("--resume", type=str, default=None,
                       help="Path to checkpoint to resume from")
    parser.add_argument("--output_dir", type=str, default="./outputs",
                       help="Directory to save outputs")
    parser.add_argument("--no_save", action="store_true",
                       help="Don't save checkpoints and results")
    
    args = parser.parse_args()
    
    # í™˜ê²½ ì„¤ì •
    setup_environment()
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    if not args.no_save:
        os.makedirs(args.output_dir, exist_ok=True)
        print(f"ğŸ“ Output directory: {args.output_dir}")
    
    # âœ… ì—…ë°ì´íŠ¸ëœ Config ì„ íƒ (MultiNLI ì¶”ê°€)
    config_map = {
        "logiqa": logiqa_cfg,
        "gsm8k": gsm8k_cfg,
        "strategyqa": strategyqa_cfg,
        "multinli": multinli_cfg  # âœ… ìƒˆë¡œ ì¶”ê°€
    }
    
    if args.dataset not in config_map:
        available = list(config_map.keys())
        raise ValueError(f"Unknown dataset: {args.dataset}. Available: {available}")
    
    # âœ… ë°ì´í„°ì…‹ë³„ ê¶Œì¥ ëª¨ë¸ ì‚¬ì´ì¦ˆ ì²´í¬
    recommendations = {
        "strategyqa": ["nano", "micro"],
        "logiqa": ["micro", "tiny"], 
        "gsm8k": ["micro", "tiny"],
        "multinli": ["tiny", "small", "base"]  # í° ë°ì´í„°ì…‹
    }
    
    if args.model_size not in recommendations[args.dataset]:
        recommended = ", ".join(recommendations[args.dataset])
        print(f"âš ï¸ Warning: {args.model_size} model on {args.dataset} may overfit!")
        print(f"   Recommended sizes for {args.dataset}: {recommended}")
        print(f"   Continuing anyway...")
    
    config = config_map[args.dataset].get_config(model_size=args.model_size)
    config.output_dir = args.output_dir
    
    print(f"\nğŸ“‹ Configuration for {args.dataset} ({args.model_size}):")
    print(f"   Model: {args.model}")
    print(f"   d_model: {config.d_model}")
    print(f"   num_slots: {config.num_slots}")
    print(f"   bilinear_rank: {config.bilinear_rank}")
    print(f"   max_reasoning_steps: {config.max_reasoning_steps}")
    print(f"   batch_size: {config.batch_size}")
    print(f"   learning_rate: {config.learning_rate}")
    print(f"   num_epochs: {config.num_epochs}")
    
    # í† í¬ë‚˜ì´ì € & ë°ì´í„°ì…‹ ìƒì„±
    print(f"\nğŸ”„ Loading data and tokenizer...")
    tokenizer, train_dataset, eval_dataset = get_tokenizer_and_dataset(args.dataset, config)
    
    # Configì— vocab_size ì„¤ì • (baseline config ê³„ì‚°ìš©)
    config.vocab_size = tokenizer.vocab_size
    
    # ëª¨ë¸ ìƒì„±
    print(f"\nğŸ—ï¸ Building {args.model} model...")
    
    if args.model == "connection":
        model = ConnectionTransformer(
            vocab_size=tokenizer.vocab_size,
            d_model=config.d_model,
            num_slots=config.num_slots,
            bilinear_rank=config.bilinear_rank,
            max_reasoning_steps=config.max_reasoning_steps,
            convergence_threshold=config.convergence_threshold,
            max_seq_len=config.max_seq_len,
            dropout=getattr(config, 'dropout', 0.1),
            pad_token_id=tokenizer.pad_token_id
        )
        
    elif args.model == "baseline":
        # Calculate matching configuration for fair comparison
        print("\nğŸ” Calculating matching baseline configuration...")
        baseline_config = calculate_matching_config(config)
        
        model = BaselineTransformer(
            vocab_size=tokenizer.vocab_size,
            d_model=config.d_model,
            num_layers=baseline_config['num_layers'],
            ffn_multiplier=baseline_config['ffn_multiplier'],
            max_seq_len=config.max_seq_len,
            dropout=getattr(config, 'dropout', 0.1),
            pad_token_id=tokenizer.pad_token_id
        )
        
        # ì„¤ì •ì— baseline ì •ë³´ ì¶”ê°€
        config.baseline_config = baseline_config
    
    else:
        raise ValueError(f"Unknown model type: {args.model}")
    
    # í›ˆë ¨
    print(f"\nğŸš€ Starting training...")
    print("=" * 70)
    
    trainer = Trainer(model, config, model_type=args.model)
    
    # Tokenizerë¥¼ trainerì— ì„¤ì •
    trainer.set_tokenizer(tokenizer)
    
    best_accuracy = trainer.train(
        train_dataset, 
        eval_dataset, 
        resume_from=args.resume
    )
    
    print(f"\nâœ… Training completed!")
    print(f"   Dataset: {args.dataset}")
    print(f"   Model: {args.model}")
    print(f"   Model size: {args.model_size}")
    print(f"   Best accuracy: {best_accuracy:.4f}")
    
    # ì¶”ê°€ ë¶„ì„ (Connection Transformerë§Œ)
    if args.model == "connection":
        print(f"\nğŸ” Analyzing connection patterns...")
        
        # Connection í†µê³„ ì¶œë ¥ (ì‹œê°í™”ëŠ” ì„ íƒì )
        analysis = model.get_connection_analysis()
        print(f"   Connection Statistics:")
        print(f"     Max strength: {analysis['max_connection']:.4f}")
        print(f"     Mean strength: {analysis['mean_connection']:.4f}")
        print(f"     Sparsity ratio: {analysis['sparsity_ratio']:.2%}")
        if 'orthogonality_quality' in analysis:
            print(f"     Orthogonality quality: {analysis['orthogonality_quality']:.4f}")
        
        # ì‹œê°í™” (utilsê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ)
        if not args.no_save:
            try:
                from utils.visualization import visualize_connection_matrix, analyze_reasoning_patterns
                
                # Connection matrix ì‹œê°í™”
                visualize_connection_matrix(
                    model, 
                    save_path=os.path.join(args.output_dir, f"connection_matrix_{args.dataset}_{args.model_size}.png"),
                    title_suffix=f" ({args.dataset})"
                )
                
                # ì¶”ë¡  íŒ¨í„´ ë¶„ì„
                analyze_reasoning_patterns(
                    model,
                    save_path=os.path.join(args.output_dir, f"reasoning_patterns_{args.dataset}_{args.model_size}.png")
                )
                
                print(f"   ğŸ“Š Visualizations saved to {args.output_dir}")
                
            except ImportError:
                print(f"   âš ï¸ Visualization utils not available, skipping plots")

if __name__ == "__main__":
    main()