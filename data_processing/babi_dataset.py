# data_processing/babi_dataset.py
import torch
from torch.utils.data import Dataset
from datasets import load_dataset
import re
from typing import Dict, List  # typing ì¶”ê°€


class BabiDataset(Dataset):
    """bAbI Task Dataset (Hugging Face Dataset Card ê¸°ì¤€)"""

    def __init__(self, task_id=1, babi_type_prefix="en-10k",  # typeì˜ prefix (ì˜ˆ: "en-10k", "en")
                 split='train', max_seq_len=128):
        self.max_seq_len = max_seq_len
        self.task_id_num = task_id
        # babi_type_prefixì™€ task_idë¥¼ ê²°í•©í•˜ì—¬ ìµœì¢… name íŒŒë¼ë¯¸í„° ìƒì„±
        # ì˜ˆ: babi_type_prefix="en-10k", task_id=1 -> hf_name_param="en-10k-qa1"
        # ì˜ˆ: babi_type_prefix="en", task_id=16 -> hf_name_param="en-qa16"
        hf_name_param = f"{babi_type_prefix}-qa{self.task_id_num}"
        self.split = split

        print(f"Loading bAbI dataset (name: '{hf_name_param}', split: '{self.split}')...")

        try:
            # name íŒŒë¼ë¯¸í„°ì— task_id ì •ë³´ í¬í•¨, task_noëŠ” ì‚¬ìš© ì•ˆ í•¨ ë˜ëŠ” None
            dataset_dict = load_dataset("facebook/babi_qa", name=hf_name_param)  # task_no ì œê±°

            actual_split = self.split
            if self.split == 'validation':
                # en-valid-* íƒ€ì…ì´ ì•„ë‹ˆë©´ validation ìŠ¤í”Œë¦¿ì´ ì—†ì„ ìˆ˜ ìˆìŒ
                if 'validation' not in dataset_dict and 'test' in dataset_dict:
                    print(f"Warning: 'validation' split not found for {hf_name_param}. Using 'test' split instead.")
                    actual_split = 'test'
                elif 'validation' not in dataset_dict:
                    raise ValueError(f"'validation' split (and no 'test' fallback) not found for {hf_name_param}.")

            if actual_split not in dataset_dict:
                available_splits = list(dataset_dict.keys())
                raise ValueError(
                    f"âŒ Split '{actual_split}' not found for bAbI (name: {hf_name_param}). "
                    f"Available splits: {available_splits}."
                )
            self.raw_data_iterable = dataset_dict[actual_split]
            print(f"âœ… Successfully loaded from facebook/babi_qa.")

        except Exception as e:
            print(f"âŒ HuggingFace ë¡œë”© ì‹¤íŒ¨ (name: {hf_name_param}): {e}")
            print("ğŸ”„ ëŒ€ì²´ ë°©ë²• ì‹œë„ ì¤‘...")
            try:
                # ëŒ€ì²´ ë¡œë”© ì‹œì—ë„ task_idì— ë§ëŠ” ë°ì´í„°ì…‹ì„ ì°¾ë„ë¡ ë¡œì§ ê°œì„  í•„ìš”
                # í˜„ì¬ëŠ” ê³ ì •ëœ ëŒ€ì²´ ë°ì´í„°ì…‹ ì‚¬ìš©
                fallback_dataset_name = "habanoz/babi_qa_en_valid_10k_qa1"
                if not (babi_type_prefix == "en-valid-10k" and self.task_id_num == 1):  # ëŒ€ì²´ ë°ì´í„°ì…‹ê³¼ ì¼ì¹˜í•˜ì§€ ì•Šìœ¼ë©´ ê²½ê³ 
                    print(
                        f"âš ï¸ Fallback dataset {fallback_dataset_name} might not match requested name {hf_name_param}.")

                dataset_fallback = load_dataset(fallback_dataset_name)
                actual_split_fallback = self.split
                if self.split == 'validation' and 'validation' not in dataset_fallback and 'test' in dataset_fallback:
                    actual_split_fallback = 'test'

                self.raw_data_iterable = dataset_fallback[
                    actual_split_fallback] if actual_split_fallback in dataset_fallback else dataset_fallback['train']
                print("âœ… ëŒ€ì²´ ë°ì´í„°ì…‹ ë¡œë”© ì„±ê³µ")
            except Exception as e_fallback:
                print(f"âŒ ëª¨ë“  ì˜¨ë¼ì¸ ì†ŒìŠ¤ ì‹¤íŒ¨: {e_fallback}")
                raise Exception("bAbI ë°ì´í„°ì…‹ ë¡œë”© ì‹¤íŒ¨.")

        self.data = self._convert_format()
        print(f"Processed {len(self.data)} QA pairs from the dataset.")

        self.vocab = self._build_vocab()
        self.word_to_id = {word: i for i, word in enumerate(self.vocab)}
        self.vocab_size = len(self.vocab)
        print(f"Vocabulary size: {self.vocab_size}")

    def _convert_format(self):
        converted_qa_pairs = []
        # self.raw_data_iterableì€ Dataset ê°ì²´, ê° ìš”ì†ŒëŠ” í•˜ë‚˜ì˜ bAbI ë¬¸ì œ ì„¸íŠ¸(story ë”•ì…”ë„ˆë¦¬ í¬í•¨)
        for example_set in self.raw_data_iterable:
            story_dict = example_set['story']  # story ìì²´ê°€ ë”•ì…”ë„ˆë¦¬

            # story_dict ì•ˆì˜ 'text', 'type', 'answer'ëŠ” ëª¨ë‘ ê°™ì€ ê¸¸ì´ì˜ ë¦¬ìŠ¤íŠ¸
            num_lines = len(story_dict['text'])

            current_story_context_lines = []
            for i in range(num_lines):
                line_text = story_dict['text'][i]
                line_type = story_dict['type'][i]  # 0: context, 1: question

                if line_type == 0:  # ë¬¸ë§¥
                    current_story_context_lines.append(line_text)
                elif line_type == 1:  # ì§ˆë¬¸
                    question_text = line_text
                    # í•´ë‹¹ ì§ˆë¬¸ ë¼ì¸ì˜ ë‹µë³€ì„ ê°€ì ¸ì˜´
                    answer_text = story_dict['answer'][i]

                    # supporting_idsë„ í•„ìš”í•˜ë©´ ì—¬ê¸°ì„œ ì¶”ì¶œ ê°€ëŠ¥
                    # supporting_fact_indices = [int(sid) -1 for sid in story_dict['supporting_ids'][i] if sid] # 0-based index
                    # supporting_facts = [current_story_context_lines[s_idx] for s_idx in supporting_fact_indices if s_idx < len(current_story_context_lines)]

                    if answer_text:  # ë‹µë³€ì´ ìˆëŠ” ì§ˆë¬¸ë§Œ QA ìŒìœ¼ë¡œ êµ¬ì„±
                        converted_qa_pairs.append({
                            'story': list(current_story_context_lines),  # í˜„ì¬ê¹Œì§€ì˜ ë¬¸ë§¥
                            'question': question_text,
                            'answer': answer_text,
                            'task_id_num': self.task_id_num  # ìˆ«ì task_id
                        })
                    # bAbIëŠ” ê° ì§ˆë¬¸ í›„ ì»¨í…ìŠ¤íŠ¸ê°€ ë¦¬ì…‹ë˜ì§€ ì•Šê³  ì´ì–´ì§ˆ ìˆ˜ ìˆìŒ.
                    # í•˜ì§€ë§Œ ì¼ë°˜ì ì¸ QA ìŒìœ¼ë¡œ ë§Œë“¤ë ¤ë©´, ì§ˆë¬¸ì´ ë‚˜ì˜¬ ë•Œë§ˆë‹¤ ì´ì „ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©.
                    # ë§Œì•½ íƒœìŠ¤í¬ê°€ "í•˜ë‚˜ì˜ ìŠ¤í† ë¦¬ ë¸”ë¡ ë‚´ ì—¬ëŸ¬ QA"ë¼ë©´, current_story_context_linesë¥¼ ì´ˆê¸°í™”í•˜ì§€ ì•ŠìŒ.
                    # ì—¬ê¸°ì„œëŠ” ê° ì§ˆë¬¸ì„ ë…ë¦½ì ì¸ QA ìŒìœ¼ë¡œ ê°„ì£¼ (ì´ì „ ì»¨í…ìŠ¤íŠ¸ ì‚¬ìš©)
        return converted_qa_pairs

    def _build_vocab(self):  # ì´ì „ê³¼ ê±°ì˜ ë™ì¼, self.data êµ¬ì¡° ë³€ê²½ì— ë”°ë¥¸ ì ‘ê·¼ë§Œ ìˆ˜ì •
        vocab = set(['<PAD>', '<UNK>', '<SEP>'])
        for qa_pair in self.data:  # self.dataëŠ” ì´ì œ QA ìŒ ë¦¬ìŠ¤íŠ¸
            story_words = ' '.join(qa_pair['story']).lower().split()
            question_words = qa_pair['question'].lower().split()
            answer_words = qa_pair['answer'].lower().split()
            for word in story_words + question_words + answer_words:
                clean_word = re.sub(r'[^\w]', '', word)
                if clean_word:
                    vocab.add(clean_word)
        return ['<PAD>', '<UNK>', '<SEP>'] + sorted(list(vocab - {'<PAD>', '<UNK>', '<SEP>'}))

    def _tokenize(self, text):  # ì´ì „ê³¼ ë™ì¼
        words = re.findall(r'\w+', text.lower())
        token_ids = []
        for word in words:
            token_ids.append(self.word_to_id.get(word, self.word_to_id['<UNK>']))
        return token_ids

    def __len__(self):
        return len(self.data)  # self.dataëŠ” ì´ì œ QA ìŒì˜ ë¦¬ìŠ¤íŠ¸

    def __getitem__(self, idx):  # ì´ì „ê³¼ ê±°ì˜ ë™ì¼, self.data êµ¬ì¡° ë³€ê²½ì— ë”°ë¥¸ ì ‘ê·¼ë§Œ ìˆ˜ì •
        qa_pair = self.data[idx]

        story_text = ' '.join(qa_pair['story'])
        question_text = qa_pair['question']
        input_text = f"{story_text} <SEP> {question_text}"
        answer_text = qa_pair['answer']

        input_ids = self._tokenize(input_text)
        tokenized_answer = self._tokenize(answer_text)
        if not tokenized_answer:
            tokenized_answer = [self.word_to_id['<UNK>']]
        answer_ids_list = tokenized_answer  # ë³€ìˆ˜ëª… ë³€ê²½

        if len(input_ids) > self.max_seq_len - 1:
            input_ids = input_ids[:self.max_seq_len - 1]

        input_length = len(input_ids)
        padded_input_ids = input_ids + [self.word_to_id['<PAD>']] * (self.max_seq_len - input_length)  # íŒ¨ë”©ëœ input_ids
        attention_mask_bool = [True] * input_length + [False] * (self.max_seq_len - input_length)

        # answer_ids_listëŠ” í† í° IDì˜ ë¦¬ìŠ¤íŠ¸. DataLoaderê°€ ë°°ì¹˜ ë§Œë“¤ ë•Œ íŒ¨ë”©í•´ì¤Œ.
        # train_modelì—ì„œëŠ” answer_ids[:, 0]ì„ ì‚¬ìš©.
        return {
            'input_ids': torch.tensor(padded_input_ids, dtype=torch.long),
            'attention_mask': torch.tensor(attention_mask_bool, dtype=torch.bool),
            'answer_ids': torch.tensor(answer_ids_list, dtype=torch.long),
            'answer_text': answer_text
        }