# training/trainer.py
import torch
import torch.nn as nn
import torch.optim as optim
import time
import math  # math ìž„í¬íŠ¸ ì¶”ê°€ (F.softmaxì—ì„œ ì‚¬ìš©)


# CONFIGë¥¼ ì§ì ‘ ì°¸ì¡°í•˜ëŠ” ëŒ€ì‹ , í•¨ìˆ˜ ì¸ìžë¡œ ë°›ë„ë¡ ìˆ˜ì •
# from configs.base_config import BASE_CONFIG # ë” ì´ìƒ ì§ì ‘ ì°¸ì¡° ì•ˆ í•¨

def train_model(model, train_loader, val_loader, config, device='cuda', model_name="Model"):
    model = model.to(device)

    optimizer = optim.AdamW(
        filter(lambda p: p.requires_grad, model.parameters()),
        lr=config["learning_rate"],
        weight_decay=config["weight_decay"]
    )

    num_training_steps = len(train_loader) * config["max_epochs"]
    # warmup_stepsê°€ num_training_stepsë³´ë‹¤ í¬ì§€ ì•Šë„ë¡ ë³´ìž¥
    actual_warmup_steps = min(config["warmup_steps"], num_training_steps // 10) if num_training_steps > 0 else config[
        "warmup_steps"]
    pct_start_val = float(actual_warmup_steps) / num_training_steps if num_training_steps > 0 else 0.1

    scheduler = optim.lr_scheduler.OneCycleLR(
        optimizer,
        max_lr=config["learning_rate"],
        total_steps=num_training_steps if num_training_steps > 0 else None,  # total_stepsê°€ 0ì´ë©´ ì—ëŸ¬ ë°œìƒ ê°€ëŠ¥
        pct_start=pct_start_val,
    ) if num_training_steps > 0 else None  # num_training_stepsê°€ 0ì´ë©´ ìŠ¤ì¼€ì¤„ëŸ¬ ì‚¬ìš© ì•ˆ í•¨

    best_val_span_acc = 0.0

    print(f"\nðŸš€ Training {model_name} for SQuAD on {device}...")
    print(f"   Total training steps: {num_training_steps if num_training_steps > 0 else 'N/A (no scheduler)'}")
    print("=" * 50)

    for epoch in range(config["max_epochs"]):
        model.train()
        total_train_loss = 0
        train_span_correct = 0
        train_total_samples = 0
        start_time = time.time()

        for batch_idx, batch in enumerate(train_loader):
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            token_type_ids = batch.get('token_type_ids', None)  # SQuADDatasetì—ì„œ ë°˜í™˜
            if token_type_ids is not None: token_type_ids = token_type_ids.to(device)
            start_positions = batch['start_positions'].to(device)
            end_positions = batch['end_positions'].to(device)

            start_logits, end_logits = model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)

            seq_len = input_ids.size(1)
            # SQuADì—ì„œ start/end positionì´ CLS (ì¸ë±ìŠ¤ 0)ì¸ ê²½ìš°ëŠ” ë‹µë³€ì´ ì—†ê±°ë‚˜ ìŠ¤íŒ¬ ë°–ì— ìžˆëŠ” ê²½ìš°.
            # ì´ ê²½ìš° loss ê³„ì‚°ì—ì„œ ì œì™¸í•˜ê±°ë‚˜, CLSë¥¼ ì˜ˆì¸¡í•˜ë„ë¡ í•™ìŠµ. ì—¬ê¸°ì„œëŠ” CLSë„ ì˜ˆì¸¡ ëŒ€ìƒ.
            # ignore_indexëŠ” íŒ¨ë”©ëœ í† í°ì— ëŒ€í•œ ê²ƒì´ ì•„ë‹ˆë¼, íƒ€ê²Ÿ ë ˆì´ë¸” ìžì²´ë¥¼ ë¬´ì‹œí•  ë•Œ ì‚¬ìš©.
            # SQuAD 1.1ì€ í•­ìƒ ë‹µë³€ì´ ìžˆìœ¼ë¯€ë¡œ, start/end_positionsëŠ” ìœ íš¨í•œ í† í° ì¸ë±ìŠ¤ì—¬ì•¼ í•¨.
            # (SQuADDatasetì—ì„œ CLSë¡œ ì„¤ì •ëœ ê²½ìš°ëŠ” í•´ë‹¹ ìŠ¤íŒ¬ì— ë‹µë³€ì´ ì—†ë‹¤ëŠ” ì˜ë¯¸)

            # start_positions.clamp_(0, seq_len - 1) # ë°ì´í„°ì…‹ì—ì„œ ì´ë¯¸ ì²˜ë¦¬ë˜ì—ˆì„ ê²ƒìœ¼ë¡œ ê°€ì •
            # end_positions.clamp_(0, seq_len - 1)

            loss_fct = nn.CrossEntropyLoss()  # ignore_index ì‚¬ìš© ì•ˆ í•¨ (ëª¨ë“  ìœ„ì¹˜ì— ëŒ€í•´ í•™ìŠµ)
            start_loss = loss_fct(start_logits, start_positions)
            end_loss = loss_fct(end_logits, end_positions)
            total_loss = (start_loss + end_loss) / 2
            loss = total_loss

            if hasattr(model, 'C') and model.C is not None and "connection_regularization" in config:
                c_reg = config["connection_regularization"] * torch.norm(model.C, 'fro') ** 2
                loss = loss + c_reg

            if hasattr(model, 'enforce_spectral_radius') and model.training:
                model.enforce_spectral_radius(config.get("spectral_radius_limit"))  # configì—ì„œ ê°€ì ¸ì˜´

            optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), config["gradient_clip"])
            optimizer.step()
            if scheduler: scheduler.step()

            total_train_loss += loss.item() * input_ids.size(0)
            pred_start = torch.argmax(start_logits, dim=1)
            pred_end = torch.argmax(end_logits, dim=1)
            train_span_correct += ((pred_start == start_positions) & (pred_end == end_positions)).sum().item()
            train_total_samples += input_ids.size(0)

            if batch_idx > 0 and batch_idx % (len(train_loader) // 10 if len(train_loader) >= 10 else 1) == 0:
                current_lr = scheduler.get_last_lr()[0] if scheduler else config["learning_rate"]
                print(f"  E{epoch + 1} B{batch_idx}/{len(train_loader)}, Loss: {loss.item():.4f}, LR: {current_lr:.2e}")

        avg_train_loss = total_train_loss / train_total_samples if train_total_samples > 0 else 0
        train_span_acc = train_span_correct / train_total_samples if train_total_samples > 0 else 0

        model.eval()
        val_span_correct = 0
        val_total_samples = 0
        total_val_loss = 0
        with torch.no_grad():
            for batch in val_loader:
                input_ids = batch['input_ids'].to(device)
                attention_mask = batch['attention_mask'].to(device)
                token_type_ids = batch.get('token_type_ids', None)
                if token_type_ids is not None: token_type_ids = token_type_ids.to(device)
                start_positions = batch['start_positions'].to(device)
                end_positions = batch['end_positions'].to(device)

                start_logits, end_logits = model(input_ids, attention_mask=attention_mask,
                                                 token_type_ids=token_type_ids)

                # start_positions.clamp_(0, input_ids.size(1) - 1)
                # end_positions.clamp_(0, input_ids.size(1) - 1)

                loss_fct_val = nn.CrossEntropyLoss()
                start_loss_val = loss_fct_val(start_logits, start_positions)
                end_loss_val = loss_fct_val(end_logits, end_positions)
                total_loss_val = (start_loss_val + end_loss_val) / 2
                total_val_loss += total_loss_val.item() * input_ids.size(0)

                pred_start_val = torch.argmax(start_logits, dim=1)
                pred_end_val = torch.argmax(end_logits, dim=1)
                val_span_correct += ((pred_start_val == start_positions) & (pred_end_val == end_positions)).sum().item()
                val_total_samples += input_ids.size(0)

        avg_val_loss = total_val_loss / val_total_samples if val_total_samples > 0 else 0
        val_span_acc = val_span_correct / val_total_samples if val_total_samples > 0 else 0

        epoch_time = time.time() - start_time
        print(f"  Epoch {epoch + 1}/{config['max_epochs']} ({epoch_time:.1f}s)")
        print(f"    Train Loss: {avg_train_loss:.4f}, Train Span Acc: {train_span_acc:.4f}")
        print(f"    Val Loss:   {avg_val_loss:.4f}, Val Span Acc:   {val_span_acc:.4f}")

        if hasattr(model, 'get_connection_stats'):
            try:
                stats = model.get_connection_stats()
                print(f"    ConnStats: SR(I+C)={stats.get('spectral_radius_I_plus_C', float('nan')):.3f}, "
                      f"Frob(C)={stats.get('frobenius_norm', float('nan')):.3f}")
            except Exception as e_stat_print_epoch:
                print(f"Error printing conn stats epoch: {e_stat_print_epoch}")

        if val_span_acc > best_val_span_acc:
            best_val_span_acc = val_span_acc
            torch.save(model.state_dict(), f'best_squad_model_{model_name.replace(" ", "_")}.pt')
            print(f"    ðŸ’¾ New best SQuAD model saved (Val Span Acc: {best_val_span_acc:.4f})")
        print("-" * 30)

    print(f"âœ… {model_name} SQuAD training completed. Best Val Span Acc: {best_val_span_acc:.4f}")
    return best_val_span_acc