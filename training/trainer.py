# training/trainer.py
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from transformers import get_linear_schedule_with_warmup
import time
import os
import json
from utils.metrics import calculate_accuracy, extract_final_answer
from utils.visualization import plot_training_curves, analyze_reasoning_patterns

class Trainer:
    def __init__(self, model, config, model_type="connection"):
        self.model = model
        self.config = config
        self.model_type = model_type
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.tokenizer = None  # ÎÇòÏ§ëÏóê ÏÑ§Ï†ïÎê®
        
        # Î™®Îç∏ÏùÑ GPUÎ°ú Ïù¥Îèô
        self.model.to(self.device)
        
        # Mixed precision scaler
        self.scaler = torch.cuda.amp.GradScaler() if config.fp16 else None
        
        # Î©îÌä∏Î¶≠ Ï∂îÏ†Å
        self.train_losses = []
        self.eval_accuracies = []
        self.reasoning_steps_history = []
        
        print(f"üöÄ Trainer initialized for {model_type} model on {self.device}")
        if config.fp16:
            print("‚ö° Mixed precision training enabled")
    
    def set_tokenizer(self, tokenizer):
        """ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä ÏÑ§Ï†ï"""
        self.tokenizer = tokenizer
        
        # Î™®Îç∏ÏóêÎèÑ pad_token_id ÏÑ§Ï†ï (ÌïÑÏöîÌïú Í≤ΩÏö∞)
        if hasattr(self.model, 'pad_token_id'):
            self.model.pad_token_id = getattr(tokenizer, 'pad_token_id', 0)
    
    def setup_optimizer_and_scheduler(self, train_loader):
        """ÏòµÌã∞ÎßàÏù¥Ï†ÄÏôÄ Ïä§ÏºÄÏ§ÑÎü¨ ÏÑ§Ï†ï"""
        # ÏòµÌã∞ÎßàÏù¥Ï†Ä
        self.optimizer = optim.AdamW(
            self.model.parameters(),
            lr=self.config.learning_rate,
            weight_decay=self.config.weight_decay,
            betas=(0.9, 0.999),
            eps=1e-8
        )
        
        # Ïä§ÏºÄÏ§ÑÎü¨
        total_steps = len(train_loader) * self.config.num_epochs
        warmup_steps = int(total_steps * self.config.warmup_ratio)
        
        self.scheduler = get_linear_schedule_with_warmup(
            self.optimizer,
            num_warmup_steps=warmup_steps,
            num_training_steps=total_steps
        )
        
        print(f"üìä Training setup:")
        print(f"   Total steps: {total_steps:,}")
        print(f"   Warmup steps: {warmup_steps:,}")
        print(f"   Learning rate: {self.config.learning_rate}")
    
    def train_epoch(self, train_loader, epoch):
        """Ìïú ÏóêÌè≠ ÌõàÎ†®"""
        self.model.train()
        total_loss = 0
        total_reasoning_steps = 0
        num_batches = 0
        
        for batch_idx, batch in enumerate(train_loader):
            # Îç∞Ïù¥ÌÑ∞Î•º GPUÎ°ú Ïù¥Îèô
            input_ids = batch['input_ids'].to(self.device)
            attention_mask = batch['attention_mask'].to(self.device)
            labels = batch['labels'].to(self.device)
            
            # Forward pass with mixed precision
            if self.config.fp16:
                with torch.cuda.amp.autocast():
                    outputs = self.model(input_ids, attention_mask, return_reasoning_trace=True)
                    logits, reasoning_info = outputs
                    
                    # ÏÜêÏã§ Í≥ÑÏÇ∞
                    loss = self.calculate_loss(logits, labels, reasoning_info)
            else:
                outputs = self.model(input_ids, attention_mask, return_reasoning_trace=True)
                logits, reasoning_info = outputs
                loss = self.calculate_loss(logits, labels, reasoning_info)
            
            # Backward pass
            self.optimizer.zero_grad()
            
            if self.config.fp16:
                self.scaler.scale(loss).backward()
                self.scaler.unscale_(self.optimizer)
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.gradient_clip)
                self.scaler.step(self.optimizer)
                self.scaler.update()
            else:
                loss.backward()
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.gradient_clip)
                self.optimizer.step()
            
            self.scheduler.step()
            
            # Î©îÌä∏Î¶≠ ÎàÑÏ†Å
            total_loss += loss.item()
            if hasattr(reasoning_info, 'actual_steps'):
                total_reasoning_steps += reasoning_info['actual_steps']
            num_batches += 1
            
            # Î°úÍπÖ
            if batch_idx % self.config.log_every == 0:
                current_lr = self.scheduler.get_last_lr()[0]
                print(f"  Epoch {epoch} [{batch_idx:4d}/{len(train_loader)}] "
                      f"Loss: {loss.item():.4f} LR: {current_lr:.2e} "
                      f"Steps: {reasoning_info.get('actual_steps', 'N/A')}")
        
        avg_loss = total_loss / num_batches
        avg_reasoning_steps = total_reasoning_steps / num_batches if num_batches > 0 else 0
        
        return avg_loss, avg_reasoning_steps
    
    def calculate_loss(self, logits, labels, reasoning_info):
        """
        ÏÜêÏã§ Ìï®Ïàò Í≥ÑÏÇ∞ - T5 ÌäπÌôî Î≤ÑÏ†Ñ
        """
        # T5 tokenizerÏùò pad_token_id ÏÇ¨Ïö©
        pad_token_id = getattr(self.tokenizer, 'pad_token_id', 0)
        if pad_token_id is None:
            pad_token_id = 0

        # T5 Ïä§ÌÉÄÏùº ÏÜêÏã§ Í≥ÑÏÇ∞ (decoder-onlyÍ∞Ä ÏïÑÎãå seq2seq)
        # logits: [B, S, V], labels: [B, S]
        
        loss_fct = nn.CrossEntropyLoss(ignore_index=pad_token_id, label_smoothing=0.1)
        
        # Flatten for cross entropy
        shift_logits = logits.view(-1, logits.size(-1))  # [B*S, V]
        shift_labels = labels.view(-1).to(logits.device)  # [B*S] ‚Üí logitsÏôÄ ÎèôÏùº ÎîîÎ∞îÏù¥Ïä§Î°ú Ïù¥Îèô

        lm_loss = loss_fct(shift_logits, shift_labels)
        
        # Connection TransformerÏùò Í≤ΩÏö∞ Ï∂îÎ°† ÎπÑÏö© Ï∂îÍ∞Ä
        if self.model_type == "connection" and hasattr(self.model, 'reasoning_cost_loss'):
            reasoning_cost = self.model.reasoning_cost_loss(
                reasoning_info.get('actual_steps', 4),
                target_steps=4,
                weight=self.config.reasoning_cost_weight
            )
            total_loss = lm_loss + reasoning_cost
        else:
            total_loss = lm_loss
        
        return total_loss

    
    def evaluate(self, eval_loader):
        """ÌèâÍ∞Ä"""
        self.model.eval()
        total_loss = 0
        predictions = []
        targets = []
        reasoning_steps_list = []
        
        with torch.no_grad():
            for batch in eval_loader:
                input_ids = batch['input_ids'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)
                labels = batch['labels'].to(self.device)
                
                # Forward pass
                outputs = self.model(input_ids, attention_mask, return_reasoning_trace=True)
                logits, reasoning_info = outputs
                
                # ÏÜêÏã§ Í≥ÑÏÇ∞
                loss = self.calculate_loss(logits, labels, reasoning_info)
                total_loss += loss.item()
                
                # ÏòàÏ∏° ÏÉùÏÑ± (Í∞ÑÎã®Ìïú greedy decoding)
                batch_predictions = self.generate_predictions(input_ids, attention_mask)
                predictions.extend(batch_predictions)
                targets.extend(batch['target_text'])
                
                # Ï∂îÎ°† Ïä§ÌÖù Í∏∞Î°ù
                if 'actual_steps' in reasoning_info:
                    reasoning_steps_list.append(reasoning_info['actual_steps'])
        
        avg_loss = total_loss / len(eval_loader)
        accuracy = calculate_accuracy(predictions, targets)
        avg_reasoning_steps = sum(reasoning_steps_list) / len(reasoning_steps_list) if reasoning_steps_list else 0
        
        return avg_loss, accuracy, avg_reasoning_steps, predictions, targets
    
    def generate_predictions(self, input_ids, attention_mask, max_new_tokens=50):
        """
        ÏòàÏ∏° ÌÖçÏä§Ìä∏ ÏÉùÏÑ± - T5 ÌäπÌôî Î≤ÑÏ†Ñ
        """
        batch_size = input_ids.size(0)
        predictions = []
        
        # T5 tokenizer ÌÜ†ÌÅ∞ IDÎì§
        eos_token_id = getattr(self.tokenizer, 'eos_token_id', 1)
        pad_token_id = getattr(self.tokenizer, 'pad_token_id', 0)
        
        with torch.no_grad():
            for i in range(batch_size):
                # Í∞Å ÏÉòÌîåÏóê ÎåÄÌï¥ Í∞úÎ≥ÑÏ†ÅÏúºÎ°ú ÏÉùÏÑ±
                single_input = input_ids[i:i+1]
                single_mask = attention_mask[i:i+1] if attention_mask is not None else None
                
                # T5Îäî encoder-decoder Íµ¨Ï°∞Ïù¥ÏßÄÎßå, Ïó¨Í∏∞ÏÑúÎäî Îã®ÏàúÌôîÎêú ÏÉùÏÑ± ÏÇ¨Ïö©
                # Ïã§Ï†ú T5ÏóêÏÑúÎäî generate() Î©îÏÑúÎìúÎ•º ÏÇ¨Ïö©Ìï¥Ïïº Ìï®
                
                # Îã®ÏàúÌïú next-token predictionÏúºÎ°ú ÏÉùÏÑ±
                generated_ids = []
                current_input = single_input
                
                for step in range(max_new_tokens):
                    outputs = self.model(current_input, attention_mask=single_mask)
                    if isinstance(outputs, tuple):
                        logits = outputs[0]
                    else:
                        logits = outputs
                    
                    # ÎßàÏßÄÎßâ ÌÜ†ÌÅ∞Ïùò logitsÏóêÏÑú Îã§Ïùå ÌÜ†ÌÅ∞ ÏòàÏ∏°
                    next_token_logits = logits[:, -1, :]
                    next_token = torch.argmax(next_token_logits, dim=-1)
                    next_token_id = next_token.item()
                    
                    # EOS ÌÜ†ÌÅ∞Ïù¥Î©¥ Ï§ëÎã®
                    if next_token_id == eos_token_id:
                        break
                    
                    generated_ids.append(next_token_id)
                    
                    # Îã§Ïùå ÏûÖÎ†• Ï§ÄÎπÑ (ÌòÑÏû¨ Íµ¨ÌòÑÏóêÏÑúÎäî Îã®ÏàúÌôî)
                    break  # Ïã§Ï†úÎ°úÎäî generated tokenÏùÑ appendÌï¥Ïïº Ìï®
                
                # ÎîîÏΩîÎî©
                try:
                    if generated_ids:
                        prediction = self.tokenizer.decode(generated_ids, skip_special_tokens=True)
                    else:
                        # Fallback: ÏûÖÎ†•ÏóêÏÑú ÏßÅÏ†ë ÏòàÏ∏° (Îã®ÏàúÌôî)
                        output_logits = self.model(single_input, single_mask)
                        if isinstance(output_logits, tuple):
                            output_logits = output_logits[0]
                        predicted_token = torch.argmax(output_logits[:, -1, :], dim=-1)
                        prediction = self.tokenizer.decode([predicted_token.item()], skip_special_tokens=True)
                except:
                    prediction = ""
                
                predictions.append(prediction.strip())
        
        return predictions
    
    def train(self, train_dataset, eval_dataset, resume_from=None):
        """Ï†ÑÏ≤¥ ÌõàÎ†® ÌîÑÎ°úÏÑ∏Ïä§"""
        # Îç∞Ïù¥ÌÑ∞ Î°úÎçî ÏÉùÏÑ±
        train_loader = DataLoader(
            train_dataset,
            batch_size=self.config.batch_size,
            shuffle=True,
            num_workers=self.config.num_workers,
            pin_memory=self.config.pin_memory
        )
        
        eval_loader = DataLoader(
            eval_dataset,
            batch_size=self.config.batch_size,
            shuffle=False,
            num_workers=self.config.num_workers,
            pin_memory=self.config.pin_memory
        )
        
        # ÏòµÌã∞ÎßàÏù¥Ï†Ä ÏÑ§Ï†ï
        self.setup_optimizer_and_scheduler(train_loader)
        
        # Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î°úÎìú
        start_epoch = 0
        best_accuracy = 0.0
        
        if resume_from and os.path.exists(resume_from):
            checkpoint = torch.load(resume_from)
            self.model.load_state_dict(checkpoint['model_state_dict'])
            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            start_epoch = checkpoint['epoch'] + 1
            best_accuracy = checkpoint.get('best_accuracy', 0.0)
            print(f"üìÇ Resumed from epoch {start_epoch}, best accuracy: {best_accuracy:.4f}")
        
        print(f"\nüöÄ Starting training for {self.config.num_epochs} epochs")
        print("="*70)
        
        for epoch in range(start_epoch, self.config.num_epochs):
            epoch_start_time = time.time()
            
            # ÌõàÎ†®
            train_loss, avg_reasoning_steps = self.train_epoch(train_loader, epoch)
            
            # ÌèâÍ∞Ä
            eval_loss, accuracy, eval_reasoning_steps, predictions, targets = self.evaluate(eval_loader)
            
            # Î©îÌä∏Î¶≠ Í∏∞Î°ù
            self.train_losses.append(train_loss)
            self.eval_accuracies.append(accuracy)
            self.reasoning_steps_history.append(avg_reasoning_steps)
            
            epoch_time = time.time() - epoch_start_time
            
            print(f"\nEpoch {epoch + 1}/{self.config.num_epochs} ({epoch_time:.1f}s)")
            print(f"  Train Loss: {train_loss:.4f}")
            print(f"  Eval Loss:  {eval_loss:.4f}")
            print(f"  Accuracy:   {accuracy:.4f}")
            if self.model_type == "connection":
                print(f"  Avg Reasoning Steps: {avg_reasoning_steps:.2f}")
            
            # ÏµúÍ≥† ÏÑ±Îä• Î™®Îç∏ Ï†ÄÏû•
            if accuracy > best_accuracy:
                best_accuracy = accuracy
                self.save_checkpoint(epoch, accuracy, is_best=True)
                print(f"  üíæ New best model saved! Accuracy: {best_accuracy:.4f}")
            
            # Ï†ïÍ∏∞ Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Ï†ÄÏû•
            if (epoch + 1) % 5 == 0:
                self.save_checkpoint(epoch, accuracy, is_best=False)
            
            print("-" * 70)
        
        print(f"\n‚úÖ Training completed!")
        print(f"   Best accuracy: {best_accuracy:.4f}")
        
        # ÌõàÎ†® Í≤∞Í≥º ÏãúÍ∞ÅÌôî Î∞è Î∂ÑÏÑù
        self.save_training_results(best_accuracy, predictions[:10], targets[:10])
        
        return best_accuracy
    
    def save_checkpoint(self, epoch, accuracy, is_best=False):
        """Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Ï†ÄÏû•"""
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'accuracy': accuracy,
            'best_accuracy': max(self.eval_accuracies) if self.eval_accuracies else accuracy,
            'config': self.config.to_dict(),
            'train_losses': self.train_losses,
            'eval_accuracies': self.eval_accuracies,
            'reasoning_steps_history': self.reasoning_steps_history
        }
        
        if is_best:
            torch.save(checkpoint, f'best_{self.model_type}_{self.config.dataset_name}.pt')
        else:
            torch.save(checkpoint, f'checkpoint_{self.model_type}_{self.config.dataset_name}_epoch_{epoch}.pt')
    
    def save_training_results(self, best_accuracy, sample_predictions, sample_targets):
        """ÌõàÎ†® Í≤∞Í≥º Ï†ÄÏû•"""
        results = {
            'model_type': self.model_type,
            'dataset': self.config.dataset_name,
            'best_accuracy': best_accuracy,
            'config': self.config.to_dict(),
            'train_losses': self.train_losses,
            'eval_accuracies': self.eval_accuracies,
            'reasoning_steps_history': self.reasoning_steps_history,
            'sample_predictions': sample_predictions,
            'sample_targets': sample_targets,
            'timestamp': time.strftime("%Y%m%d_%H%M%S")
        }
        
        filename = f'results_{self.model_type}_{self.config.dataset_name}_{results["timestamp"]}.json'
        with open(filename, 'w') as f:
            json.dump(results, f, indent=2, default=str)  # default=str for non-serializable objects
        
        print(f"üìä Results saved to {filename}")
        
        # ÏãúÍ∞ÅÌôî
        if len(self.train_losses) > 1:
            plot_training_curves(
                self.train_losses, 
                self.eval_accuracies, 
                self.reasoning_steps_history,
                save_path=f'training_curves_{self.model_type}_{self.config.dataset_name}.png'
            )
        
        # Connection Transformer Î∂ÑÏÑù
        if self.model_type == "connection" and hasattr(self.model, 'get_connection_analysis'):
            analyze_reasoning_patterns(
                self.model,
                save_path=f'reasoning_analysis_{self.config.dataset_name}.png'
            )