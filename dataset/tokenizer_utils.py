# dataset/tokenizer_utils.py

from transformers import T5Tokenizer
from .logiqa_dataset import LogiQADataset
from .gsm8k_dataset import GSM8KDataset
from .strategyqa_dataset import StrategyQADataset
from .multinli_dataset import MultiNLIDataset

def get_tokenizer_and_dataset(dataset_name, config):
    """
    ìµœì‹  T5 í† í¬ë‚˜ì´ì €ì™€ ë°ì´í„°ì…‹ ë¡œë”©
    
    í•µì‹¬ ìˆ˜ì •ì‚¬í•­:
    1. ìµœì‹  T5 í† í¬ë‚˜ì´ì € ì‚¬ìš©ë²• í™•ì¸
    2. as_target_tokenizer() ì—¬ì „íˆ ìœ íš¨ (2024ë…„ ê¸°ì¤€)
    3. DataCollatorForSeq2Seqì™€ í˜¸í™˜ì„± í™•ë³´
    """
    
    print(f"ğŸ”„ Loading T5 tokenizer: {config.tokenizer_name}")
    
    try:
        # ìµœì‹  T5 í† í¬ë‚˜ì´ì € ë¡œë”©
        tokenizer = T5Tokenizer.from_pretrained(
            config.tokenizer_name,
            legacy=False  # ìµœì‹  ë°©ì‹ ì‚¬ìš©
        )
        print(f"âœ… T5Tokenizer loaded successfully")
    except Exception as e:
        print(f"âš ï¸ Latest tokenizer failed, using fallback: {e}")
        tokenizer = T5Tokenizer.from_pretrained(config.tokenizer_name)
    
    # T5 í† í¬ë‚˜ì´ì € ì •ë³´ ì¶œë ¥
    print(f"âœ… Tokenizer info:")
    print(f"   - Vocab size: {tokenizer.vocab_size:,}")
    print(f"   - Pad token: '{tokenizer.pad_token}' (id: {tokenizer.pad_token_id})")
    print(f"   - EOS token: '{tokenizer.eos_token}' (id: {tokenizer.eos_token_id})")
    print(f"   - Extra IDs: {getattr(tokenizer, 'extra_ids', 100)} (for T5 special tokens)")
    
    # ë°ì´í„°ì…‹ í´ë˜ìŠ¤ ë§¤í•‘
    dataset_classes = {
        "logiqa": LogiQADataset,
        "gsm8k": GSM8KDataset,
        "strategyqa": StrategyQADataset,
        "multinli": MultiNLIDataset
    }
    
    if dataset_name not in dataset_classes:
        available = list(dataset_classes.keys())
        raise ValueError(f"Unknown dataset: {dataset_name}. Available: {available}")
    
    dataset_class = dataset_classes[dataset_name]
    
    print(f"ğŸ”„ Loading {dataset_name} dataset with T5 optimization...")
    
    # í›ˆë ¨ ë°ì´í„°ì…‹
    train_dataset = dataset_class(tokenizer, config, split="train")
    
    # ê²€ì¦ ë°ì´í„°ì…‹
    try:
        eval_dataset = dataset_class(tokenizer, config, split="validation")
    except:
        try:
            eval_dataset = dataset_class(tokenizer, config, split="test")
            print("âš ï¸ Using test split as validation")
        except:
            # í›ˆë ¨ ë°ì´í„°ì—ì„œ ë¶„í•  (ë” ì •êµí•œ ë°©ë²•)
            print("âš ï¸ Creating validation from train split")
            train_size = len(train_dataset)
            
            # ë°ì´í„°ì…‹ í¬ê¸°ì— ë”°ë¥¸ ë¶„í•  ë¹„ìœ¨ ì¡°ì •
            if train_size < 1000:
                eval_ratio = 0.2  # ì‘ì€ ë°ì´í„°ì…‹: 20%
            elif train_size < 10000:
                eval_ratio = 0.15  # ì¤‘ê°„ ë°ì´í„°ì…‹: 15%
            else:
                eval_ratio = 0.1   # í° ë°ì´í„°ì…‹: 10%
            
            eval_size = int(train_size * eval_ratio)
            train_size_new = train_size - eval_size
            
            # ë” ì•ˆì „í•œ Subset êµ¬í˜„
            class SafeDatasetSubset:
                def __init__(self, dataset, indices):
                    self.dataset = dataset
                    self.indices = list(indices)
                
                def __len__(self):
                    return len(self.indices)
                
                def __getitem__(self, idx):
                    if idx >= len(self.indices):
                        raise IndexError(f"Index {idx} out of range for subset of size {len(self.indices)}")
                    return self.dataset[self.indices[idx]]
            
            # ì¸ë±ìŠ¤ ë¶„í•  (ë’¤ìª½ì„ evalë¡œ)
            eval_indices = list(range(train_size_new, train_size))
            train_indices = list(range(train_size_new))
            
            eval_dataset = SafeDatasetSubset(train_dataset, eval_indices)
            train_dataset = SafeDatasetSubset(train_dataset, train_indices)
    
    print(f"âœ… Dataset loaded:")
    print(f"   Train: {len(train_dataset):,} examples")
    print(f"   Eval: {len(eval_dataset):,} examples")
    
    # ğŸ” ë°ì´í„° ìƒ˜í”Œ í™•ì¸ (ì¤‘ìš”!)
    print(f"\nğŸ” Data sample verification:")
    try:
        sample = train_dataset[0]
        print(f"   Sample keys: {list(sample.keys())}")
        print(f"   Input shape: {sample['input_ids'].shape}")
        print(f"   Labels shape: {sample['labels'].shape}")
        
        # ì‹¤ì œ í…ìŠ¤íŠ¸ í™•ì¸
        input_text = tokenizer.decode(sample['input_ids'], skip_special_tokens=True)
        print(f"   Input text: '{input_text[:80]}...'")
        print(f"   Target text: '{sample['target_text']}'")
        
        # Labels ê²€ì¦ (ì¤‘ìš”!)
        labels = sample['labels']
        mask_count = (labels == -100).sum().item()
        valid_count = (labels != -100).sum().item()
        print(f"   Labels: {valid_count} valid tokens, {mask_count} masked tokens")
        
        # ì ì¬ì  ë¬¸ì œ ê°ì§€
        if mask_count == 0:
            print("   âš ï¸ WARNING: No masked tokens! This will cause training issues.")
        if valid_count == 0:
            print("   âš ï¸ WARNING: No valid tokens! This will cause training issues.")
        if valid_count < 2:
            print("   âš ï¸ WARNING: Very few valid tokens! Consider longer targets.")
        
        # í† í° ID ë¶„í¬ í™•ì¸
        unique_tokens = torch.unique(labels[labels != -100])
        print(f"   Unique token IDs in labels: {len(unique_tokens)} (sample: {unique_tokens[:5].tolist()})")
        
        if len(unique_tokens) < 2:
            print("   âš ï¸ WARNING: Very few unique tokens in labels!")
            
    except Exception as e:
        print(f"   âŒ Sample verification failed: {e}")
        print("   This might indicate a data preprocessing issue.")
    
    return tokenizer, train_dataset, eval_dataset

def verify_tokenizer_setup(tokenizer, sample_texts=None):
    """í† í¬ë‚˜ì´ì € ì„¤ì • ê²€ì¦ í•¨ìˆ˜"""
    print("\nğŸ” Tokenizer verification:")
    
    if sample_texts is None:
        sample_texts = [
            ("solve: 2 + 2 = ?", "4"),
            ("strategy: Is the sky blue?", "Yes"),
            ("reason: All birds fly. Question: Do penguins fly? A) Yes B) No", "B"),
            ("infer: Premise: It's sunny. Hypothesis: It's bright.", "entailment")
        ]
    
    for input_text, target_text in sample_texts:
        print(f"\n   Testing: '{input_text[:40]}...' -> '{target_text}'")
        
        # ì…ë ¥ í† í¬ë‚˜ì´ì§•
        inputs = tokenizer(
            input_text, 
            max_length=128, 
            padding="max_length", 
            truncation=True, 
            return_tensors="pt"
        )
        
        # íƒ€ê²Ÿ í† í¬ë‚˜ì´ì§• (as_target_tokenizer ì‚¬ìš©)
        with tokenizer.as_target_tokenizer():
            targets = tokenizer(
                target_text,
                max_length=32,
                padding="max_length",
                truncation=True,
                return_tensors="pt"
            )
        
        # Labels ì²˜ë¦¬
        labels = targets.input_ids.clone()
        labels[labels == tokenizer.pad_token_id] = -100
        
        print(f"      Input IDs shape: {inputs.input_ids.shape}")
        print(f"      Labels shape: {labels.shape}")
        print(f"      Masked tokens: {(labels == -100).sum().item()}")
        print(f"      Valid tokens: {(labels != -100).sum().item()}")
        
        # ë””ì½”ë”© í™•ì¸
        decoded_input = tokenizer.decode(inputs.input_ids[0], skip_special_tokens=True)
        valid_labels = labels[0][labels[0] != -100]
        if len(valid_labels) > 0:
            decoded_target = tokenizer.decode(valid_labels, skip_special_tokens=True)
            print(f"      Decoded input: '{decoded_input}'")
            print(f"      Decoded target: '{decoded_target}'")
        else:
            print(f"      âš ï¸ No valid tokens to decode!")
            
    print("\nâœ… Tokenizer verification completed")