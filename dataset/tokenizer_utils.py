# dataset/tokenizer_utils.py
from transformers import T5Tokenizer
from .logiqa_dataset import LogiQADataset
from .gsm8k_dataset import GSM8KDataset
from .strategyqa_dataset import StrategyQADataset

def get_tokenizer_and_dataset(dataset_name, config):
    """
    í† í¬ë‚˜ì´ì €ì™€ ë°ì´í„°ì…‹ì„ í•¨ê»˜ ë°˜í™˜
    
    Args:
        dataset_name: str - "logiqa", "gsm8k", "strategyqa"
        config: ì„¤ì • ê°ì²´
        
    Returns:
        tokenizer, train_dataset, eval_dataset
    """
    
    # T5 í† í¬ë‚˜ì´ì €ë§Œ ì‚¬ìš© (ì—°êµ¬ì—ì„œ ëª…ì‹œëœ ë°©ì‹)
    print(f"ğŸ”„ Loading tokenizer: {config.tokenizer_name}")
    
    try:
        tokenizer = T5Tokenizer.from_pretrained(config.tokenizer_name)
    except ImportError as e:
        if "sentencepiece" in str(e).lower():
            raise ImportError(
                "T5Tokenizer requires SentencePiece. Please install it:\n"
                "pip install sentencepiece>=0.1.97"
            ) from e
        else:
            raise e
    
    # íŒ¨ë”© í† í° í™•ì¸ ë° ì„¤ì •
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
        print(f"   âš ï¸ pad_tokenì´ Noneì´ì–´ì„œ eos_tokenìœ¼ë¡œ ì„¤ì •")
    
    print(f"âœ… Tokenizer loaded. Vocab size: {tokenizer.vocab_size:,}")
    print(f"   Pad token: {tokenizer.pad_token} (id: {tokenizer.pad_token_id})")
    print(f"   EOS token: {tokenizer.eos_token} (id: {tokenizer.eos_token_id})")
    print(f"   UNK token: {tokenizer.unk_token} (id: {tokenizer.unk_token_id})")
    
    # ë°ì´í„°ì…‹ ìƒì„±
    dataset_classes = {
        "logiqa": LogiQADataset,
        "gsm8k": GSM8KDataset,
        "strategyqa": StrategyQADataset
    }
    
    if dataset_name not in dataset_classes:
        raise ValueError(f"Unknown dataset: {dataset_name}. Available: {list(dataset_classes.keys())}")
    
    dataset_class = dataset_classes[dataset_name]
    
    print(f"ğŸ”„ Loading {dataset_name} dataset...")
    train_dataset = dataset_class(tokenizer, config, split="train")
    
    # validation splitì´ ì—†ëŠ” ê²½ìš° test ì‚¬ìš©
    eval_dataset = None
    for split_name in ["validation", "test"]:
        try:
            eval_dataset = dataset_class(tokenizer, config, split=split_name)
            if split_name == "test":
                print("âš ï¸ Validation split not found, using test split")
            break
        except:
            continue
    
    if eval_dataset is None:
        # ìµœí›„ì˜ ìˆ˜ë‹¨: trainì˜ ì‘ì€ ë¶€ë¶„ì„ validationìœ¼ë¡œ ì‚¬ìš©
        print("âš ï¸ No validation/test split found, creating validation from train subset")
        eval_dataset = dataset_class(tokenizer, config, split="train")
        # ì‹¤ì œë¡œëŠ” train datasetì˜ ì¼ë¶€ë§Œ ì‚¬ìš©í•˜ë„ë¡ ìˆ˜ì • í•„ìš”
    
    print(f"âœ… Dataset loaded. Train: {len(train_dataset):,}, Eval: {len(eval_dataset):,}")
    
    return tokenizer, train_dataset, eval_dataset