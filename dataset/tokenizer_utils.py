# dataset/tokenizer_utils.py

from transformers import T5Tokenizer
from .logiqa_dataset import LogiQADataset
from .gsm8k_dataset import GSM8KDataset
from .strategyqa_dataset import StrategyQADataset
from .multinli_dataset import MultiNLIDataset

def get_tokenizer_and_dataset(dataset_name, config):
    """í† í¬ë‚˜ì´ì €ì™€ ë°ì´í„°ì…‹ì„ í•¨ê»˜ ë°˜í™˜"""
    
    # í† í¬ë‚˜ì´ì € ë¡œë”© (ê¸°ì¡´ê³¼ ë™ì¼)
    print(f"ğŸ”„ Loading tokenizer: {config.tokenizer_name}")
    
    try:
        tokenizer = T5Tokenizer.from_pretrained(
            config.tokenizer_name, 
            legacy=False
        )
        print(f"âœ… Using modern T5Tokenizer (legacy=False)")
    except Exception as e:
        print(f"âš ï¸ Modern tokenizer failed, falling back to legacy mode: {e}")
        tokenizer = T5Tokenizer.from_pretrained(config.tokenizer_name)
    
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
        print(f"   âš ï¸ pad_tokenì´ Noneì´ì–´ì„œ eos_tokenìœ¼ë¡œ ì„¤ì •")
    
    print(f"âœ… Tokenizer loaded. Vocab size: {tokenizer.vocab_size:,}")
    print(f"   Pad token: {tokenizer.pad_token} (id: {tokenizer.pad_token_id})")
    print(f"   EOS token: {tokenizer.eos_token} (id: {tokenizer.eos_token_id})")
    
    # âœ… ì—¬ê¸°ê°€ ì‹¤ì œ ë³€ê²½ ë¶€ë¶„ - MultiNLI ì¶”ê°€
    dataset_classes = {
        "logiqa": LogiQADataset,
        "gsm8k": GSM8KDataset,
        "strategyqa": StrategyQADataset,
        "multinli": MultiNLIDataset  # ì´ í•œ ì¤„ë§Œ ì¶”ê°€í•˜ë©´ ë¨
    }
    
    # ë‚˜ë¨¸ì§€ëŠ” ê¸°ì¡´ê³¼ ë™ì¼
    if dataset_name not in dataset_classes:
        available = list(dataset_classes.keys())
        raise ValueError(f"Unknown dataset: {dataset_name}. Available: {available}")
    
    dataset_class = dataset_classes[dataset_name]
    
    print(f"ğŸ”„ Loading {dataset_name} dataset...")
    train_dataset = dataset_class(tokenizer, config, split="train")
    
    try:
        eval_dataset = dataset_class(tokenizer, config, split="validation")
    except:
        try:
            eval_dataset = dataset_class(tokenizer, config, split="test")
            print("âš ï¸ Validation split not found, using test split")
        except:
            print("âš ï¸ No validation/test split, creating validation from train")
            eval_dataset = dataset_class(tokenizer, config, split="train")
    
    print(f"âœ… Dataset loaded. Train: {len(train_dataset):,}, Eval: {len(eval_dataset):,}")
    
    return tokenizer, train_dataset, eval_dataset