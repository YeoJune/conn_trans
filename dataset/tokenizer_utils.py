# dataset/tokenizer_utils.py

from transformers import T5Tokenizer
from .logiqa_dataset import LogiQADataset
from .gsm8k_dataset import GSM8KDataset
from .strategyqa_dataset import StrategyQADataset
from .multinli_dataset import MultiNLIDataset

def get_tokenizer_and_dataset(dataset_name, config):
    """
    T5ì— ìµœì í™”ëœ í† í¬ë‚˜ì´ì €ì™€ ë°ì´í„°ì…‹ ë¡œë”©
    
    ì£¼ìš” ìˆ˜ì • ì‚¬í•­:
    1. as_target_tokenizer() ì‚¬ìš©
    2. labelsì—ì„œ pad_token_id -> -100 ë³€í™˜
    3. ì•ˆì •ì ì¸ ë°ì´í„°ì…‹ ì†ŒìŠ¤ ì‚¬ìš©
    """
    
    print(f"ğŸ”„ Loading T5 tokenizer: {config.tokenizer_name}")
    
    try:
        # T5 í† í¬ë‚˜ì´ì € (ìµœì‹  ë°©ì‹)
        tokenizer = T5Tokenizer.from_pretrained(
            config.tokenizer_name,
            legacy=False,
            use_fast=True  # ë¹ ë¥¸ í† í¬ë‚˜ì´ì € ì‚¬ìš©
        )
        print(f"âœ… T5Tokenizer loaded successfully")
    except Exception as e:
        print(f"âš ï¸ Fast tokenizer failed, falling back: {e}")
        tokenizer = T5Tokenizer.from_pretrained(config.tokenizer_name)
    
    # T5ëŠ” ê¸°ë³¸ì ìœ¼ë¡œ pad_tokenì´ ì„¤ì •ë˜ì–´ ìˆìŒ
    print(f"âœ… Tokenizer loaded. Vocab size: {tokenizer.vocab_size:,}")
    print(f"   Pad token: '{tokenizer.pad_token}' (id: {tokenizer.pad_token_id})")
    print(f"   EOS token: '{tokenizer.eos_token}' (id: {tokenizer.eos_token_id})")
    
    # ë°ì´í„°ì…‹ í´ë˜ìŠ¤ ë§¤í•‘
    dataset_classes = {
        "logiqa": LogiQADataset,
        "gsm8k": GSM8KDataset,
        "strategyqa": StrategyQADataset,
        "multinli": MultiNLIDataset
    }
    
    if dataset_name not in dataset_classes:
        available = list(dataset_classes.keys())
        raise ValueError(f"Unknown dataset: {dataset_name}. Available: {available}")
    
    dataset_class = dataset_classes[dataset_name]
    
    print(f"ğŸ”„ Loading {dataset_name} dataset with T5 optimization...")
    
    # í›ˆë ¨ ë°ì´í„°ì…‹
    train_dataset = dataset_class(tokenizer, config, split="train")
    
    # ê²€ì¦ ë°ì´í„°ì…‹
    try:
        eval_dataset = dataset_class(tokenizer, config, split="validation")
    except:
        try:
            eval_dataset = dataset_class(tokenizer, config, split="test")
            print("âš ï¸ Using test split as validation")
        except:
            # StrategyQAì˜ ê²½ìš° trainì—ì„œ ë¶„í• 
            print("âš ï¸ Creating validation from train split")
            total_size = len(train_dataset)
            eval_size = min(500, total_size // 5)  # ìµœëŒ€ 500ê°œ ë˜ëŠ” 20%
            
            # ê°„ë‹¨í•œ ë¶„í• 
            eval_indices = list(range(total_size - eval_size, total_size))
            train_indices = list(range(total_size - eval_size))
            
            # Subset ìƒì„± (ê°„ë‹¨í•œ êµ¬í˜„)
            class DatasetSubset:
                def __init__(self, dataset, indices):
                    self.dataset = dataset
                    self.indices = indices
                
                def __len__(self):
                    return len(self.indices)
                
                def __getitem__(self, idx):
                    return self.dataset[self.indices[idx]]
            
            eval_dataset = DatasetSubset(train_dataset, eval_indices)
            train_dataset = DatasetSubset(train_dataset, train_indices)
    
    print(f"âœ… Dataset loaded:")
    print(f"   Train: {len(train_dataset):,} examples")
    print(f"   Eval: {len(eval_dataset):,} examples")
    
    # ë°ì´í„° ìƒ˜í”Œ í™•ì¸
    print(f"\nğŸ” Sample data check:")
    sample = train_dataset[0]
    print(f"   Input shape: {sample['input_ids'].shape}")
    print(f"   Labels shape: {sample['labels'].shape}")
    print(f"   Input text: {sample.get('target_text', 'N/A')[:50]}...")
    
    # Labelsì— -100ì´ ì œëŒ€ë¡œ ìˆëŠ”ì§€ í™•ì¸
    labels = sample['labels']
    mask_count = (labels == -100).sum().item()
    print(f"   Labels masked tokens: {mask_count}")
    
    return tokenizer, train_dataset, eval_dataset