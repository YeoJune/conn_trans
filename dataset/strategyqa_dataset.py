# dataset/strategyqa_dataset.py
from datasets import load_dataset
from .base_dataset import BaseReasoningDataset

class StrategyQADataset(BaseReasoningDataset):
    @property
    def dataset_name(self):
        return "StrategyQA"
    
    def _load_raw_data(self):
        """StrategyQA ë°ì´í„° ë¡œë”©"""
        sources = [
            "ChilleD/StrategyQA",
            "wics/strategy-qa"
        ]
        
        for source in sources:
            try:
                dataset = load_dataset(source, split=self.split)
                print(f"âœ… Loaded {source} {self.split}: {len(dataset)} examples")
                return dataset
            except Exception as e:
                print(f"âŒ {source} failed: {str(e)[:50]}...")
                continue
        
        raise RuntimeError("Failed to load StrategyQA")
    
    def _process_item(self, item, idx):
        """StrategyQAë¥¼ ì¶”ë¡  ê°€ëŠ¥í•œ input_textë¡œ ë³€í™˜"""
        
        # 1. ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
        question = item.get('question', '').strip()
        answer = item.get('answer', '')
        decomposition = item.get('decomposition', [])
        evidence = item.get('evidence', [])
        
        if not question:
            return None
        
        # 2. ë‹µë³€ ì •ê·œí™”
        if isinstance(answer, str):
            if answer.lower() in ['yes', 'true', '1']:
                target_text = "Yes"
            elif answer.lower() in ['no', 'false', '0']:
                target_text = "No"
            else:
                target_text = "No"
        elif isinstance(answer, bool):
            target_text = "Yes" if answer else "No"
        else:
            target_text = "No"
        
        # 3. ğŸ”¥ í•µì‹¬: êµ¬ì¡°í™”ëœ input_text ìƒì„±
        input_parts = [f"{self.task_prefix}: {question}"]
        
        # Decompositionì´ ìˆìœ¼ë©´ ì¶”ê°€ (ì¶”ë¡  ë‹¨ê³„)
        if decomposition and len(decomposition) > 0:
            input_parts.append("Reasoning steps:")
            for i, step in enumerate(decomposition, 1):
                input_parts.append(f"{i}. {step}")
        
        # Evidenceê°€ ìˆìœ¼ë©´ ì¶”ê°€ (ì¦ê±°/ë§¥ë½)
        if evidence and len(evidence) > 0:
            input_parts.append("Evidence:")
            for i, fact in enumerate(evidence, 1):
                input_parts.append(f"- {fact}")
        
        # ìµœì¢… ì§ˆë¬¸ ë°˜ë³µ (ëª…í™•ì„±ì„ ìœ„í•´)
        input_parts.append(f"Answer (Yes or No): ")
        
        input_text = " ".join(input_parts)
        
        return {
            'input_text': input_text,
            'target_text': target_text,
            'metadata': {
                'question': question,
                'original_answer': answer,
                'decomposition': decomposition,
                'evidence': evidence,
                'index': idx
            }
        }
    
    def _get_default_answer(self):
        return "No"
    
    def _is_valid_item(self, item):
        """StrategyQA ê²€ì¦ - êµ¬ì¡°í™”ëœ ë°ì´í„° ê¸°ì¤€"""
        if not super()._is_valid_item(item):
            return False
        
        # ë©”íƒ€ë°ì´í„° ê²€ì¦
        metadata = item.get('metadata', {})
        question = metadata.get('question', '')
        
        # ê¸°ë³¸ ìš”êµ¬ì‚¬í•­
        return (
            len(question.split()) >= 3 and  # ìµœì†Œ 3ë‹¨ì–´
            item.get('target_text') in ['Yes', 'No']  # ìœ íš¨í•œ ë‹µë³€
        )
    
    def verify_split_integrity(self):
        """ë°ì´í„° ê²€ì¦ - êµ¬ì¡° í’ˆì§ˆ í™•ì¸"""
        print(f"\nğŸ” StrategyQA {self.split} Split Verification")
        print(f"ğŸ“Š Total samples: {len(self.data)}")
        
        # êµ¬ì¡° ë¶„ì„
        has_decomposition = 0
        has_evidence = 0
        answers = []
        input_lengths = []
        
        sample_size = min(50, len(self.data))
        
        for i in range(sample_size):
            try:
                raw_item = self.data[i]  # ì›ë³¸ ë°ì´í„°
                processed_item = self.__getitem__(i)  # ì²˜ë¦¬ëœ ë°ì´í„°
                
                # êµ¬ì¡° ì •ë³´ ìˆ˜ì§‘
                decomposition = raw_item.get('metadata', {}).get('decomposition', [])
                evidence = raw_item.get('metadata', {}).get('evidence', [])
                
                if decomposition and len(decomposition) > 0:
                    has_decomposition += 1
                if evidence and len(evidence) > 0:
                    has_evidence += 1
                
                answers.append(processed_item['target_text'])
                input_lengths.append(len(processed_item['input_text']))
                
            except Exception as e:
                print(f"âŒ Error in item {i}: {e}")
        
        from collections import Counter
        answer_dist = Counter(answers)
        
        print(f"ğŸ¯ Answer distribution: {answer_dist}")
        print(f"ğŸ§© Decomposition coverage: {has_decomposition}/{sample_size} ({has_decomposition/sample_size*100:.1f}%)")
        print(f"ğŸ“š Evidence coverage: {has_evidence}/{sample_size} ({has_evidence/sample_size*100:.1f}%)")
        print(f"ğŸ“ Avg input length: {sum(input_lengths)/len(input_lengths):.0f} chars")
        
        # ìƒ˜í”Œ ì¶œë ¥ (êµ¬ì¡° í™•ì¸)
        print(f"\nğŸ“‹ Sample inputs:")
        for i in range(min(2, len(self.data))):
            processed_item = self.__getitem__(i)
            print(f"\nSample {i+1}:")
            print(f"Input: {processed_item['input_text'][:200]}...")
            print(f"Target: {processed_item['target_text']}")
        
        # í’ˆì§ˆ í‰ê°€
        if has_decomposition < sample_size * 0.5:
            print("âš ï¸ WARNING: Low decomposition coverage - model may not learn multi-step reasoning")
        
        if has_evidence < sample_size * 0.5:
            print("âš ï¸ WARNING: Low evidence coverage - model may not learn fact-based reasoning")
        
        yes_ratio = answer_dist.get('Yes', 0) / max(sum(answer_dist.values()), 1)
        if 0.2 <= yes_ratio <= 0.8:
            print("âœ… Balanced Yes/No distribution")
        else:
            print(f"âš ï¸ Imbalanced distribution (Yes: {yes_ratio:.2f})")
        
        return {
            'answer_distribution': answer_dist,
            'decomposition_coverage': has_decomposition / sample_size,
            'evidence_coverage': has_evidence / sample_size,
            'avg_input_length': sum(input_lengths) / len(input_lengths)
        }