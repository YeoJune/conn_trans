# dataset/strategyqa_dataset.py
import torch
from torch.utils.data import Dataset
from datasets import load_dataset

class StrategyQADataset(Dataset):
    """StrategyQA Dataset - Yes/No ì§ˆë¬¸ (í˜¸í™˜ì„± ê°œì„ )"""
    
    def __init__(self, tokenizer, config, split="train"):
        self.tokenizer = tokenizer
        self.config = config
        self.max_length = config.max_seq_len
        self.answer_max_length = getattr(config, 'answer_max_length', 16)
        self.task_prefix = getattr(config, 'task_prefix', 'strategy')
        
        print(f"ğŸ“¦ Loading StrategyQA dataset ({split} split)...")
        
        # ê²€ì¦ëœ ë°ì´í„°ì…‹ ì‚¬ìš©
        try:
            # wics/strategy-qaê°€ ê°€ì¥ ì•ˆì •ì ì´ê³  test splitë§Œ ìˆìŒ
            dataset = load_dataset("wics/strategy-qa", split="test")
            print(f"âœ… Successfully loaded from wics/strategy-qa")
            print(f"   Raw dataset size: {len(dataset)}")
            
            # train/eval ë¶„í•  (datasetì´ testë§Œ ìˆìœ¼ë¯€ë¡œ)
            total_size = len(dataset)
            if split == "train":
                # ì²˜ìŒ 80%ë¥¼ trainìœ¼ë¡œ
                end_idx = int(total_size * 0.8)
                dataset = dataset.select(range(end_idx))
            else:
                # ë‚˜ë¨¸ì§€ 20%ë¥¼ evalë¡œ
                start_idx = int(total_size * 0.8)
                dataset = dataset.select(range(start_idx, total_size))
                
        except Exception as e:
            print(f"âŒ StrategyQA loading failed: {e}")
            raise RuntimeError("Failed to load StrategyQA dataset")
        
        # ì „ì²˜ë¦¬ ë° ê²€ì¦
        self.data = self._preprocess_and_validate(dataset)
        print(f"StrategyQA {split}: {len(self.data)} examples (after validation)")
    
    def _preprocess_and_validate(self, dataset):
        """ì „ì²˜ë¦¬ ë° ë°ì´í„° ê²€ì¦"""
        processed = []
        skipped = 0
        
        for i, item in enumerate(dataset):
            try:
                # í•„ë“œ ì¶”ì¶œ
                question = item['question'].strip()
                answer = item['answer']  # boolean ê°’
                
                # ê²€ì¦: ë¹ˆ ì§ˆë¬¸ ê±´ë„ˆë›°ê¸°
                if not question:
                    skipped += 1
                    continue
                
                # ë„ˆë¬´ ê¸´ ì§ˆë¬¸ ê±´ë„ˆë›°ê¸°
                if len(question) > 400:
                    skipped += 1
                    continue
                
                # ì…ë ¥ êµ¬ì„±
                input_text = f"{self.task_prefix}: {question}"
                
                # ì¶œë ¥: "Yes" ë˜ëŠ” "No"
                target_text = "Yes" if answer else "No"
                
                processed.append({
                    'input_text': input_text,
                    'target_text': target_text,
                    'question': question,
                    'original_answer': answer,
                    'index': i
                })
                
            except Exception as e:
                print(f"   âš ï¸ Error processing item {i}: {e}")
                skipped += 1
                continue
        
        if skipped > 0:
            print(f"   âš ï¸ Skipped {skipped} invalid examples")
        
        # ìƒ˜í”Œ ê²€ì¦
        if len(processed) > 0:
            self._validate_samples(processed[:3])
        
        return processed
    
    def _validate_samples(self, samples):
        """ìƒ˜í”Œ ë°ì´í„° ê²€ì¦"""
        print(f"   ğŸ” Validating {len(samples)} samples:")
        
        for i, sample in enumerate(samples):
            print(f"      Sample {i+1}:")
            print(f"         Input: '{sample['input_text'][:60]}...'")
            print(f"         Target: '{sample['target_text']}'")
            
            # í† í¬ë‚˜ì´ì§• í…ŒìŠ¤íŠ¸
            try:
                inputs = self.tokenizer(
                    sample['input_text'],
                    max_length=self.max_length,
                    padding='max_length',
                    truncation=True,
                    return_tensors='pt'
                )
                
                with self.tokenizer.as_target_tokenizer():
                    targets = self.tokenizer(
                        sample['target_text'],
                        max_length=self.answer_max_length,
                        padding='max_length',
                        truncation=True,
                        return_tensors='pt'
                    )
                
                labels = targets.input_ids.clone()
                labels[labels == self.tokenizer.pad_token_id] = -100
                
                valid_tokens = (labels != -100).sum().item()
                print(f"         Tokenization: âœ… ({valid_tokens} valid tokens)")
                
                if valid_tokens == 0:
                    print(f"         âš ï¸ WARNING: No valid tokens!")
                
            except Exception as e:
                print(f"         Tokenization: âŒ {e}")
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        """í˜¸í™˜ì„± ê°œì„ ëœ ë°ì´í„° ë°˜í™˜"""
        if idx >= len(self.data):
            raise IndexError(f"Index {idx} out of range for dataset of size {len(self.data)}")
        
        item = self.data[idx]
        
        try:
            # ì…ë ¥ í† í¬ë‚˜ì´ì§•
            inputs = self.tokenizer(
                item['input_text'],
                max_length=self.max_length,
                padding='max_length',
                truncation=True,
                return_tensors='pt'
            )
            
            # T5 íƒ€ê²Ÿ í† í¬ë‚˜ì´ì§•
            with self.tokenizer.as_target_tokenizer():
                targets = self.tokenizer(
                    item['target_text'],
                    max_length=self.answer_max_length,
                    padding='max_length',
                    truncation=True,
                    return_tensors='pt'
                )
            
            # Labels ì²˜ë¦¬: -100ìœ¼ë¡œ ë§ˆìŠ¤í‚¹
            labels = targets.input_ids.squeeze().clone()
            labels[labels == self.tokenizer.pad_token_id] = -100
            
            return {
                'input_ids': inputs.input_ids.squeeze(),
                'attention_mask': inputs.attention_mask.squeeze(),
                'labels': labels,
                'target_text': item['target_text'],
                'question': item['question']
            }
            
        except Exception as e:
            print(f"âš ï¸ Error in __getitem__ for index {idx}: {e}")
            # ì•ˆì „í•œ ê¸°ë³¸ê°’ ë°˜í™˜
            return {
                'input_ids': torch.zeros(self.max_length, dtype=torch.long),
                'attention_mask': torch.zeros(self.max_length, dtype=torch.long),
                'labels': torch.full((self.answer_max_length,), -100, dtype=torch.long),
                'target_text': "No",
                'question': "Error loading question"
            }