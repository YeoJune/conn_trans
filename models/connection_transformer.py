# models/connection_transformer.py
import torch
import torch.nn as nn
import torch.nn.functional as F
import math

class ConnectionTransformer(nn.Module):
    """
    Connection Transformer with Encoder-Decoder Architecture
    """
    
    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=256, num_slots=128,
                 bilinear_rank=32, max_reasoning_steps=6,
                 convergence_threshold=0.01, max_seq_len=512,
                 dropout=0.1, src_pad_token_id=0, tgt_pad_token_id=0,
                 num_decoder_layers=6, num_heads=8):
        super().__init__()
        
        self.d_model = d_model
        self.num_slots = num_slots
        self.bilinear_rank = bilinear_rank
        self.max_reasoning_steps = max_reasoning_steps
        self.convergence_threshold = convergence_threshold
        self.src_pad_token_id = src_pad_token_id
        self.tgt_pad_token_id = tgt_pad_token_id
        self.num_heads = num_heads
        
        # === ENCODER COMPONENTS ===
        # Source embeddings
        self.src_token_embedding = nn.Embedding(src_vocab_size, d_model, padding_idx=src_pad_token_id)
        self.src_pos_embedding = nn.Embedding(max_seq_len, d_model)
        
        # Fixed semantic slots (H) - orthogonal initialization
        self.register_buffer('H', self._create_orthogonal_slots(num_slots, d_model))
        
        # Bilinear connection matrices - will be orthogonally initialized
        self.W_source = nn.Parameter(torch.zeros(num_slots, num_slots, bilinear_rank))
        self.W_target = nn.Parameter(torch.zeros(num_slots, num_slots, bilinear_rank))
        
        # Encoder cross-attention projection matrices
        self.W_q_input = nn.Linear(d_model, d_model, bias=False)
        self.W_k_slots = nn.Linear(d_model, d_model, bias=False)
        self.W_v_input = nn.Linear(d_model, d_model, bias=False)
        
        # Layer normalization for reasoning steps
        self.reasoning_norms = nn.ModuleList([
            nn.LayerNorm(d_model) for _ in range(max_reasoning_steps)
        ])
        
        # === DECODER COMPONENTS ===
        # Target embeddings
        self.tgt_token_embedding = nn.Embedding(tgt_vocab_size, d_model, padding_idx=tgt_pad_token_id)
        self.tgt_pos_embedding = nn.Embedding(max_seq_len, d_model)
        
        # Decoder layers
        decoder_layer = nn.TransformerDecoderLayer(
            d_model=d_model,
            nhead=num_heads,
            dim_feedforward=d_model * 4,
            dropout=dropout,
            activation='gelu',
            batch_first=True,
            norm_first=True
        )
        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_decoder_layers)
        
        # Output projection
        self.output_norm = nn.LayerNorm(d_model)
        self.output_projection = nn.Linear(d_model, tgt_vocab_size, bias=False)
        
        # Dropout
        self.dropout = nn.Dropout(dropout)
        
        # Orthogonal regularization parameters
        self.orthogonal_weight = 0.01
        
        self._init_parameters()
        
        # Report parameter count
        total_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
        print(f"ğŸ”¹ Connection Transformer (Encoder-Decoder): {total_params:,} parameters")
        print(f"   - Encoder slots & bilinear: {self.W_source.numel() + self.W_target.numel():,}")
        print(f"   - Decoder layers: {num_decoder_layers}")

    def _create_orthogonal_slots(self, num_slots, d_model):
        """Create orthogonal semantic slots for independent semantic spaces."""
        if num_slots <= d_model:
            Q, _ = torch.qr(torch.randn(d_model, num_slots))
            H = Q.T  # (num_slots, d_model)
        else:
            H = torch.zeros(num_slots, d_model)
            for start in range(0, num_slots, d_model):
                end = min(start + d_model, num_slots)
                group_size = end - start
                Q, _ = torch.qr(torch.randn(d_model, group_size))
                H[start:end] = Q.T
        
        return H
    
    def _init_parameters(self):
        """Orthogonal initialization for all parameters"""
        # Source embeddings
        nn.init.normal_(self.src_token_embedding.weight, std=0.02)
        nn.init.normal_(self.src_pos_embedding.weight, std=0.02)
        
        # Target embeddings
        nn.init.normal_(self.tgt_token_embedding.weight, std=0.02)
        nn.init.normal_(self.tgt_pos_embedding.weight, std=0.02)
        
        # Padding token embeddingsì„ 0ìœ¼ë¡œ ì„¤ì •
        if self.src_pad_token_id is not None:
            with torch.no_grad():
                self.src_token_embedding.weight[self.src_pad_token_id].fill_(0)
        
        if self.tgt_pad_token_id is not None:
            with torch.no_grad():
                self.tgt_token_embedding.weight[self.tgt_pad_token_id].fill_(0)
        
        # Bilinear connections - Orthogonal initialization
        self._orthogonal_init_bilinear()
        
        # Encoder cross-attention projections
        for module in [self.W_q_input, self.W_k_slots, self.W_v_input]:
            nn.init.orthogonal_(module.weight)
        
        # Output projection
        nn.init.orthogonal_(self.output_projection.weight)
    
    def _orthogonal_init_bilinear(self):
        """ê°„ì†Œí™”ëœ orthogonal ì´ˆê¸°í™”"""
        with torch.no_grad():
            for i in range(self.num_slots):
                for j in range(self.num_slots):
                    if i != j:  # Only non-self connections
                        # ë‹¨ìˆœí•œ orthogonal ì´ˆê¸°í™”
                        nn.init.orthogonal_(self.W_source[i, j].unsqueeze(0))
                        nn.init.orthogonal_(self.W_target[i, j].unsqueeze(0))
    
    def bilinear_transform(self, H_state):
        """ì²­í¬ ë‹¨ìœ„ë¡œ einsum ì²˜ë¦¬"""
        # ì—°ê²° ê°•ë„ ê³„ì‚°: [N, N, r] * [N, N, r] -> [N, N]
        connection_matrix = torch.sum(self.W_source * self.W_target, dim=-1)  # [N, N]
        
        # ìê¸° ì—°ê²° ì œê±°
        connection_matrix.fill_diagonal_(0.0)
        
        batch_size, num_slots, d_model = H_state.shape
        
        # einsum ì²­í¬ í¬ê¸° ê²°ì • (ì œí•œ: 128 ë¯¸ë§Œ)
        chunk_size = min(127, num_slots)  # einsum ì œí•œ ê³ ë ¤
        
        if num_slots <= 127:
            # í•œ ë²ˆì— ì²˜ë¦¬ ê°€ëŠ¥
            influence = torch.einsum('bid,ij->bjd', H_state, connection_matrix)
        else:
            # ì²­í¬ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬
            influence = torch.zeros_like(H_state)
            
            for j_start in range(0, num_slots, chunk_size):
                j_end = min(j_start + chunk_size, num_slots)
                
                # j_start:j_end ìŠ¬ë¡¯ë“¤ì´ ë°›ëŠ” ì˜í–¥ ê³„ì‚°
                # ëª¨ë“  i ìŠ¬ë¡¯ìœ¼ë¡œë¶€í„°ì˜ ê¸°ì—¬ í•©ì‚°
                chunk_influence = torch.einsum(
                    'bid,ij->bjd', 
                    H_state,  # [B, N, D] - ëª¨ë“  ì†ŒìŠ¤ ìŠ¬ë¡¯
                    connection_matrix[:, j_start:j_end]  # [N, chunk_size] - íƒ€ê²Ÿ ì²­í¬ë¡œì˜ ì—°ê²°
                )
                
                influence[:, j_start:j_end, :] = chunk_influence
        
        return influence

    def encode(self, src_input_ids, src_attention_mask=None, return_reasoning_trace=False):
        """
        Encoder: Input tokens â†’ Semantic slots with bilinear reasoning
        
        Args:
            src_input_ids: [batch_size, src_seq_len]
            src_attention_mask: [batch_size, src_seq_len]
            return_reasoning_trace: bool
            
        Returns:
            semantic_slots: [batch_size, num_slots, d_model]
            reasoning_info: dict (optional)
        """
        batch_size, src_seq_len = src_input_ids.shape
        device = src_input_ids.device
        
        # === STEP 1: SOURCE INPUT PROCESSING ===
        positions = torch.arange(src_seq_len, device=device).unsqueeze(0).expand(batch_size, -1)
        X_src = self.src_token_embedding(src_input_ids) + self.src_pos_embedding(positions)
        X_src = self.dropout(X_src)
        
        # === STEP 2: INPUT â†’ SEMANTIC SLOT COMPRESSION ===
        Q_input = self.W_q_input(X_src)
        K_slots = self.W_k_slots(self.H)
        V_input = self.W_v_input(X_src)
        
        A_compress = F.softmax(Q_input @ K_slots.T / math.sqrt(self.d_model), dim=-1)
        
        if src_attention_mask is not None:
            mask_expanded = src_attention_mask.unsqueeze(-1).float()
            A_compress = A_compress * mask_expanded
            A_compress = A_compress / (A_compress.sum(dim=1, keepdim=True) + 1e-8)
        
        IR_activation = A_compress.transpose(-1, -2) @ V_input
        H_state = self.H.unsqueeze(0).expand(batch_size, -1, -1) + IR_activation
        
        reasoning_trace = [H_state.clone()] if return_reasoning_trace else []
        
        # === STEP 3: BILINEAR REASONING ===
        actual_steps = 0
        final_change_magnitude = torch.zeros(batch_size, self.num_slots, device=device)
        
        for step in range(self.max_reasoning_steps):
            influence = self.bilinear_transform(H_state)
            
            step_update = F.relu(influence)
            H_state = H_state + step_update
            H_state = self.reasoning_norms[step](H_state)
            
            if return_reasoning_trace:
                reasoning_trace.append(H_state.clone())
            
            change_magnitude = torch.norm(step_update, dim=-1)
            final_change_magnitude = change_magnitude
            
            converged_mask = change_magnitude <= self.convergence_threshold
            all_converged = converged_mask.all()
            
            actual_steps = step + 1
            
            if all_converged:
                break
        
        if return_reasoning_trace:
            reasoning_info = {
                'actual_steps': actual_steps,
                'reasoning_trace': reasoning_trace,
                'final_change_magnitude': final_change_magnitude
            }
            return H_state, reasoning_info
        else:
            return H_state
    
    def decode(self, tgt_input_ids, semantic_slots, tgt_attention_mask=None):
        """
        Decoder: Target tokens generation with semantic slots memory
        
        Args:
            tgt_input_ids: [batch_size, tgt_seq_len]
            semantic_slots: [batch_size, num_slots, d_model] (from encoder)
            tgt_attention_mask: [batch_size, tgt_seq_len]
            
        Returns:
            logits: [batch_size, tgt_seq_len, tgt_vocab_size]
        """
        batch_size, tgt_seq_len = tgt_input_ids.shape
        device = tgt_input_ids.device
        
        # Target embeddings
        positions = torch.arange(tgt_seq_len, device=device).unsqueeze(0).expand(batch_size, -1)
        tgt_embeddings = self.tgt_token_embedding(tgt_input_ids) + self.tgt_pos_embedding(positions)
        tgt_embeddings = self.dropout(tgt_embeddings)
        
        # Create causal mask for decoder self-attention
        causal_mask = torch.triu(torch.ones(tgt_seq_len, tgt_seq_len, device=device), diagonal=1).bool()
        
        # Create target key padding mask
        if tgt_attention_mask is not None:
            tgt_key_padding_mask = (tgt_attention_mask == 0)
        else:
            tgt_key_padding_mask = None
        
        # Apply transformer decoder layers
        decoder_output = self.decoder(
            tgt=tgt_embeddings,
            memory=semantic_slots,
            tgt_mask=causal_mask,
            tgt_key_padding_mask=tgt_key_padding_mask
        )
        
        # Output projection
        output = self.output_norm(decoder_output)
        output = self.dropout(output)
        logits = self.output_projection(output)
        
        return logits
    
    def forward(self, src_input_ids, tgt_input_ids, src_attention_mask=None, 
                tgt_attention_mask=None, return_reasoning_trace=False):
        """
        Full forward pass: Encoder + Decoder
        
        Args:
            src_input_ids: [batch_size, src_seq_len]
            tgt_input_ids: [batch_size, tgt_seq_len]
            src_attention_mask: [batch_size, src_seq_len]
            tgt_attention_mask: [batch_size, tgt_seq_len]
            return_reasoning_trace: bool
            
        Returns:
            logits: [batch_size, tgt_seq_len, tgt_vocab_size]
            reasoning_info: dict (optional)
        """
        # Encode source
        if return_reasoning_trace:
            semantic_slots, reasoning_info = self.encode(
                src_input_ids, src_attention_mask, return_reasoning_trace=True
            )
        else:
            semantic_slots = self.encode(src_input_ids, src_attention_mask)
        
        # Decode target
        logits = self.decode(tgt_input_ids, semantic_slots, tgt_attention_mask)
        
        if return_reasoning_trace:
            return logits, reasoning_info
        else:
            return logits
    
    def orthogonal_regularization_loss(self):
        """ê°„ì†Œí™”ëœ orthogonal regularization"""
        device = self.W_source.device
        
        # ìê¸° ì—°ê²° ì œì™¸ ë§ˆìŠ¤í¬
        mask = torch.eye(self.num_slots, device=device, dtype=torch.bool)
        
        # W_source orthogonality (ë²¡í„° ê°„ ì§êµì„±)
        W_source_valid = self.W_source[~mask]  # [N*(N-1), r]
        gram_source = W_source_valid @ W_source_valid.T
        identity_source = torch.eye(W_source_valid.size(0), device=device)
        source_loss = F.mse_loss(gram_source, identity_source)
        
        # W_target orthogonality
        W_target_valid = self.W_target[~mask]  # [N*(N-1), r]
        gram_target = W_target_valid @ W_target_valid.T
        identity_target = torch.eye(W_target_valid.size(0), device=device)
        target_loss = F.mse_loss(gram_target, identity_target)
        
        return (source_loss + target_loss) / 2
    
    def get_connection_analysis(self):
        """ê°„ì†Œí™”ëœ bilinearì— ë§ì¶˜ ì—°ê²° ë¶„ì„"""
        with torch.no_grad():
            # ì—°ê²° ê°•ë„ ê³„ì‚°: [N, N, r] * [N, N, r] -> [N, N]
            connection_magnitudes = torch.sum(self.W_source * self.W_target, dim=-1)
            
            # ìê¸° ì—°ê²° ì œê±° (ëŒ€ê°ì„  0ìœ¼ë¡œ)
            mask = torch.eye(self.num_slots, device=connection_magnitudes.device, dtype=torch.bool)
            connection_magnitudes = connection_magnitudes.masked_fill(mask, 0.0)
            
            # Orthogonality ë¶„ì„ (ë²¡í„°ë“¤ ê°„ì˜ ì§êµì„±)
            orthogonality_errors = []
            
            # W_source ë²¡í„°ë“¤ì˜ ì§êµì„± ë¶„ì„
            W_source_valid = self.W_source[~mask]  # [N*(N-1), r] - ìê¸° ì—°ê²° ì œì™¸
            if W_source_valid.size(0) > 0:
                # ë²¡í„°ë“¤ ê°„ì˜ ë‚´ì  í–‰ë ¬ ê³„ì‚°
                gram_source = W_source_valid @ W_source_valid.T  # [N*(N-1), N*(N-1)]
                # ëŒ€ê°ì„ ì€ 1ì´ì–´ì•¼ í•˜ê³ , ë¹„ëŒ€ê°ì„ ì€ 0ì— ê°€ê¹Œì›Œì•¼ í•¨
                gram_source.fill_diagonal_(0)  # ëŒ€ê°ì„  ì œê±°í•˜ê³  ë¹„ëŒ€ê°ì„  ìš”ì†Œë§Œ í™•ì¸
                source_orth_error = torch.norm(gram_source, 'fro').item()
                orthogonality_errors.append(source_orth_error)
            
            # W_target ë²¡í„°ë“¤ì˜ ì§êµì„± ë¶„ì„
            W_target_valid = self.W_target[~mask]  # [N*(N-1), r]
            if W_target_valid.size(0) > 0:
                gram_target = W_target_valid @ W_target_valid.T
                gram_target.fill_diagonal_(0)
                target_orth_error = torch.norm(gram_target, 'fro').item()
                orthogonality_errors.append(target_orth_error)
            
            # í‰ê·  ì§êµì„± ì˜¤ì°¨
            avg_orthogonality_error = sum(orthogonality_errors) / len(orthogonality_errors) if orthogonality_errors else 0.0
            
            # ì—°ê²° íŒ¨í„´ ë¶„ì„
            abs_connections = torch.abs(connection_magnitudes)
            threshold = 0.01
            
            # ì–‘ìˆ˜/ìŒìˆ˜ ì—°ê²° ë¶„ì„
            positive_connections = (connection_magnitudes > threshold).sum().item()
            negative_connections = (connection_magnitudes < -threshold).sum().item()
            total_possible = self.num_slots * (self.num_slots - 1)  # ìê¸° ì—°ê²° ì œì™¸
            
            # ì—°ê²° ê°•ë„ í†µê³„
            non_zero_connections = connection_magnitudes[connection_magnitudes != 0]
            
            return {
                'connection_matrix': connection_magnitudes,
                'sparsity_ratio': (abs_connections < threshold).float().mean().item(),
                'max_connection': abs_connections.max().item(),
                'min_connection': abs_connections.min().item(),
                'mean_connection': abs_connections.mean().item(),
                'std_connection': abs_connections.std().item(),
                'median_connection': abs_connections.median().item(),
                
                # ì—°ê²° íŒ¨í„´
                'positive_connections': positive_connections,
                'negative_connections': negative_connections,
                'total_possible_connections': total_possible,
                'active_connection_ratio': (positive_connections + negative_connections) / total_possible,
                
                # ì§êµì„± í’ˆì§ˆ
                'orthogonality_error': avg_orthogonality_error,
                'orthogonality_quality': 1.0 / (1.0 + avg_orthogonality_error),
                
                # ì—°ê²° ê°•ë„ ë¶„í¬
                'connection_range': abs_connections.max().item() - abs_connections.min().item(),
                'connection_entropy': self._calculate_connection_entropy(abs_connections),
                
                # ì›ì‹œ ë°ì´í„° (ë””ë²„ê¹…ìš©)
                'raw_source_weights': self.W_source.clone(),
                'raw_target_weights': self.W_target.clone()
            }

    def _calculate_connection_entropy(self, connection_strengths):
        """ì—°ê²° ê°•ë„ì˜ ì—”íŠ¸ë¡œí”¼ ê³„ì‚° (ë‹¤ì–‘ì„± ì¸¡ì •)"""
        try:
            # ì—°ê²° ê°•ë„ë¥¼ í™•ë¥  ë¶„í¬ë¡œ ë³€í™˜
            abs_strengths = torch.abs(connection_strengths).flatten()
            abs_strengths = abs_strengths[abs_strengths > 1e-8]  # 0ì— ê°€ê¹Œìš´ ê°’ ì œê±°
            
            if len(abs_strengths) == 0:
                return 0.0
            
            # ì •ê·œí™”í•˜ì—¬ í™•ë¥  ë¶„í¬ë¡œ ë§Œë“¤ê¸°
            probs = abs_strengths / abs_strengths.sum()
            
            # ì—”íŠ¸ë¡œí”¼ ê³„ì‚°: H = -sum(p * log(p))
            entropy = -(probs * torch.log(probs + 1e-8)).sum().item()
            
            return entropy
        except:
            return 0.0
    
    def reasoning_cost_loss(self, actual_steps, target_steps=4, weight=0.001):
        """Regularization loss for reasoning efficiency"""
        if isinstance(actual_steps, int):
            actual_steps = torch.tensor(actual_steps, dtype=torch.float32, device=next(self.parameters()).device)
        target = torch.full_like(actual_steps, target_steps, dtype=torch.float32)
        return weight * F.mse_loss(actual_steps.float(), target)

    def load_pretrained_weights(self, model_name="google-t5/t5-base"):
        """T5 pre-trained weights ë¡œë”© (d_model í¬ê¸° ì•ˆì „ ì²˜ë¦¬)"""
        try:
            from transformers import T5Model
            pretrained = T5Model.from_pretrained(model_name)
            
            # 1. Token embeddings (d_model ì°¨ì› ì²˜ë¦¬)
            pretrained_embed = pretrained.shared.weight.data  # [vocab_size, pretrained_d_model]
            current_vocab_size = self.src_token_embedding.weight.size(0)
            current_d_model = self.src_token_embedding.weight.size(1)
            pretrained_vocab_size = pretrained_embed.size(0)
            pretrained_d_model = pretrained_embed.size(1)
            
            print(f"ğŸ” Dimensions: current=({current_vocab_size}, {current_d_model}), pretrained=({pretrained_vocab_size}, {pretrained_d_model})")
            
            if current_d_model == pretrained_d_model:
                # d_modelì´ ê°™ìœ¼ë©´ vocab_sizeë§Œ ë§ì¶°ì„œ ë³µì‚¬
                min_vocab_size = min(current_vocab_size, pretrained_vocab_size)
                self.src_token_embedding.weight.data[:min_vocab_size] = pretrained_embed[:min_vocab_size].clone()
                self.tgt_token_embedding.weight.data[:min_vocab_size] = pretrained_embed[:min_vocab_size].clone()
                print(f"âœ… Token embeddings: {min_vocab_size} tokens, d_model={current_d_model}")
            else:
                # d_modelì´ ë‹¤ë¥´ë©´ ì°¨ì› ë§ì¶°ì„œ ë³µì‚¬ (ì‘ì€ ìª½ê¹Œì§€ë§Œ)
                min_vocab_size = min(current_vocab_size, pretrained_vocab_size)
                min_d_model = min(current_d_model, pretrained_d_model)
                self.src_token_embedding.weight.data[:min_vocab_size, :min_d_model] = pretrained_embed[:min_vocab_size, :min_d_model].clone()
                self.tgt_token_embedding.weight.data[:min_vocab_size, :min_d_model] = pretrained_embed[:min_vocab_size, :min_d_model].clone()
                print(f"âœ… Token embeddings: {min_vocab_size} tokens, {min_d_model}/{current_d_model} dimensions")
            
            # 2. Position embeddings (T5 ìŠ¤íƒ€ì¼, d_model í¬ê¸° ë§ì¶¤)
            max_pos = min(self.src_pos_embedding.weight.size(0), 512)
            pos_init = torch.randn(max_pos, current_d_model) * 0.02  # ìš°ë¦¬ d_model í¬ê¸°ë¡œ
            self.src_pos_embedding.weight.data[:max_pos] = pos_init
            self.tgt_pos_embedding.weight.data[:max_pos] = pos_init
            print(f"âœ… Position embeddings: {max_pos} positions, d_model={current_d_model}")
            
            # 3. Output projection (d_model ì°¨ì› ì²˜ë¦¬)
            if hasattr(pretrained, 'lm_head'):
                pretrained_proj = pretrained.lm_head.weight.data  # [vocab_size, pretrained_d_model]
                current_vocab_out = self.output_projection.weight.size(0)
                current_d_model_out = self.output_projection.weight.size(1)
                pretrained_vocab_out = pretrained_proj.size(0)
                pretrained_d_model_out = pretrained_proj.size(1)
                
                if current_d_model_out == pretrained_d_model_out:
                    # d_modelì´ ê°™ìœ¼ë©´ vocabë§Œ ë§ì¶°ì„œ
                    min_vocab_out = min(current_vocab_out, pretrained_vocab_out)
                    self.output_projection.weight.data[:min_vocab_out] = pretrained_proj[:min_vocab_out].clone()
                    print(f"âœ… Output projection: {min_vocab_out} tokens, d_model={current_d_model_out}")
                else:
                    # d_modelì´ ë‹¤ë¥´ë©´ ë‘˜ ë‹¤ ë§ì¶°ì„œ
                    min_vocab_out = min(current_vocab_out, pretrained_vocab_out)
                    min_d_model_out = min(current_d_model_out, pretrained_d_model_out)
                    self.output_projection.weight.data[:min_vocab_out, :min_d_model_out] = pretrained_proj[:min_vocab_out, :min_d_model_out].clone()
                    print(f"âœ… Output projection: {min_vocab_out} tokens, {min_d_model_out}/{current_d_model_out} dimensions")
            
            print(f"ğŸ¯ Pre-trained initialization from {model_name} completed")
            return True
            
        except Exception as e:
            print(f"âš ï¸ Failed to load pre-trained weights: {e}")
            return False