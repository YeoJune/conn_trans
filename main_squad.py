import torch
import numpy as np
import time
import json
import warnings
import math  # math ì„í¬íŠ¸ ì¶”ê°€ (SQuADDatasetì—ì„œ ì‚¬ìš©ë  ìˆ˜ ìˆìŒ)

# ì„¤ì • íŒŒì¼ ë¡œë“œ (í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ê²½ë¡œ ì„¤ì • í•„ìš”)
from configs.squad_config import get_squad_config  # SQuADìš© ì„¤ì •
from datasets.squad_dataset import SQuADDataset
from models.base_conn_trans import ConnectionTransformer
from models.conn_trans_ffn import ConnTransWithFFN
from models.standard_transformer import StandardTransformer
from training.trainer import train_model  # SQuADìš©ìœ¼ë¡œ ìˆ˜ì •ëœ trainer ì‚¬ìš©
from utils.visualization import visualize_connection_matrix, analyze_reasoning_evolution, print_comparison_results


# from utils.metrics import compute_squad_em_f1 # í•„ìš”ì‹œ EM/F1 ê³„ì‚° í•¨ìˆ˜ ì„í¬íŠ¸

# JSON ì¸ì½”ë” (ê²°ê³¼ ì €ì¥ ì‹œ í•„ìš”í•  ìˆ˜ ìˆìŒ)
class NpEncoder(json.JSONEncoder):
    def default(self, o):
        if isinstance(o, np.integer): return int(o)
        if isinstance(o, np.floating): return float(o)
        if isinstance(o, np.ndarray): return o.tolist()
        if isinstance(o, (torch.Tensor)): return o.cpu().tolist()
        if isinstance(o, (torch.float32, torch.float64, torch.float16)): return float(o)
        if isinstance(o, (torch.int32, torch.int64, torch.int16, torch.int8, torch.uint8)): return int(o)
        return super(NpEncoder, self).default(o)


def main_squad():
    CFG = get_squad_config()  # SQuAD ì„¤ì • ë¡œë“œ
    print("ğŸš€ CONNECTION TRANSFORMER - SQuAD 1.1 IMPLEMENTATION (with AutoTokenizer)")
    print(f"   Using Tokenizer: {CFG['tokenizer_name']}")
    print("=" * 70)

    if torch.cuda.is_available():
        torch.backends.cudnn.benchmark = True
        # torch.backends.cudnn.deterministic = False # í•„ìš”ì‹œ ì„¤ì •

    # ì‹œë“œ ê³ ì • (ì¬í˜„ì„±ì„ ìœ„í•´)
    torch.manual_seed(42)
    np.random.seed(42)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(42)

    print("\nğŸ“¦ SQuAD 1.1 Data Loading with AutoTokenizer...")
    try:
        # SQuADDataset í´ë˜ìŠ¤ ë³€ìˆ˜ tokenizer ì´ˆê¸°í™” (ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ì‹œ í•œ ë²ˆë§Œ í† í¬ë‚˜ì´ì € ë¡œë“œë˜ë„ë¡)
        SQuADDataset.tokenizer = None

        train_dataset = SQuADDataset(split='train',
                                     max_seq_len=CFG["max_seq_len"],
                                     tokenizer_name=CFG["tokenizer_name"],
                                     doc_stride=CFG["doc_stride"],
                                     max_query_length=CFG["max_query_length"])
        # val_datasetì€ train_datasetì—ì„œ ì´ˆê¸°í™”ëœ tokenizerë¥¼ ê³µìœ  (SQuADDataset ë‚´ë¶€ ë¡œì§)
        val_dataset = SQuADDataset(split='validation',  # SQuADëŠ” 'validation' ìŠ¤í”Œë¦¿ ì‚¬ìš©
                                   max_seq_len=CFG["max_seq_len"],
                                   tokenizer_name=CFG["tokenizer_name"],
                                   doc_stride=CFG["doc_stride"],
                                   max_query_length=CFG["max_query_length"])
        print("âœ… SQuAD Data loading successful")
    except Exception as e:
        print(f"âŒ SQuAD Data loading failed: {e}")
        import traceback
        traceback.print_exc()
        return {}  # ì˜¤ë¥˜ ë°œìƒ ì‹œ ë¹ˆ dict ë°˜í™˜ ë˜ëŠ” í”„ë¡œê·¸ë¨ ì¢…ë£Œ

    # DataLoader num_workers ì„¤ì •
    cpu_cores = torch.multiprocessing.cpu_count() if hasattr(torch.multiprocessing, 'cpu_count') else 2
    # nw = min(4, CFG["batch_size"] // 4 if CFG["batch_size"] >=4 else 0, cpu_cores // 2 if cpu_cores > 1 else 0)
    # SQuADëŠ” ë°ì´í„° ì „ì²˜ë¦¬ê°€ ë³µì¡í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ, num_workersëŠ” 0ìœ¼ë¡œ ì‹œì‘í•˜ê±°ë‚˜ ë‚®ê²Œ ì„¤ì •í•˜ëŠ” ê²ƒì´ ì•ˆì •ì ì¼ ìˆ˜ ìˆìŒ
    nw = 0 if not torch.cuda.is_available() else min(2, cpu_cores // 2 if cpu_cores > 1 else 0)  # ë””ë²„ê¹… ì‹œ 0 ì¶”ì²œ
    print(f"Using num_workers: {nw} for DataLoaders.")

    train_loader = DataLoader(train_dataset, batch_size=CFG["batch_size"], shuffle=True, num_workers=nw,
                              pin_memory=torch.cuda.is_available())
    val_loader = DataLoader(val_dataset, batch_size=CFG["batch_size"], shuffle=False, num_workers=nw,
                            pin_memory=torch.cuda.is_available())

    vocab_size = train_dataset.tokenizer.vocab_size  # í† í¬ë‚˜ì´ì €ë¡œë¶€í„° vocab_size ê°€ì ¸ì˜´
    device = 'cuda' if torch.cuda.is_available() else 'cpu'

    print(f"  âœ… Device: {device}, Tokenizer Vocab Size: {vocab_size:,}")
    print(f"  ğŸ”¢ Train features: {len(train_dataset):,} (SQuADëŠ” exampleë‹¹ ì—¬ëŸ¬ feature ìƒì„± ê°€ëŠ¥)")
    print(f"  ğŸ”¢ Val features: {len(val_dataset):,}")
    print(f"  ğŸ“¦ Batch size: {CFG['batch_size']}, Train batches: {len(train_loader)}, Val batches: {len(val_loader)}")

    results_squad = {}

    # í•™ìŠµí•  ëª¨ë¸ ì •ì˜
    models_to_train_squad = [
        ("Connection Transformer", ConnectionTransformer),
        ("Standard Transformer", StandardTransformer),
        ("ConnTrans + FFN", ConnTransWithFFN),
    ]

    # ìƒ˜í”Œ ë°ì´í„° ì¤€ë¹„ (analyze_reasoning_evolutionìš©)
    sample_batch_for_trace = None
    if len(val_loader) > 0:
        try:
            sample_batch_for_trace = next(iter(val_loader))  # DataLoaderì—ì„œ ì²« ë°°ì¹˜ ê°€ì ¸ì˜¤ê¸°
            print(f"  ğŸ“Š Prepared a sample batch from validation loader for reasoning trace.")
        except Exception as e_sample:
            print(f"  âš ï¸ Could not get sample batch from val_loader: {e_sample}")
            sample_batch_for_trace = None  # ì‹¤íŒ¨ ì‹œ Noneìœ¼ë¡œ ì„¤ì •

    for model_name, model_class in models_to_train_squad:
        print("\n" + "=" * 60 + f"\nâ–¶ï¸ SQuAD EXPERIMENT: {model_name}" + "\n" + "=" * 60)
        if torch.cuda.is_available(): torch.cuda.empty_cache()  # ì´ì „ ëª¨ë¸ ë©”ëª¨ë¦¬ ì •ë¦¬

        # ëª¨ë¸ ì´ˆê¸°í™” ì‹œ CONFIG ê°’ ì „ë‹¬
        # ê° ëª¨ë¸ í´ë˜ìŠ¤ì˜ __init__ ì‹œê·¸ë‹ˆì²˜ì— ë§ê²Œ ì¸ì ì „ë‹¬
        if model_class == StandardTransformer:
            model_instance = model_class(
                vocab_size=vocab_size,
                d_model=CFG["d_model"],
                num_heads=CFG.get("num_heads", 8),  # base_configì— num_heads ì¶”ê°€ í•„ìš” ë˜ëŠ” ê¸°ë³¸ê°’ ì‚¬ìš©
                num_layers=CFG.get("num_transformer_layers", CFG["num_reasoning_steps"]),
                ffn_dim_multiplier=CFG.get("ffn_dim_multiplier", 4),
                dropout=CFG.get("dropout", 0.1),
                max_seq_len=CFG["max_seq_len"]
            )
        elif model_class == ConnTransWithFFN:
            model_instance = model_class(
                vocab_size=vocab_size,
                d_model=CFG["d_model"],
                num_slots=CFG["num_slots"],
                num_reasoning_steps=CFG["num_reasoning_steps"],
                max_seq_len=CFG["max_seq_len"],
                connection_init_std=CFG.get("connection_init_std", 0.01),  # base_conn_transì—ì„œ ì‚¬ìš©
                spectral_radius_limit=CFG.get("spectral_radius_limit", 0.95),  # base_conn_transì—ì„œ ì‚¬ìš©
                ffn_dim_multiplier=CFG.get("ffn_dim_multiplier", 4),
                dropout=CFG.get("dropout", 0.1)
            )
        else:  # ConnectionTransformer (base)
            model_instance = model_class(
                vocab_size=vocab_size,
                d_model=CFG["d_model"],
                num_slots=CFG["num_slots"],
                num_reasoning_steps=CFG["num_reasoning_steps"],
                max_seq_len=CFG["max_seq_len"],
                connection_init_std=CFG.get("connection_init_std", 0.01),
                spectral_radius_limit=CFG.get("spectral_radius_limit", 0.95)
            )

        # ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜ ì¶œë ¥ (ëª¨ë¸ __init__ì—ì„œ ì´ë¯¸ ì¶œë ¥ë¨)
        # total_params = sum(p.numel() for p in model_instance.parameters() if p.requires_grad)
        # print(f"   {model_name}: {total_params:,} trainable parameters")

        metric_val = train_model(model_instance, train_loader, val_loader, CFG, device,
                                 model_name)  # SQuADìš© train_model ì‚¬ìš©
        results_squad[model_name] = metric_val

        # í•™ìŠµ í›„ ë¶„ì„ (metric_valì´ ì˜ë¯¸ìˆëŠ” ê°’ì¼ ë•Œë§Œ)
        if isinstance(metric_val, float) and metric_val > 0.0 and sample_batch_for_trace:
            if hasattr(model_instance, 'C') and model_instance.C is not None:
                try:
                    visualize_connection_matrix(model_instance, f"{model_name.replace(' ', '_')}_squad_C.png",
                                                f" ({model_name} SQuAD)")
                except Exception as e_vis_cm:
                    print(f"Error visualizing CM for {model_name}: {e_vis_cm}")

            if hasattr(model_instance, 'get_reasoning_trace'):
                try:
                    analyze_reasoning_evolution(model_instance, sample_batch_for_trace,
                                                f"{model_name.replace(' ', '_')}_squad_evo.png", model_name)
                except Exception as e_vis_evo:
                    print(f"Error analyzing/visualizing evo for {model_name}: {e_vis_evo}")

        del model_instance  # í•™ìŠµ ì™„ë£Œëœ ëª¨ë¸ ë©”ëª¨ë¦¬ì—ì„œ ëª…ì‹œì ìœ¼ë¡œ í•´ì œ
        if torch.cuda.is_available(): torch.cuda.empty_cache()

    print_comparison_results(results_squad, metric_name="Val Span Acc (SQuAD)")

    print(f"\nğŸ’¾ Saving SQuAD Experimental Results...")
    squad_exp_results = {
        "experiment_type": "squad_1.1_conn_trans_comparison_autotokenizer_refactored",
        "dataset": "SQuAD 1.1",
        "tokenizer": CFG["tokenizer_name"],
        "config_hyperparameters": CFG,  # config -> config_hyperparameters
        "results_metric": "Validation Span Accuracy",  # ì‚¬ìš©ëœ í‰ê°€ì§€í‘œ ëª…ì‹œ
        "model_accuracies": results_squad,  # results -> model_accuracies
        "timestamp": time.strftime("%Y%m%d_%H%M%S")
    }
    squad_results_filename = f"squad_results_autotok_refactored_{squad_exp_results['timestamp']}.json"
    try:
        with open(squad_results_filename, "w") as f:
            json.dump(squad_exp_results, f, indent=2, cls=NpEncoder)  # NpEncoder ì‚¬ìš©
        print(f"  ğŸ“„ SQuAD Results: {squad_results_filename}")
    except Exception as e_json_squad:
        print(f"âš ï¸ Error saving SQuAD JSON: {e_json_squad}")

    print(f"\nâœ¨ SQuAD Experiment Completed!")
    return results_squad


if __name__ == "__main__":
    # ì´ íŒŒì¼ì´ ì§ì ‘ ì‹¤í–‰ë  ë•Œ main_squad() í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤.
    # main_babi.pyëŠ” ë³„ë„ë¡œ ì‹¤í–‰í•´ì•¼ í•©ë‹ˆë‹¤.

    warnings.filterwarnings("ignore", category=UserWarning)  # ì¼ë°˜ì ì¸ UserWarning ë¬´ì‹œ
    # íŠ¹ì • ë¼ì´ë¸ŒëŸ¬ë¦¬ ê²½ê³  ë¬´ì‹œ (í•„ìš”ì‹œ)
    # warnings.filterwarnings("ignore", message="The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option")

    print("=" * 20 + " Starting SQuAD Experiments " + "=" * 20)
    try:
        final_results_squad = main_squad()
        if final_results_squad:
            print(f"\nğŸ‰ SQuAD Experiments Final Results (Val Span Acc):")
            for model, acc in final_results_squad.items():
                print(f"  - {model}: {acc:.4f}")
    except KeyboardInterrupt:
        print(f"\nğŸ›‘ SQuAD Experiment interrupted by user.")
    except Exception as e_squad_main:
        print(f"\nâŒ SQuAD Main Experiment failed: {e_squad_main}")
        import traceback

        traceback.print_exc()

    print("\n\n" + "=" * 20 + " SQuAD Script Finished " + "=" * 20)