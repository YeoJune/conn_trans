# Connection Transformer: 개선된 아키텍처 완전 가이드

## 핵심 아이디어

### Semantic Slot Hypothesis

복잡한 추론 과정을 다음과 같이 분해할 수 있다:

1. **고정된 의미 슬롯 (H)**: N개의 추상적 의미 컨테이너
2. **학습 가능한 연결**: 슬롯 간의 상호작용을 정의하는 연결 메커니즘
3. **동적 추론**: 입력에 따라 적응적으로 변화하는 추론 과정

**핵심 통찰**: 전체 시퀀스에서 추론하는 대신, 압축된 의미 공간에서 명시적 추론을 수행

---

## 전체 아키텍처 흐름

```
Input Tokens → Compression → Reasoning → Expansion → Output Logits
   [B,S]         ↓            [B,N,D]       ↓         [B,S,V]
              [B,S,D]                    [B,S,D]
```

### 1단계: 입력 압축 (Input Compression)

**목적**: 긴 시퀀스를 고정된 개수의 의미 슬롯으로 압축

```python
# 토큰 임베딩
X_input = TokenEmbedding(input_ids) + PositionalEmbedding(positions)  # (B,S,D)

# 고정된 의미 슬롯 (학습되지 않음)
H = fixed_semantic_slots  # (N,D) - 모든 입력에 대해 동일

# Cross-attention으로 압축
Q_input = X_input @ W_q_input    # (B,S,D)
K_slots = H @ W_k_slots          # (N,D)
V_input = X_input @ W_v_input    # (B,S,D)

attention_weights = softmax(Q_input @ K_slots^T / √D)  # (B,S,N)
slot_activation = attention_weights^T @ V_input        # (B,N,D)

# 초기 슬롯 상태
H_state = H.expand(B,-1,-1) + slot_activation  # (B,N,D)
```

**의미**: 각 토큰이 어떤 의미 슬롯에 얼마나 기여하는지 학습

### 2단계: 반복적 추론 (Iterative Reasoning)

**목적**: 의미 슬롯들 간의 상호작용을 통한 추론

#### 옵션 A: 선형 연결 (단순한 버전)

```python
# 제약: N = D (슬롯 수 = 모델 차원)
C = learnable_connection_matrix  # (N,N)

for step in range(max_steps):
    influence = H_state @ C                    # (B,N,D) @ (N,N) = (B,N,D)
    step_update = torch.relu(influence)        # 임계값 활성화
    H_state = H_state + step_update

    # 동적 종료 조건
    change_magnitude = torch.norm(step_update, dim=-1)
    if change_magnitude.max() < threshold:
        break  # 수렴했으면 추론 종료
```

#### 옵션 B: Bilinear 연결 (고급 버전) ⭐

```python
# 자유도: N과 D 독립적 설정 가능
source_proj = learnable_parameter  # (N,N,D,rank)
target_proj = learnable_parameter  # (N,N,rank,D)

for step in range(max_steps):
    influence = torch.zeros_like(H_state)

    for i in range(N):
        for j in range(N):
            if i != j:  # 자기 자신 제외
                # 비선형 변환: D → rank → D
                temp = H_state[:,i,:] @ source_proj[i,j]     # (B,D) → (B,rank)
                transformed = temp @ target_proj[i,j]        # (B,rank) → (B,D)
                influence[:,j,:] += transformed

    step_update = torch.relu(influence)
    H_state = H_state + step_update

    # 동적 종료 조건
    change_magnitude = torch.norm(step_update, dim=-1)
    active_slots = change_magnitude > threshold
    if active_slots.sum() == 0:
        break  # 모든 슬롯이 수렴
```

**핵심 차이점**:

- **선형**: 단순한 스케일링, N=D 제약
- **Bilinear**: 복잡한 변환, N≠D 가능, 더 풍부한 표현력

### 3단계: 출력 확장 (Output Expansion)

**목적**: 추론된 의미 슬롯을 다시 시퀀스로 확장

```python
Q_output = X_input @ W_q_output      # (B,S,D)
K_final = H_state @ W_k_final        # (B,N,D)
V_final = H_state @ W_v_final        # (B,N,D)

attention_weights = softmax(Q_output @ K_final^T / √D)  # (B,S,N)
Y_output = attention_weights @ V_final                  # (B,S,D)

logits = Y_output @ W_vocab          # (B,S,V)
```

**의미**: 각 출력 위치가 관련된 의미 슬롯에서 정보를 가져옴

---

## 핵심 혁신 포인트

### 1. 적응적 추론 (Dynamic Reasoning)

```python
# 전통적 방식: 항상 K번 고정 반복
for step in range(K):  # K는 하이퍼파라미터
    reasoning_step()

# Connection Transformer: 수렴까지만 추론
while not_converged and step < max_steps:
    change = reasoning_step()
    if change < threshold:
        break  # "충분히 생각했다"고 판단
```

**생물학적 의미**: 뇌도 문제 난이도에 따라 다른 시간만큼 생각함

### 2. 명시적 연결 학습

```python
# 일반적 Transformer: 암시적 추론
attention_weights = softmax(Q @ K^T)  # 동적, 해석 어려움

# Connection Transformer: 명시적 추론
C[i,j] = "슬롯 i가 슬롯 j에게 미치는 영향"  # 고정, 해석 가능
```

**해석가능성**: 학습된 C를 분석해서 모델의 "사고 과정" 직접 관찰

### 3. 압축된 추론 공간

```python
# 일반적 Transformer: 전체 시퀀스에서 추론
computation = O(S² × D)  # S가 길면 매우 비효율적

# Connection Transformer: 고정된 슬롯에서 추론
computation = O(N² × D)  # N << S이면 효율적
```

---

## 설계 선택지와 트레이드오프

### 연결 방식 선택

| 방식         | 장점                  | 단점                     | 제약조건   |
| ------------ | --------------------- | ------------------------ | ---------- |
| **선형**     | 단순함, 빠른 계산     | 제한된 표현력            | N = D 필수 |
| **Bilinear** | 풍부한 표현력, 자유도 | 복잡함, 더 많은 파라미터 | 없음       |

### N과 D 비율 선택 (Bilinear만 해당)

| N/D 비율         | 의미           | 장점                   | 단점                   |
| ---------------- | -------------- | ---------------------- | ---------------------- |
| **N < D** (압축) | 강한 정보 압축 | 빠른 추론, 적은 메모리 | 정보 손실 가능         |
| **N = D** (동일) | 균형잡힌 설정  | 안정적                 | 특별한 장점 없음       |
| **N > D** (확장) | 세밀한 추론    | 더 많은 전문화         | 느린 추론, 과적합 위험 |

### rank 선택 (Bilinear만 해당)

```python
rank = 8   # 압축된 변환, 빠름
rank = 32  # 균형잡힌 변환 ⭐ 추천
rank = 64  # 풍부한 변환, 느림
```

---

## 실제 동작 예시

### 예시: "The cat sat on the mat" 처리

#### 1단계: 압축

```
Tokens: [The, cat, sat, on, the, mat] → 6개 토큰
Slots: [Entity, Action, Location, ...] → N개 슬롯

압축 결과:
- Entity 슬롯: "cat" 정보 집중
- Action 슬롯: "sat" 정보 집중
- Location 슬롯: "mat" 정보 집중
```

#### 2단계: 추론 (Bilinear 버전)

```
Step 1: Entity → Action 영향
"cat이 앉는다" 관계 강화

Step 2: Action → Location 영향
"앉는 위치는 mat" 관계 강화

Step 3: 변화량 < threshold
추론 종료 (2.5단계만에 수렴)
```

#### 3단계: 확장

```
각 출력 위치가 관련 슬롯에서 정보 가져옴:
- Position 0: Entity 슬롯 참조 → "The"
- Position 1: Entity 슬롯 참조 → "cat"
- Position 2: Action 슬롯 참조 → "sat"
- ...
```

---

## 기대 효과

### 성능

- **더 명시적인 추론**: 복잡한 추론 태스크에서 성능 향상
- **적응적 계산**: 간단한 입력은 빠르게, 복잡한 입력은 충분히

### 효율성

- **파라미터**: N² (선형) 또는 N²×rank×2 (Bilinear)
- **계산**: O(N²×D) vs O(S²×D), N << S이면 유리
- **메모리**: 작은 N 선택으로 메모리 절약 가능

### 해석가능성

- **연결 분석**: 어떤 개념들이 어떻게 연결되는가?
- **수렴 분석**: 모델이 언제 "답을 찾았다"고 판단하는가?
- **슬롯 특화**: 각 슬롯이 어떤 역할을 담당하는가?
