# RTX 4090 ê¸°ì¤€ Connection Transformer ì‹¤í—˜ ì…‹ì—…

## í•˜ë“œì›¨ì–´ ìŠ¤í™

- **GPU**: RTX 4090 (24GB VRAM)
- **ë©”ëª¨ë¦¬**: ì¶©ë¶„í•œ RAM (32GB+ ê¶Œì¥)
- **ì €ì¥ê³µê°„**: SSD 500GB+ (ë°ì´í„°ì…‹ + ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸)

---

## ì‹¤í—˜ ì„¤ì •

### ëª¨ë¸ Configuration (í˜„ì¬ êµ¬í˜„ ê¸°ì¤€)

```python
# micro ì„¤ì • (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸)
micro_config = {
    "d_model": 128,
    "num_slots": 128,
    "bilinear_rank": 1,
    "max_reasoning_steps": 1,
    "num_decoder_layers": 4,
    "num_heads": 4,
    "max_seq_len": 128,
    "batch_size": 64
}

# small ì„¤ì • (ê· í˜•)
small_config = {
    "d_model": 256,
    "num_slots": 256,          # N = D baseline
    "bilinear_rank": 1,
    "max_reasoning_steps": 1,
    "num_decoder_layers": 5,
    "num_heads": 4,
    "max_seq_len": 256,
    "batch_size": 48
}

# base ì„¤ì • (ì„±ëŠ¥ ì¤‘ì‹¬)
base_config = {
    "d_model": 512,
    "num_slots": 512,          # N = D for full capacity
    "bilinear_rank": 1,
    "max_reasoning_steps": 1,
    "num_decoder_layers": 6,
    "num_heads": 8,
    "max_seq_len": 256,
    "batch_size": 32
}

# large ì„¤ì • (ìµœëŒ€ ì„±ëŠ¥)
large_config = {
    "d_model": 768,
    "num_slots": 768,
    "bilinear_rank": 1,
    "max_reasoning_steps": 1,
    "num_decoder_layers": 6,
    "num_heads": 8,
    "max_seq_len": 256,
    "batch_size": 24
}
```

### ë°°ì¹˜ í¬ê¸° ìµœì í™” (Encoder-Decoder êµ¬ì¡°)

```python
# RTX 4090 (24GB) ê¸°ì¤€ ê¶Œì¥ ë°°ì¹˜ í¬ê¸°
batch_sizes = {
    "micro": {
        "train_batch": 64,      # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: ~12GB
        "eval_batch": 128,
        "gradient_accumulation": 1
    },
    "small": {
        "train_batch": 48,      # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: ~16GB
        "eval_batch": 64,
        "gradient_accumulation": 2
    },
    "base": {
        "train_batch": 32,      # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: ~20GB
        "eval_batch": 48,
        "gradient_accumulation": 2
    },
    "large": {
        "train_batch": 24,      # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: ~22GB
        "eval_batch": 32,
        "gradient_accumulation": 3
    }
}
```

### í˜„ì¬ êµ¬í˜„ ìµœì í™” ì„¤ì •

```python
optimization_config = {
    # Mixed precision training (trainer.py ê¸°ì¤€)
    "bf16": True,               # BF16 ì§€ì›ì‹œ ì‚¬ìš©
    "fp16": False,              # fallback

    # Gradient ì„¤ì •
    "gradient_clip": 1.0,
    "weight_decay": 0.01,

    # ì •ê·œí™” (í˜„ì¬ êµ¬í˜„)
    "orthogonal_weight": 0.01,  # Connection regularization
    "label_smoothing": 0.1,

    # DataLoader ì„¤ì •
    "num_workers": 0,           # í˜„ì¬ ê¸°ë³¸ê°’
    "pin_memory": True,

    # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„
    "warmup_ratio": 0.1,
}
```

---

## ë°ì´í„°ì…‹ êµ¬ì„± (í˜„ì¬ ì§€ì› ë°ì´í„°ì…‹)

### Primary ë°ì´í„°ì…‹

```python
primary_datasets = {
    "strategyqa": {
        "task_prefix": "strategy",
        "answer_max_length": 8,
        "num_epochs": 5,
        "batch_size": 16,
        "learning_rate": 1e-4,
        "description": "Yes/No reasoning (2.7K examples)"
    },
    "logiqa": {
        "task_prefix": "reason",
        "answer_max_length": 16,
        "num_epochs": 3,
        "max_seq_len": 384,
        "learning_rate": 1.5e-4,
        "description": "Multiple choice reasoning"
    },
    "gsm8k": {
        "task_prefix": "solve",
        "answer_max_length": 128,
        "num_epochs": 15,
        "max_seq_len": 512,
        "description": "Math word problems (8.8K examples)"
    }
}
```

### Secondary ë°ì´í„°ì…‹

```python
secondary_datasets = {
    "multinli": {
        "task_prefix": "infer",
        "answer_max_length": 16,
        "num_epochs": 12,
        "batch_size": 64,
        "learning_rate": 5e-5,
        "early_stopping_patience": 2,
        "description": "Natural language inference (433K examples)"
    },
    "eli5": {
        "task_prefix": "explain",
        "answer_max_length": 200,
        "max_seq_len": 320,
        "num_epochs": 6,
        "batch_size": 12,
        "learning_rate": 8e-5,
        "description": "Explain Like I'm 5"
    },
    "commongen": {
        "task_prefix": "connect",
        "answer_max_length": 80,
        "max_seq_len": 200,
        "num_epochs": 10,
        "batch_size": 32,
        "learning_rate": 1e-4,
        "description": "Concept-to-text generation"
    }
}
```

---

## ì‹¤í—˜ ê³„íš

### Phase 1: ê¸°ë³¸ ê²€ì¦ (í˜„ì¬ êµ¬í˜„ í…ŒìŠ¤íŠ¸)

```python
baseline_experiments = [
    {
        "name": "Connection_Micro",
        "model": "connection",
        "size": "micro",
        "datasets": ["strategyqa", "logiqa"],
        "purpose": "ë¹ ë¥¸ ê¸°ëŠ¥ ê²€ì¦"
    },
    {
        "name": "Baseline_Micro",
        "model": "baseline",
        "size": "micro",
        "datasets": ["strategyqa", "logiqa"],
        "purpose": "ëŒ€ì¡°êµ° ì„±ëŠ¥ í™•ì¸"
    },
    {
        "name": "Connection_Small",
        "model": "connection",
        "size": "small",
        "datasets": ["strategyqa", "logiqa", "gsm8k"],
        "purpose": "ì¤‘ê°„ í¬ê¸° ì„±ëŠ¥ í™•ì¸"
    }
]
```

### Phase 2: ëª¨ë¸ í¬ê¸°ë³„ ë¹„êµ

```python
size_comparison = [
    # Connection Transformer ìŠ¤ì¼€ì¼ë§
    {"model": "connection", "size": "micro", "focus": "efficiency"},
    {"model": "connection", "size": "small", "focus": "balance"},
    {"model": "connection", "size": "base", "focus": "performance"},

    # Baseline ë¹„êµ
    {"model": "baseline", "size": "micro", "focus": "efficiency"},
    {"model": "baseline", "size": "small", "focus": "balance"},
    {"model": "baseline", "size": "base", "focus": "performance"}
]
```

### Phase 3: ë°ì´í„°ì…‹ë³„ íŠ¹ì„± ë¶„ì„

```python
dataset_analysis = {
    "reasoning_complexity": ["strategyqa", "logiqa"],
    "mathematical": ["gsm8k"],
    "large_scale": ["multinli"],
    "generation": ["eli5", "commongen"]
}
```

---

## í•™ìŠµ ì„¤ì • (í˜„ì¬ Trainer í´ë˜ìŠ¤ ê¸°ì¤€)

### í›ˆë ¨ íŒŒë¼ë¯¸í„°

```python
training_config = {
    # ì˜µí‹°ë§ˆì´ì € (Trainer._setup_optimizer)
    "optimizer": "AdamW",
    "learning_rate": 1e-4,      # ë°ì´í„°ì…‹ë³„ ìë™ ì¡°ì •
    "weight_decay": 0.01,

    # ìŠ¤ì¼€ì¤„ëŸ¬ (get_linear_schedule_with_warmup)
    "warmup_ratio": 0.1,

    # ì •ê·œí™”
    "gradient_clip": 1.0,
    "orthogonal_weight": 0.01,   # Connection specific
    "label_smoothing": 0.1,

    # ì¡°ê¸° ì¢…ë£Œ
    "early_stopping_patience": 3,
    "eval_every": 100,

    # ì •ë°€ë„
    "bf16": True,               # ìë™ ê°ì§€
    "fp16": False               # fallback
}
```

### ëª¨ë‹ˆí„°ë§ ë©”íŠ¸ë¦­ (í˜„ì¬ êµ¬í˜„ ê¸°ì¤€)

```python
metrics_to_track = {
    "performance": {
        "accuracy": "calculate_accuracy from utils.metrics",
        "dataset_specific": "extract_final_answer per dataset type"
    },
    "efficiency": {
        "reasoning_steps": "avg_reasoning_steps from trainer",
        "training_time": "epoch duration",
        "memory_usage": "GPU memory tracking"
    },
    "connection_analysis": {
        "sparsity_ratio": "model.get_connection_analysis()",
        "max_connection": "connection strength analysis",
        "orthogonality_quality": "regularization metrics"
    },
    "training_dynamics": {
        "loss": "train/eval loss curves",
        "grad_norm": "gradient clipping tracking",
        "lr": "learning rate schedule"
    }
}
```

---

## ì‹¤í—˜ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸

### ë‹¨ì¼ ì‹¤í—˜ ì‹¤í–‰

```bash
# ê¸°ë³¸ Connection Transformer
python main.py --dataset strategyqa --model connection --model_size micro --output_dir ./outputs

# Baseline ë¹„êµ
python main.py --dataset strategyqa --model baseline --model_size micro --output_dir ./outputs

# ë‹¤ë¥¸ í¬ê¸° í…ŒìŠ¤íŠ¸
python main.py --dataset gsm8k --model connection --model_size small --output_dir ./outputs
```

### ë°°ì¹˜ ì‹¤í—˜ (run_experiments.sh ì‚¬ìš©)

```bash
# ëª¨ë“  ë°ì´í„°ì…‹ ê¸°ë³¸ í¬ê¸°ë¡œ ì‹¤í–‰
./run_experiments.sh all

# íŠ¹ì • ë°ì´í„°ì…‹ íŠ¹ì • í¬ê¸°
./run_experiments.sh strategyqa micro

# ëª¨ë“  ë°ì´í„°ì…‹ì„ small í¬ê¸°ë¡œ
./run_experiments.sh all small
```

### ì²´ê³„ì  ë¹„êµ ì‹¤í—˜

```bash
#!/bin/bash
# systematic_comparison.sh

echo "ğŸš€ Systematic Connection Transformer Evaluation"

# Phase 1: Micro scale validation
echo "ğŸ“Š Phase 1: Micro Scale Validation"
./run_experiments.sh strategyqa micro
./run_experiments.sh logiqa micro

# Phase 2: Small scale comparison
echo "ğŸ“Š Phase 2: Small Scale Comparison"
for dataset in strategyqa logiqa gsm8k; do
    python main.py --dataset $dataset --model connection --model_size small --output_dir ./outputs
    python main.py --dataset $dataset --model baseline --model_size small --output_dir ./outputs
done

# Phase 3: Base scale performance
echo "ğŸ“Š Phase 3: Base Scale Performance"
for dataset in gsm8k multinli; do
    python main.py --dataset $dataset --model connection --model_size base --output_dir ./outputs
    python main.py --dataset $dataset --model baseline --model_size base --output_dir ./outputs
done

# Phase 4: Generation tasks
echo "ğŸ“Š Phase 4: Generation Tasks"
for dataset in eli5 commongen; do
    python main.py --dataset $dataset --model connection --model_size base --output_dir ./outputs
    python main.py --dataset $dataset --model baseline --model_size base --output_dir ./outputs
done

# Final analysis
echo "ğŸ“ˆ Running comprehensive analysis..."
python analyze_results.py --output_dir ./outputs
```

---

## ì‹¤í—˜ ì¼ì • ë° ì˜ˆìƒ ì‹œê°„

### ì „ì²´ íƒ€ì„ë¼ì¸ (RTX 4090 ê¸°ì¤€)

```
Phase 1 (1ì¼): Micro scale validation
- strategyqa micro: ~30ë¶„ Ã— 2ëª¨ë¸ = 1ì‹œê°„
- logiqa micro: ~45ë¶„ Ã— 2ëª¨ë¸ = 1.5ì‹œê°„

Phase 2 (2ì¼): Small scale comparison
- 3ê°œ ë°ì´í„°ì…‹ Ã— 2ëª¨ë¸ Ã— ~2ì‹œê°„ = 12ì‹œê°„

Phase 3 (2ì¼): Base scale performance
- 2ê°œ ë°ì´í„°ì…‹ Ã— 2ëª¨ë¸ Ã— ~4ì‹œê°„ = 16ì‹œê°„

Phase 4 (2ì¼): Generation tasks
- 2ê°œ ë°ì´í„°ì…‹ Ã— 2ëª¨ë¸ Ã— ~6ì‹œê°„ = 24ì‹œê°„

Phase 5 (1ì¼): Analysis and documentation
- ê²°ê³¼ ë¶„ì„ ë° ë¦¬í¬íŠ¸ ìƒì„±
```

### ì¼ì¼ ì²´í¬ë¦¬ìŠ¤íŠ¸

```
â–¡ GPU ì˜¨ë„ ë° ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
â–¡ ì‹¤í—˜ ë¡œê·¸ ì •ìƒ ê¸°ë¡ í™•ì¸
â–¡ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ í™•ì¸
â–¡ compare analysis ìë™ ì‹¤í–‰ í™•ì¸
â–¡ ê²°ê³¼ ë°±ì—… (experiments/, analysis/, comparisons/)
```

---

## ë©”ëª¨ë¦¬ ìµœì í™” íŒ

### í˜„ì¬ êµ¬í˜„ ê¸°ì¤€ ìµœì í™”

```python
# trainer.pyì—ì„œ ìë™ ì ìš©ë˜ëŠ” ìµœì í™”ë“¤
memory_optimizations = {
    "autocast": "ìë™ mixed precision",
    "gradient_scaling": "FP16 ì‚¬ìš©ì‹œ ìë™ ìŠ¤ì¼€ì¼ë§",
    "OOM_handling": "ë°°ì¹˜ ìŠ¤í‚µìœ¼ë¡œ ì•ˆì •ì„± í™•ë³´",
    "periodic_cleanup": "torch.cuda.empty_cache() ì£¼ê¸°ì  ì‹¤í–‰"
}

# ì¶”ê°€ ìµœì í™” ì˜µì…˜
additional_optimizations = {
    "gradient_checkpointing": "ë©”ëª¨ë¦¬ ì ˆì•½ (ì•½ 30% ê°ì†Œ)",
    "smaller_batch_accumulation": "ì‘ì€ ë°°ì¹˜ + accumulation",
    "eval_batch_larger": "í‰ê°€ì‹œ ë” í° ë°°ì¹˜ ì‚¬ìš© ê°€ëŠ¥"
}
```

### RTX 4090 í™œìš©ë„ ê·¹ëŒ€í™”

```python
rtx4090_optimization = {
    "tensor_cores": "BF16/FP16 ìë™ í™œìš©",
    "memory_bandwidth": "24GB VRAM ìµœëŒ€ í™œìš©",
    "compute_capability": "8.9 - ëª¨ë“  ìµœì‹  ê¸°ëŠ¥ ì§€ì›",
    "nvcc_arch": "sm_89 ì»´íŒŒì¼ ìµœì í™”"
}
```
