# RTX 4090 ê¸°ì¤€ Connection Transformer ì‹¤í—˜ ì…‹ì—…

## í•˜ë“œì›¨ì–´ ìŠ¤í™

- **GPU**: RTX 4090 (24GB VRAM)
- **ë©”ëª¨ë¦¬**: ì¶©ë¶„í•œ RAM (32GB+ ê¶Œì¥)
- **ì €ì¥ê³µê°„**: SSD 500GB+ (ë°ì´í„°ì…‹ + ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸)

---

## ì‹¤í—˜ ì„¤ì •

### ëª¨ë¸ Configuration

```python
# ê¸°ë³¸ ì„¤ì • (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )
base_config = {
    "d_model": 256,
    "num_slots": 128,           # N < D for compression
    "bilinear_rank": 32,        # ì ë‹¹í•œ ë³µì¡ë„
    "max_reasoning_steps": 6,
    "convergence_threshold": 0.01,
    "vocab_size": 32000,        # T5 tokenizer ê¸°ì¤€
    "max_seq_len": 512
}

# í° ëª¨ë¸ ì„¤ì • (ì„±ëŠ¥ ì¤‘ì‹¬)
large_config = {
    "d_model": 512,
    "num_slots": 256,
    "bilinear_rank": 64,
    "max_reasoning_steps": 8,
    "convergence_threshold": 0.005,
    "vocab_size": 32000,
    "max_seq_len": 512
}
```

### ë°°ì¹˜ í¬ê¸° ìµœì í™”

```python
# RTX 4090 (24GB) ê¸°ì¤€ ê¶Œì¥ ë°°ì¹˜ í¬ê¸°
batch_sizes = {
    "base_config": {
        "train_batch": 32,      # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: ~18GB
        "eval_batch": 64,       # ì¶”ë¡ ì‹œ ë” í° ë°°ì¹˜ ê°€ëŠ¥
        "gradient_accumulation": 2  # íš¨ê³¼ì  ë°°ì¹˜ = 64
    },
    "large_config": {
        "train_batch": 16,      # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: ~20GB
        "eval_batch": 32,
        "gradient_accumulation": 4  # íš¨ê³¼ì  ë°°ì¹˜ = 64
    }
}
```

### ë©”ëª¨ë¦¬ ìµœì í™” ì „ëµ

```python
optimization_config = {
    # Mixed precision training
    "fp16": True,
    "bf16": False,  # RTX 4090ì—ì„œëŠ” fp16ì´ ë” ì•ˆì •ì 

    # Gradient checkpointing
    "gradient_checkpointing": True,  # ë©”ëª¨ë¦¬ ì ˆì•½

    # DataLoader ì„¤ì •
    "num_workers": 4,
    "pin_memory": True,
    "persistent_workers": True,

    # ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±
    "empty_cache_steps": 100,  # ì£¼ê¸°ì  ìºì‹œ ì •ë¦¬
}
```

---

## ë°ì´í„°ì…‹ êµ¬ì„±

### Primary ë°ì´í„°ì…‹ (ë…¼ë¬¸ìš©)

```python
primary_datasets = {
    "LogiQA": {
        "size": "8,678 examples",
        "split": "train(6,678) / dev(1,000) / test(1,000)",
        "preprocessing": "T5 format: 'reason: <premise> question: <question>'",
        "max_length": 256
    },
    "GSM8K": {
        "size": "8,792 examples",
        "split": "train(7,473) / test(1,319)",
        "preprocessing": "T5 format: 'solve: <problem>'",
        "max_length": 512
    },
    "StrategyQA": {
        "size": "2,780 examples",
        "split": "train(2,290) / test(490)",
        "preprocessing": "T5 format: 'strategy: <question>'",
        "max_length": 256
    }
}
```

### Secondary ë°ì´í„°ì…‹ (ì¶”ê°€ ê²€ì¦)

```python
secondary_datasets = {
    "CommonsenseQA": {
        "size": "12,247 examples",
        "split": "train(9,741) / dev(1,221) / test(1,285)",
        "max_length": 128
    },
    "SST-2": {  # ë‹¨ìˆœ íƒœìŠ¤í¬ ëŒ€ì¡°êµ°
        "size": "67,349 examples",
        "split": "train(67,349) / dev(872) / test(1,821)",
        "max_length": 64
    }
}
```

---

## ì‹¤í—˜ ê³„íš

### Phase 1: ê¸°ë³¸ ê²€ì¦ (1-2ì¼)

```python
baseline_experiments = [
    {
        "name": "Linear_N=D",
        "config": "N=256, D=256, Linear connections",
        "purpose": "ê¸°ì¡´ ë°©ì‹ ì¬í˜„"
    },
    {
        "name": "Bilinear_Base",
        "config": base_config,
        "purpose": "ê¸°ë³¸ bilinear ì„±ëŠ¥ í™•ì¸"
    },
    {
        "name": "Bilinear_Large",
        "config": large_config,
        "purpose": "ìŠ¤ì¼€ì¼ì—… íš¨ê³¼ í™•ì¸"
    }
]

# ê° ì‹¤í—˜ë‹¹ ì˜ˆìƒ ì‹œê°„
training_time_estimates = {
    "LogiQA": "2-3ì‹œê°„ (base) / 4-5ì‹œê°„ (large)",
    "GSM8K": "3-4ì‹œê°„ (base) / 6-7ì‹œê°„ (large)",
    "StrategyQA": "1-2ì‹œê°„ (base) / 2-3ì‹œê°„ (large)"
}
```

### Phase 2: N/D ë¹„ìœ¨ íƒìƒ‰ (2-3ì¼)

```python
nd_ratio_experiments = [
    {"N": 64,  "D": 256, "name": "4to1_compression"},
    {"N": 128, "D": 256, "name": "2to1_compression"},
    {"N": 256, "D": 256, "name": "1to1_baseline"},
    {"N": 512, "D": 256, "name": "2to1_expansion"}
]

# bilinear_rank=32 ê³ ì •, 3ê°œ ë°ì´í„°ì…‹ì—ì„œ í…ŒìŠ¤íŠ¸
```

### Phase 3: Rank ìµœì í™” (1-2ì¼)

```python
rank_experiments = [
    {"rank": 8,  "name": "minimal_rank"},
    {"rank": 16, "name": "small_rank"},
    {"rank": 32, "name": "medium_rank"},
    {"rank": 64, "name": "large_rank"}
]

# ìµœì  N/D ë¹„ìœ¨ì—ì„œ rankë§Œ ë³€ê²½
```

### Phase 4: Dynamic Reasoning ë¶„ì„ (1ì¼)

```python
dynamic_experiments = [
    {"threshold": 0.001, "name": "sensitive_termination"},
    {"threshold": 0.01,  "name": "medium_termination"},
    {"threshold": 0.1,   "name": "loose_termination"},
    {"fixed_steps": 4,   "name": "no_dynamic_baseline"}
]
```

---

## í•™ìŠµ ì„¤ì •

### ìµœì í™” íŒŒë¼ë¯¸í„°

```python
training_config = {
    # ì˜µí‹°ë§ˆì´ì €
    "optimizer": "AdamW",
    "learning_rate": 1e-4,
    "weight_decay": 0.01,
    "betas": (0.9, 0.999),

    # ìŠ¤ì¼€ì¤„ëŸ¬
    "scheduler": "linear_warmup_cosine",
    "warmup_ratio": 0.1,
    "min_lr_ratio": 0.1,

    # ì •ê·œí™”
    "gradient_clip": 1.0,
    "reasoning_cost_weight": 0.001,

    # ì—í­
    "max_epochs": 20,
    "patience": 5,  # Early stopping
    "eval_every": 500,  # steps
}
```

### ëª¨ë‹ˆí„°ë§ ë©”íŠ¸ë¦­

```python
metrics_to_track = {
    "performance": ["accuracy", "f1", "exact_match"],
    "efficiency": ["avg_reasoning_steps", "early_termination_rate"],
    "interpretability": ["connection_sparsity", "slot_specialization"],
    "training": ["loss", "grad_norm", "lr", "memory_usage"]
}
```

---

## ì‹¤í—˜ ìŠ¤í¬ë¦½íŠ¸ ì˜ˆì‹œ

### ë©”ì¸ í›ˆë ¨ ìŠ¤í¬ë¦½íŠ¸

```python
#!/usr/bin/env python3
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from transformers import T5Tokenizer, AdamW, get_linear_schedule_with_warmup
import wandb
import os

def setup_experiment():
    # GPU ì„¤ì •
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    torch.backends.cudnn.benchmark = True  # ìµœì í™”

    # ë©”ëª¨ë¦¬ ê´€ë¦¬
    torch.cuda.empty_cache()

    return device

def train_loop(model, train_loader, eval_loader, config):
    # Mixed precision training
    scaler = torch.cuda.amp.GradScaler()

    optimizer = AdamW(model.parameters(),
                     lr=config['learning_rate'],
                     weight_decay=config['weight_decay'])

    scheduler = get_linear_schedule_with_warmup(
        optimizer,
        num_warmup_steps=int(len(train_loader) * config['warmup_ratio']),
        num_training_steps=len(train_loader) * config['max_epochs']
    )

    for epoch in range(config['max_epochs']):
        for step, batch in enumerate(train_loader):
            with torch.cuda.amp.autocast():
                outputs = model(batch['input_ids'], return_reasoning_trace=True)
                logits, reasoning_info = outputs

                # ê¸°ë³¸ ì†ì‹¤
                loss = nn.CrossEntropyLoss()(logits.view(-1, logits.size(-1)),
                                           batch['labels'].view(-1))

                # ì¶”ë¡  ë¹„ìš© ì •ê·œí™”
                reasoning_cost = model.reasoning_cost_loss(
                    reasoning_info['actual_steps'],
                    target_steps=4,
                    weight=config['reasoning_cost_weight']
                )

                total_loss = loss + reasoning_cost

            # Backward pass
            scaler.scale(total_loss).backward()
            scaler.unscale_(optimizer)
            torch.nn.utils.clip_grad_norm_(model.parameters(), config['gradient_clip'])
            scaler.step(optimizer)
            scaler.update()
            scheduler.step()
            optimizer.zero_grad()

            # ë¡œê¹…
            if step % 100 == 0:
                wandb.log({
                    "train_loss": total_loss.item(),
                    "reasoning_steps": reasoning_info['actual_steps'],
                    "learning_rate": scheduler.get_last_lr()[0],
                    "memory_used": torch.cuda.memory_allocated() / 1e9
                })

            # ë©”ëª¨ë¦¬ ì •ë¦¬
            if step % config.get('empty_cache_steps', 100) == 0:
                torch.cuda.empty_cache()
```

### ë°°ì¹˜ ì‹¤í—˜ ìŠ¤í¬ë¦½íŠ¸

```bash
#!/bin/bash
# run_experiments.sh

# Phase 1: ê¸°ë³¸ ê²€ì¦
python train.py --config configs/linear_baseline.yaml --dataset LogiQA
python train.py --config configs/bilinear_base.yaml --dataset LogiQA
python train.py --config configs/bilinear_large.yaml --dataset LogiQA

# Phase 2: N/D ë¹„ìœ¨ (bilinear_base ê¸°ì¤€)
for nd_ratio in "64_256" "128_256" "256_256" "512_256"; do
    python train.py --config configs/nd_${nd_ratio}.yaml --dataset LogiQA
done

# Phase 3: Rank ìµœì í™” (ìµœì  N/Dì—ì„œ)
for rank in 8 16 32 64; do
    python train.py --config configs/rank_${rank}.yaml --dataset LogiQA
done

# ëª¨ë“  ì‹¤í—˜ì„ 3ê°œ ë°ì´í„°ì…‹ì—ì„œ ë°˜ë³µ
for dataset in "LogiQA" "GSM8K" "StrategyQA"; do
    python train.py --config configs/final_best.yaml --dataset $dataset
done
```

---

## ì‹¤í—˜ ì¼ì •

### ì „ì²´ íƒ€ì„ë¼ì¸ (7-10ì¼)

```
Day 1-2: Phase 1 (ê¸°ë³¸ ê²€ì¦)
Day 3-4: Phase 2 (N/D ë¹„ìœ¨)
Day 5-6: Phase 3 (Rank ìµœì í™”)
Day 7: Phase 4 (Dynamic ë¶„ì„)
Day 8-9: ìµœì  ì„¤ì •ìœ¼ë¡œ ëª¨ë“  ë°ì´í„°ì…‹ ì¬ì‹¤í—˜
Day 10: ê²°ê³¼ ì •ë¦¬ ë° ë¶„ì„
```

### ì¼ì¼ ì²´í¬ë¦¬ìŠ¤íŠ¸

```
â–¡ ì‹¤í—˜ ì‹œì‘ ì „ GPU ë©”ëª¨ë¦¬ í™•ì¸
â–¡ wandb ë¡œê¹… ì •ìƒ ì‘ë™ í™•ì¸
â–¡ ì¤‘ê°„ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ í™•ì¸
â–¡ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
â–¡ ì‹¤í—˜ ê²°ê³¼ ë°±ì—…
```

ì´ ì„¤ì •ìœ¼ë¡œ RTX 4090 í•œ ì¥ìœ¼ë¡œë„ ì¶©ë¶„íˆ ì˜ë¯¸ìˆëŠ” ì‹¤í—˜ë“¤ì„ ëŒë¦´ ìˆ˜ ìˆì„ ê²ƒ ê°™ì•„ìš”! ğŸš€
