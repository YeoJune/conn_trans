# final_verification.py
"""
ì‹œìŠ¤í…œ ê²€ì¦ ì „ìš© ìŠ¤í¬ë¦½íŠ¸ - í›ˆë ¨ ì „ í™˜ê²½ê³¼ ì½”ë“œ ê²€ì¦
"""
import torch
import warnings
import sys
from pathlib import Path

warnings.filterwarnings("ignore")

class SystemVerifier:
    """ì‹œìŠ¤í…œ í™˜ê²½ ë° ì½”ë“œ ê²€ì¦"""
    
    def __init__(self):
        self.tests_passed = 0
        self.tests_total = 0
        self.errors = []
    
    def run_test(self, test_name, test_func):
        """ë‹¨ì¼ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        self.tests_total += 1
        print(f"\nğŸ” Testing {test_name}...")
        
        try:
            result = test_func()
            if result:
                print(f"âœ… {test_name} PASSED")
                self.tests_passed += 1
                return True
            else:
                print(f"âŒ {test_name} FAILED")
                self.errors.append(f"{test_name}: Test returned False")
                return False
        except Exception as e:
            print(f"âŒ {test_name} ERROR: {e}")
            self.errors.append(f"{test_name}: {str(e)}")
            return False
    
    def test_basic_imports(self):
        """í•µì‹¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë° ëª¨ë“ˆ import í…ŒìŠ¤íŠ¸"""
        try:
            # ì™¸ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬
            import torch
            import transformers
            
            # ëª¨ë¸ ëª¨ë“ˆ
            from models.connection_transformer import ConnectionTransformer
            from models.baseline_transformer import BaselineTransformer
            
            # í›ˆë ¨ ëª¨ë“ˆ
            from training.trainer import Trainer
            from training.data_collator import T5DataCollator
            
            # ë°ì´í„°ì…‹ ëª¨ë“ˆ
            from dataset.tokenizer_utils import get_tokenizer_and_dataset
            
            # ìœ í‹¸ë¦¬í‹° ëª¨ë“ˆ
            from utils.metrics import calculate_accuracy
            from utils.result_manager import ResultManager
            
            # ì„¤ì • ëª¨ë“ˆ
            import configs.strategyqa_config
            import configs.logiqa_config
            import configs.gsm8k_config
            import configs.multinli_config
            import configs.eli5_config
            import configs.commongen_config
            
            print("   All critical imports successful")
            return True
            
        except ImportError as e:
            print(f"   Import error: {e}")
            return False
    
    def test_config_system(self):
        """ì„¤ì • ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
        try:
            from configs.strategyqa_config import get_config
            
            # ë‹¤ì–‘í•œ ëª¨ë¸ í¬ê¸° í…ŒìŠ¤íŠ¸
            for size in ["micro", "small", "base"]:
                config = get_config(size)
                
                # í•„ìˆ˜ ì†ì„± í™•ì¸
                required_attrs = [
                    'd_model', 'num_slots', 'bilinear_rank', 'max_reasoning_steps',
                    'learning_rate', 'batch_size', 'num_epochs'
                ]
                
                for attr in required_attrs:
                    if not hasattr(config, attr):
                        print(f"   Missing attribute: {attr}")
                        return False
                
                # ê°’ ê²€ì¦
                if config.d_model <= 0 or config.batch_size <= 0:
                    print(f"   Invalid config values for {size}")
                    return False
            
            print("   Config system working correctly")
            return True
            
        except Exception as e:
            print(f"   Config error: {e}")
            return False
    
    def test_model_creation(self):
        """ëª¨ë¸ ìƒì„± ë° ê¸°ë³¸ ìˆœì „íŒŒ í…ŒìŠ¤íŠ¸"""
        try:
            from models.connection_transformer import ConnectionTransformer
            from models.baseline_transformer import BaselineTransformer
            from transformers import T5Tokenizer
            
            # í† í¬ë‚˜ì´ì € ë¡œë“œ
            tokenizer = T5Tokenizer.from_pretrained("google-t5/t5-base")
            vocab_size = tokenizer.vocab_size
            
            # Connection Transformer í…ŒìŠ¤íŠ¸
            conn_model = ConnectionTransformer(
                src_vocab_size=vocab_size,
                tgt_vocab_size=vocab_size,
                d_model=64,
                num_slots=16,
                bilinear_rank=4,
                max_reasoning_steps=2,
                max_seq_len=32,
                src_pad_token_id=tokenizer.pad_token_id,
                tgt_pad_token_id=tokenizer.pad_token_id,
                num_decoder_layers=3,
                num_heads=4
            )
            
            # Baseline Transformer í…ŒìŠ¤íŠ¸
            base_model = BaselineTransformer(
                src_vocab_size=vocab_size,
                tgt_vocab_size=vocab_size,
                d_model=64,
                num_encoder_layers=3,
                num_decoder_layers=3,
                num_heads=4,
                max_seq_len=32,
                src_pad_token_id=tokenizer.pad_token_id,
                tgt_pad_token_id=tokenizer.pad_token_id
            )
            
            # ìˆœì „íŒŒ í…ŒìŠ¤íŠ¸
            batch_size, src_len, tgt_len = 2, 8, 6
            src_ids = torch.randint(1, 1000, (batch_size, src_len))
            tgt_ids = torch.randint(1, 1000, (batch_size, tgt_len))
            src_mask = torch.ones(batch_size, src_len, dtype=torch.bool)
            tgt_mask = torch.ones(batch_size, tgt_len, dtype=torch.bool)
            
            with torch.no_grad():
                # Connection Transformer
                conn_output = conn_model(src_ids, tgt_ids, src_mask, tgt_mask)
                expected_shape = (batch_size, tgt_len, vocab_size)
                if conn_output.shape != expected_shape:
                    print(f"   Wrong Connection output shape: {conn_output.shape} vs {expected_shape}")
                    return False
                
                # Reasoning trace í…ŒìŠ¤íŠ¸
                conn_output, reasoning_info = conn_model(src_ids, tgt_ids, src_mask, tgt_mask, return_reasoning_trace=True)
                if 'actual_steps' not in reasoning_info:
                    print("   Missing reasoning info")
                    return False
                
                # Baseline Transformer
                base_output = base_model(src_ids, tgt_ids, src_mask, tgt_mask)
                if base_output.shape != expected_shape:
                    print(f"   Wrong Baseline output shape: {base_output.shape} vs {expected_shape}")
                    return False
            
            print("   Model creation and forward pass successful")
            return True
            
        except Exception as e:
            print(f"   Model error: {e}")
            return False
    
    def test_dataset_loading(self):
        """ë°ì´í„°ì…‹ ë¡œë”© í…ŒìŠ¤íŠ¸"""
        try:
            from dataset.tokenizer_utils import get_tokenizer_and_dataset
            from configs.strategyqa_config import get_config
            
            # ìµœì†Œ ì„¤ì •ìœ¼ë¡œ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸
            config = get_config("micro")
            
            # ë°ì´í„°ì…‹ ë¡œë”© í…ŒìŠ¤íŠ¸
            tokenizer, train_dataset, eval_dataset = get_tokenizer_and_dataset("strategyqa", config)
            
            # ë°ì´í„°ì…‹ ê²€ì¦
            if len(train_dataset) == 0:
                print("   Empty train dataset")
                return False
            
            if len(eval_dataset) == 0:
                print("   Empty eval dataset")
                return False
            
            # ìƒ˜í”Œ ë°ì´í„° ê²€ì¦
            sample = train_dataset[0]
            required_keys = ['input_ids', 'attention_mask', 'decoder_input_ids', 'labels']
            for key in required_keys:
                if key not in sample:
                    print(f"   Missing key in sample: {key}")
                    return False
                if not torch.is_tensor(sample[key]):
                    print(f"   {key} is not a tensor")
                    return False
            
            print(f"   Dataset loading successful: {len(train_dataset)} train, {len(eval_dataset)} eval")
            return True
            
        except Exception as e:
            print(f"   Dataset error: {e}")
            return False
    
    def test_training_setup(self):
        """í›ˆë ¨ í™˜ê²½ ì„¤ì • í…ŒìŠ¤íŠ¸"""
        try:
            from training.trainer import Trainer
            from models.connection_transformer import ConnectionTransformer
            from configs.strategyqa_config import get_config
            from transformers import T5Tokenizer
            
            # ì„¤ì •
            config = get_config("micro")
            tokenizer = T5Tokenizer.from_pretrained("google-t5/t5-base")
            config.vocab_size = tokenizer.vocab_size
            config.pad_token_id = tokenizer.pad_token_id
            
            model = ConnectionTransformer(
                src_vocab_size=config.vocab_size,
                tgt_vocab_size=config.vocab_size,
                d_model=config.d_model,
                num_slots=config.num_slots,
                bilinear_rank=config.bilinear_rank,
                max_reasoning_steps=config.max_reasoning_steps,
                max_seq_len=config.max_seq_len,
                src_pad_token_id=config.pad_token_id,
                tgt_pad_token_id=config.pad_token_id,
                num_decoder_layers=config.num_decoder_layers,
                num_heads=config.num_heads
            )
            
            # íŠ¸ë ˆì´ë„ˆ ìƒì„±
            trainer = Trainer(model, config, model_type="connection")
            trainer.set_tokenizer(tokenizer)
            
            # íŠ¸ë ˆì´ë„ˆ ê²€ì¦
            if trainer.device.type not in ['cuda', 'cpu']:
                print(f"   Invalid device: {trainer.device}")
                return False
            
            if trainer.model_type != "connection":
                print(f"   Wrong model type: {trainer.model_type}")
                return False
            
            print("   Training setup successful")
            return True
            
        except Exception as e:
            print(f"   Training setup error: {e}")
            return False
    
    def test_metrics_system(self):
        """ë©”íŠ¸ë¦­ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
        try:
            from utils.metrics import calculate_accuracy, extract_final_answer, exact_match_score
            
            # ë©”íŠ¸ë¦­ í•¨ìˆ˜ í…ŒìŠ¤íŠ¸
            predictions = ["Yes", "A", "42", "entailment"]
            targets = ["Yes", "A", "42", "entailment"]
            
            # ì •í™•ë„ ê³„ì‚° í…ŒìŠ¤íŠ¸
            acc = calculate_accuracy(predictions, targets, "strategyqa")
            if acc != 1.0:
                print(f"   Wrong accuracy: {acc} (expected 1.0)")
                return False
            
            # ë‹µë³€ ì¶”ì¶œ í…ŒìŠ¤íŠ¸
            test_cases = [
                ("Yes, this is correct", "strategyqa", "Yes"),
                ("The answer is A", "logiqa", "A"),
                ("42 is the answer", "gsm8k", "42"),
                ("entailment is correct", "multinli", "entailment")
            ]
            
            for text, dataset_type, expected in test_cases:
                result = extract_final_answer(text, dataset_type)
                if result != expected:
                    print(f"   Wrong extraction: '{result}' vs '{expected}' for {dataset_type}")
                    return False
            
            print("   Metrics system working correctly")
            return True
            
        except Exception as e:
            print(f"   Metrics error: {e}")
            return False
    
    def run_all_tests(self):
        """ëª¨ë“  ê²€ì¦ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        print("ğŸš€ Connection Transformer System Verification")
        print("=" * 60)
        
        tests = [
            ("Basic Imports", self.test_basic_imports),
            ("Config System", self.test_config_system),
            ("Model Creation", self.test_model_creation),
            ("Dataset Loading", self.test_dataset_loading),
            ("Training Setup", self.test_training_setup),
            ("Metrics System", self.test_metrics_system)
        ]
        
        for test_name, test_func in tests:
            self.run_test(test_name, test_func)
        
        # ìµœì¢… ë¦¬í¬íŠ¸
        print("\n" + "=" * 60)
        print(f"ğŸ“Š Test Results: {self.tests_passed}/{self.tests_total} passed")
        
        if self.tests_passed == self.tests_total:
            print("ğŸ‰ All tests PASSED! System is ready.")
            print("\nğŸš€ You can now run:")
            print("   python main.py --dataset strategyqa --model connection --model_size micro")
            return True
        else:
            print(f"âŒ {self.tests_total - self.tests_passed} tests FAILED:")
            for error in self.errors:
                print(f"   â€¢ {error}")
            print("\nğŸ”§ Please fix the issues above before training.")
            return False
    
    def quick_test(self):
        """í•„ìˆ˜ í…ŒìŠ¤íŠ¸ë§Œ ë¹ ë¥´ê²Œ ì‹¤í–‰"""
        print("âš¡ Quick System Check")
        print("-" * 30)
        
        essential_tests = [
            ("Imports", self.test_basic_imports),
            ("Config", self.test_config_system),
            ("Models", self.test_model_creation)
        ]
        
        for test_name, test_func in essential_tests:
            if not self.run_test(test_name, test_func):
                print(f"\nâŒ Quick test failed at {test_name}")
                return False
        
        print(f"\nâœ… Quick test passed! ({len(essential_tests)}/{len(essential_tests)})")
        return True

def main():
    """ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸ ë©”ì¸ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description="Verify Connection Transformer system")
    parser.add_argument("--quick", action="store_true", 
                       help="Run quick tests only")
    parser.add_argument("--verbose", action="store_true",
                       help="Show detailed output")
    
    args = parser.parse_args()
    
    if not args.verbose:
        warnings.filterwarnings("ignore")
    
    verifier = SystemVerifier()
    
    if args.quick:
        success = verifier.quick_test()
    else:
        success = verifier.run_all_tests()
    
    return 0 if success else 1

if __name__ == "__main__":
    exit(main())