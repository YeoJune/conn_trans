import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from datasets import load_dataset
import numpy as np
import time
import json
import re
from typing import Dict, List, Tuple
import matplotlib.pyplot as plt
import seaborn as sns
import warnings

# ===== RTX 4090 ÏµúÏ†ÅÌôî ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ =====
CONFIG = {
    # Î™®Îç∏ ÌÅ¨Í∏∞ (4090 24GB ÏµúÏ†ÅÌôî)
    "d_model": 512,
    "num_ir": 1024,     # 2 * d_model
    "num_steps": 4,     # Ï∂îÎ°† Îã®Í≥Ñ (Transformer layersÏôÄ ÎèôÏùº)
    "num_heads": 8,
    "ffn_dim": 2048,    # 4 * d_model
    "dropout": 0.1,
    
    # ÌïôÏäµ ÏÑ§Ï†ï
    "batch_size": 32,   # 4090 Í≥†ÏÑ±Îä• ÌôúÏö©
    "max_seq_len": 128,
    "learning_rate": 1e-4,
    "weight_decay": 0.01,
    "warmup_steps": 500,
    "max_epochs": 15, # Reduced for quicker example, original was 15
    "gradient_clip": 1.0,
    
    # Ï†ïÍ∑úÌôî Î∞è ÏïàÏ†ïÏÑ±
    "c_regularization": 1e-4,
    "spectral_radius_limit": 0.9,  # ÏïàÏ†ïÏÑ±ÏùÑ ÏúÑÌïú Ïä§ÌéôÌä∏Îüº Ï†úÌïú
    "connection_scale": 0.1,       # Connection Ïä§ÏºÄÏùºÎßÅ
}

class PureConnTrans(nn.Module):
    """Pure Connection Transformer - ÏàòÏπò ÏïàÏ†ïÏÑ± Í∞ïÌôî Î≤ÑÏ†Ñ"""
    
    def __init__(self, vocab_size, config=CONFIG):
        super().__init__()
        
        self.config = config
        d_model = config["d_model"]
        num_ir = config["num_ir"]
        num_steps = config["num_steps"]
        num_heads = config["num_heads"]
        
        # ÏûÑÎ≤†Îî©
        self.token_embedding = nn.Embedding(vocab_size, d_model)
        self.pos_embedding = nn.Embedding(1000, d_model) # Max sequence length of 1000 for positions
        
        # Í≥†Ï†ï IR ÎÖ∏Îìú (ÌïôÏäµÎêòÏßÄ ÏïäÏùå)
        self.register_buffer('H', torch.randn(num_ir, d_model) * 0.02)
        
        # Ïó∞Í≤∞ ÌñâÎ†¨ (ÌïµÏã¨ ÌïôÏäµ ÌååÎùºÎØ∏ÌÑ∞!) - ÏïàÏ†ÑÌïú Ï¥àÍ∏∞Ìôî
        self.C = nn.Parameter(self._init_connection_matrix(num_ir))
        
        # Connection Ïä§ÏºÄÏùºÎßÅ (ÌïôÏäµ Í∞ÄÎä•)
        self.connection_scale = nn.Parameter(torch.tensor(config["connection_scale"]))
        
        # Ïñ¥ÌÖêÏÖò
        self.input_attention = nn.MultiheadAttention(d_model, num_heads, batch_first=True)
        self.output_attention = nn.MultiheadAttention(d_model, num_heads, batch_first=True)
        
        # Ï†ïÍ∑úÌôî
        self.input_norm = nn.LayerNorm(d_model)
        self.output_norm = nn.LayerNorm(d_model)
        self.connection_norm = nn.LayerNorm(d_model)  # Connection ÌõÑ Ï†ïÍ∑úÌôî
        
        # Î∂ÑÎ•òÍ∏∞
        self.classifier = nn.Linear(d_model, vocab_size)
        
        # Ï¥àÍ∏∞Ìôî
        self._init_weights()
        
        # ÏàòÏπò ÏïàÏ†ïÏÑ± Î™®ÎãàÌÑ∞ÎßÅ
        self.numerical_warnings = 0
        
        # ÌååÎùºÎØ∏ÌÑ∞ Ïàò Ï∂úÎ†•
        total_params = sum(p.numel() for p in self.parameters())
        print(f"üîπ Pure Conn-Trans: {total_params:,} parameters")
    
    def _init_connection_matrix(self, num_ir):
        """ÏïàÏ†ÑÌïú Connection Matrix Ï¥àÍ∏∞Ìôî"""
        C = torch.randn(num_ir, num_ir) * 0.001
        diagonal_idx = torch.arange(num_ir)
        C[diagonal_idx, diagonal_idx] = -0.1
        C = C * 0.01
        return C
    
    def _init_weights(self):
        nn.init.normal_(self.token_embedding.weight, std=0.02)
        nn.init.normal_(self.pos_embedding.weight, std=0.02)
    
    def spectral_normalize_connection(self):
        """Connection MatrixÏùò Ïä§ÌéôÌä∏Îüº Ï†ïÍ∑úÌôî"""
        with torch.no_grad():
            try:
                eigenvals = torch.linalg.eigvals(self.C)
                spectral_radius = torch.abs(eigenvals).max().real
                
                if spectral_radius > self.config["spectral_radius_limit"]:
                    scale_factor = self.config["spectral_radius_limit"] / spectral_radius
                    self.C.data *= scale_factor
                    if self.numerical_warnings < 3:
                        print(f"‚ö†Ô∏è Connection Matrix Ï†ïÍ∑úÌôî: spectral_radius={spectral_radius:.3f}")
                        self.numerical_warnings += 1
            except Exception as e:
                if self.numerical_warnings < 3:
                    print(f"‚ö†Ô∏è Ïä§ÌéôÌä∏Îüº Í≥ÑÏÇ∞ Ïã§Ìå®: {e}")
                    self.numerical_warnings += 1
    
    def check_numerical_stability(self):
        """ÏàòÏπò ÏïàÏ†ïÏÑ± Ï≤¥ÌÅ¨"""
        C_norm = torch.norm(self.C, 'fro').item()
        C_max = self.C.abs().max().item()
        
        if C_norm > 10 and self.numerical_warnings < 3:
            print(f"‚ö†Ô∏è Warning: C norm large: {C_norm:.3f}")
            self.numerical_warnings += 1
        if C_max > 5 and self.numerical_warnings < 3:
            print(f"‚ö†Ô∏è Warning: C max value large: {C_max:.3f}")
            self.numerical_warnings += 1
        return C_norm, C_max
    
    def forward(self, input_ids, attention_mask=None):
        batch_size, seq_len = input_ids.shape
        device = input_ids.device
        
        if self.training:
            self.spectral_normalize_connection()
            if torch.rand(1).item() < 0.1:
                self.check_numerical_stability()
        
        positions = torch.arange(seq_len, device=device).unsqueeze(0).expand(batch_size, -1)
        token_emb = self.token_embedding(input_ids)
        pos_emb = self.pos_embedding(positions)
        input_emb = token_emb + pos_emb
        
        H_batch = self.H.unsqueeze(0).expand(batch_size, -1, -1)
        X, _ = self.input_attention(
            query=H_batch,
            key=input_emb,
            value=input_emb,
            key_padding_mask=~attention_mask if attention_mask is not None else None
        )
        X = self.input_norm(X)
        
        I = torch.eye(self.config["num_ir"], device=device)
        scaled_C = self.connection_scale * self.C
        
        for step in range(self.config["num_steps"]):
            knowledge_injection = torch.matmul(scaled_C, self.H)
            state_evolution = torch.matmul(I + scaled_C, X)
            X_new = knowledge_injection.unsqueeze(0) + state_evolution
            X = self.connection_norm(X_new)
            X = torch.clamp(X, min=-10, max=10)
        
        H_effective = self.H.unsqueeze(0) + X
        output_states, _ = self.output_attention(
            query=input_emb,
            key=H_effective,
            value=H_effective
        )
        output_states = self.output_norm(output_states)
        logits = self.classifier(output_states)
        return logits
    
    def get_reasoning_trace(self, input_ids, attention_mask=None):
        batch_size, seq_len = input_ids.shape
        device = input_ids.device
        
        positions = torch.arange(seq_len, device=device).unsqueeze(0).expand(batch_size, -1)
        token_emb = self.token_embedding(input_ids)
        pos_emb = self.pos_embedding(positions)
        input_emb = token_emb + pos_emb
        
        H_batch = self.H.unsqueeze(0).expand(batch_size, -1, -1)
        X, _ = self.input_attention(
            query=H_batch, key=input_emb, value=input_emb,
            key_padding_mask=~attention_mask if attention_mask is not None else None
        )
        X = self.input_norm(X)
        
        reasoning_trace = [X.clone()]
        norms = [torch.norm(X, dim=-1).mean().item()]
        
        I = torch.eye(self.config["num_ir"], device=device)
        scaled_C = self.connection_scale * self.C
        
        for step in range(self.config["num_steps"]):
            knowledge_injection = torch.matmul(scaled_C, self.H)
            state_evolution = torch.matmul(I + scaled_C, X)
            X_new = knowledge_injection.unsqueeze(0) + state_evolution
            X = self.connection_norm(X_new)
            X = torch.clamp(X, min=-10, max=10)
            reasoning_trace.append(X.clone())
            norms.append(torch.norm(X, dim=-1).mean().item())
        
        return reasoning_trace, norms
    
    def get_connection_stats(self):
        with torch.no_grad():
            C_scaled = self.connection_scale * self.C
            eigenvals = torch.linalg.eigvals(C_scaled)
            
            return {
                'connection_scale': self.connection_scale.item(),
                'frobenius_norm': torch.norm(C_scaled, 'fro').item(),
                'spectral_radius': torch.abs(eigenvals).max().real.item(),
                'max_eigenval_real': eigenvals.real.max().item(),
                'min_eigenval_real': eigenvals.real.min().item(),
                'condition_number': torch.linalg.cond(C_scaled).item() if C_scaled.shape[0] == C_scaled.shape[1] else float('inf') # Cond only for square
            }

class ConnTransWithFFN(PureConnTrans):
    """Connection Transformer with FFN - ÏàòÏπò ÏïàÏ†ïÏÑ± Í∞ïÌôî"""
    
    def __init__(self, vocab_size, config=CONFIG):
        super().__init__(vocab_size, config)
        
        d_model = config["d_model"]
        ffn_dim = config["ffn_dim"]
        dropout = config["dropout"]
        
        self.reasoning_ffn = nn.Sequential(
            nn.Linear(d_model, ffn_dim),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(ffn_dim, d_model),
            nn.Dropout(dropout)
        )
        self.reasoning_norm2 = nn.LayerNorm(d_model)
        
        total_params = sum(p.numel() for p in self.parameters())
        # Correcting the print statement to reflect it's for ConnTransWithFFN
        print(f"üî∏ Conn-Trans + FFN: {total_params:,} parameters (updated from PureConnTrans count)")


    def forward(self, input_ids, attention_mask=None):
        batch_size, seq_len = input_ids.shape
        device = input_ids.device
        
        if self.training:
            self.spectral_normalize_connection()
            if torch.rand(1).item() < 0.1:
                self.check_numerical_stability()
        
        positions = torch.arange(seq_len, device=device).unsqueeze(0).expand(batch_size, -1)
        token_emb = self.token_embedding(input_ids)
        pos_emb = self.pos_embedding(positions)
        input_emb = token_emb + pos_emb
        
        H_batch = self.H.unsqueeze(0).expand(batch_size, -1, -1)
        X, _ = self.input_attention(
            query=H_batch, key=input_emb, value=input_emb,
            key_padding_mask=~attention_mask if attention_mask is not None else None
        )
        X = self.input_norm(X)
        
        I = torch.eye(self.config["num_ir"], device=device)
        scaled_C = self.connection_scale * self.C
        
        for step in range(self.config["num_steps"]):
            knowledge_injection = torch.matmul(scaled_C, self.H)
            state_evolution = torch.matmul(I + scaled_C, X)
            X_conn = knowledge_injection.unsqueeze(0) + state_evolution
            X_conn = self.connection_norm(X_conn)
            
            X_ffn = X_conn + self.reasoning_ffn(X_conn)
            X = self.reasoning_norm2(X_ffn)
            X = torch.clamp(X, min=-10, max=10)
        
        H_effective = self.H.unsqueeze(0) + X
        output_states, _ = self.output_attention(
            query=input_emb, key=H_effective, value=H_effective
        )
        output_states = self.output_norm(output_states)
        logits = self.classifier(output_states)
        return logits

class StandardTransformer(nn.Module):
    """Standard Transformer - Í≥µÏ†ïÌïú ÎπÑÍµêÎ•º ÏúÑÌïú Î≤†Ïù¥Ïä§ÎùºÏù∏"""
    
    def __init__(self, vocab_size, config=CONFIG):
        super().__init__()
        
        self.config = config
        d_model = config["d_model"]
        num_heads = config["num_heads"]
        num_layers = config["num_steps"]
        ffn_dim = config["ffn_dim"]
        dropout = config["dropout"]
        
        self.token_embedding = nn.Embedding(vocab_size, d_model)
        self.pos_embedding = nn.Embedding(1000, d_model) # Max sequence length of 1000 for positions
        
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=num_heads,
            dim_feedforward=ffn_dim,
            dropout=dropout,
            activation='gelu',
            batch_first=True,
            norm_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)
        self.norm = nn.LayerNorm(d_model)
        self.classifier = nn.Linear(d_model, vocab_size)
        self._init_weights()
        
        total_params = sum(p.numel() for p in self.parameters())
        print(f"üî∂ Standard Transformer: {total_params:,} parameters")
    
    def _init_weights(self):
        nn.init.normal_(self.token_embedding.weight, std=0.02)
        nn.init.normal_(self.pos_embedding.weight, std=0.02)
    
    def forward(self, input_ids, attention_mask=None):
        batch_size, seq_len = input_ids.shape
        device = input_ids.device
        
        positions = torch.arange(seq_len, device=device).unsqueeze(0).expand(batch_size, -1)
        token_emb = self.token_embedding(input_ids)
        pos_emb = self.pos_embedding(positions)
        x = token_emb + pos_emb
        
        src_key_padding_mask = ~attention_mask if attention_mask is not None else None
        
        x = self.transformer(x, src_key_padding_mask=src_key_padding_mask)
        x = self.norm(x)
        logits = self.classifier(x)
        return logits

class BabiDataset(Dataset):
    """bAbI Task Dataset - ÏµúÏã† HuggingFace Î∞©Ïãù ÎåÄÏùë"""
    
    def __init__(self, task_id=16, split='train', max_seq_len=128, type='en'):
        self.max_seq_len = max_seq_len
        self.task_id = task_id
        self.split = split
        self.type = type
        
        print(f"üì¶ Loading bAbI task {task_id} ({split}, type={type})...")

        task_name = f"qa{task_id}"
        try:
            # Ensure 'name' argument is used instead of 'task_no' for new versions if needed
            # For "facebook/babi_qa", 'task_no' is correct for the specific config name.
            dataset = load_dataset("facebook/babi_qa", name=type, task_no=task_name)
        except Exception as e:
            # Trying with 'name' as task_no for some loaders, or default behavior
            try:
                dataset = load_dataset("facebook/babi_qa", type, task_no=task_name) # Original call
            except Exception as e_inner:
                 raise RuntimeError(f"‚ùå Failed to load bAbI dataset: {e_inner} (Outer exception: {e})")


        # bAbI typically has 'train' and 'test' splits.
        if split not in dataset:
            suggestion = " Consider using 'test' for validation." if split == 'validation' else ""
            # Corrected the task_name variable for the f-string
            raise ValueError(f"‚ùå Split '{split}' not found in dataset for task {task_name}. Available: {list(dataset.keys())}.{suggestion}")

        self.raw_data = dataset[split]
        self.data = self._convert_format()
        print(f"‚úÖ Loaded {len(self.data)} examples for split '{split}'")

        self.vocab = self._build_vocab()
        self.word_to_id = {word: i for i, word in enumerate(self.vocab)}
        self.vocab_size = len(self.vocab)
        print(f"üî§ Vocabulary size (from {split} data): {self.vocab_size}")

    def _convert_format(self):
        converted_data = []
        for example in self.raw_data:
            converted_data.append({
                'story': example.get('story', []),
                'question': example.get('question', ''),
                'answer': example.get('answer', ''),
                'task': self.task_id
            })
        return converted_data

    def _build_vocab(self):
        # Vocab should ideally be built from the training set and reused for validation/test
        # For simplicity here, it's built per dataset object.
        # Consider passing vocab for val/test splits.
        vocab = set(['<PAD>', '<UNK>', '<SEP>'])
        for ex in self.data:
            story_words = ' '.join(ex['story']).lower().split()
            question_words = ex['question'].lower().split()
            answer_words = ex['answer'].lower().split()
            for word in story_words + question_words + answer_words:
                clean = re.sub(r'[^\w]', '', word)
                if clean:
                    vocab.add(clean)
        return ['<PAD>', '<UNK>', '<SEP>'] + sorted(vocab - {'<PAD>', '<UNK>', '<SEP>'})

    def _tokenize(self, text):
        words = re.findall(r'\w+', text.lower())
        return [self.word_to_id.get(w, self.word_to_id['<UNK>']) for w in words]

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        ex = self.data[idx]
        story_text = ' '.join(ex['story'])
        input_text = f"{story_text} <SEP> {ex['question']}"
        input_ids = self._tokenize(input_text)
        answer_ids = self._tokenize(ex['answer'])

        input_ids = input_ids[:self.max_seq_len - 1] # Leave space for potential EOS if needed, though not used here
        input_length = len(input_ids)
        
        padded_input_ids = input_ids + [self.word_to_id['<PAD>']] * (self.max_seq_len - input_length)
        attention_mask = [1] * input_length + [0] * (self.max_seq_len - input_length)
        
        # Ensure answer_ids are also padded/truncated if they were to be used as decoder inputs
        # For this model, only the first answer token is used for loss.
        # If answer_ids is empty, provide a default PAD token to avoid errors.
        if not answer_ids:
            answer_ids = [self.word_to_id['<PAD>']]
            
        return {
            'input_ids': torch.tensor(padded_input_ids, dtype=torch.long),
            'attention_mask': torch.tensor(attention_mask, dtype=torch.bool),
            'answer_ids': torch.tensor(answer_ids, dtype=torch.long), # May need padding if used differently
            'answer_text': ex['answer']
        }

def train_model(model, train_loader, val_loader, config=CONFIG, device='cuda', model_name="Model"):
    """ÏïàÏ†ÑÌïú Î™®Îç∏ ÌïôÏäµ"""
    model = model.to(device)
    
    optimizer = optim.AdamW(
        model.parameters(),
        lr=config["learning_rate"],
        weight_decay=config["weight_decay"]
    )
    
    scheduler = optim.lr_scheduler.OneCycleLR(
        optimizer,
        max_lr=config["learning_rate"],
        steps_per_epoch=len(train_loader),
        epochs=config["max_epochs"]
    )
    
    best_val_acc = 0
    training_unstable = False
    
    print(f"\nüöÄ Training {model_name}...")
    print("=" * 50)
    print(f"üìç CHECKPOINT: Starting training for {model_name}.")
    
    for epoch in range(config["max_epochs"]):
        print(f"üìç CHECKPOINT: Starting Epoch {epoch+1}/{config['max_epochs']} for {model_name}.")
        model.train()
        train_loss = 0
        train_correct = 0
        train_total = 0
        start_time = time.time()
        
        for batch_idx, batch in enumerate(train_loader):
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            answer_ids = batch['answer_ids'].to(device)
            
            # Ensure answer_ids is not empty before accessing its first element
            if answer_ids.size(1) == 0: # If answer_ids is [B, 0]
                print(f"‚ö†Ô∏è Skipping batch {batch_idx} due to empty answer_ids for {model_name}.")
                continue

            try:
                logits = model(input_ids, attention_mask)
                
                if torch.isnan(logits).any():
                    print(f"‚ö†Ô∏è NaN detected in logits at epoch {epoch+1}, batch {batch_idx} for {model_name}.")
                    print(f"üìç CHECKPOINT: NaN in logits. Training unstable for {model_name}.")
                    training_unstable = True
                    break
                
                last_token_logits = logits[:, -1, :] 
                first_answer_token = answer_ids[:, 0] 
                
                loss = F.cross_entropy(last_token_logits, first_answer_token)
                
                if hasattr(model, 'C'):
                    c_reg = config["c_regularization"] * torch.norm(model.C, 'fro')
                    loss = loss + c_reg
                
                if torch.isnan(loss):
                    print(f"‚ö†Ô∏è NaN detected in loss at epoch {epoch+1}, batch {batch_idx} for {model_name}.")
                    print(f"üìç CHECKPOINT: NaN in loss. Training unstable for {model_name}.")
                    training_unstable = True
                    break
                
                optimizer.zero_grad()
                loss.backward()
                total_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), config["gradient_clip"])
                if total_norm > 10 and total_norm != float('inf'): # Added inf check
                    print(f"‚ö†Ô∏è Large gradient norm: {total_norm:.3f} for {model_name} at epoch {epoch+1}, batch {batch_idx}.")
                
                optimizer.step()
                scheduler.step()
                
                train_loss += loss.item()
                predicted = torch.argmax(last_token_logits, dim=1)
                train_correct += (predicted == first_answer_token).sum().item()
                train_total += input_ids.size(0)
                
                if batch_idx % 50 == 0:
                    print(f"  Epoch {epoch+1}, Batch {batch_idx}/{len(train_loader)}, Loss: {loss.item():.4f}")
                    if hasattr(model, 'get_connection_stats') and batch_idx % 200 == 0:
                        stats = model.get_connection_stats()
                        print(f"    Connection stats: scale={stats['connection_scale']:.3f}, "
                              f"spectral_radius={stats['spectral_radius']:.3f}")
                        
            except RuntimeError as e:
                print(f"‚ùå Runtime error at epoch {epoch+1}, batch {batch_idx} for {model_name}: {e}")
                print(f"üìç CHECKPOINT: Runtime error. Training unstable for {model_name}.")
                training_unstable = True
                break
        
        if training_unstable:
            print(f"‚ùå Training unstable for {model_name}, stopping early at epoch {epoch+1}.")
            break
        
        model.eval()
        val_correct = 0
        val_total = 0
        val_loss = 0
        
        with torch.no_grad():
            for batch in val_loader:
                input_ids = batch['input_ids'].to(device)
                attention_mask = batch['attention_mask'].to(device)
                answer_ids = batch['answer_ids'].to(device)

                if answer_ids.size(1) == 0:
                    print(f"‚ö†Ô∏è Skipping validation batch due to empty answer_ids for {model_name}.")
                    continue
                
                try:
                    logits = model(input_ids, attention_mask)
                    last_token_logits = logits[:, -1, :]
                    first_answer_token = answer_ids[:, 0]
                    
                    loss = F.cross_entropy(last_token_logits, first_answer_token)
                    val_loss += loss.item()
                    
                    predicted = torch.argmax(last_token_logits, dim=1)
                    val_correct += (predicted == first_answer_token).sum().item()
                    val_total += input_ids.size(0)
                except RuntimeError as e:
                    print(f"‚ö†Ô∏è Validation error for {model_name}: {e}")
                    continue
        
        epoch_time = time.time() - start_time
        train_acc = train_correct / train_total if train_total > 0 else 0
        val_acc = val_correct / val_total if val_total > 0 else 0
        avg_train_loss = train_loss / len(train_loader) if len(train_loader) > 0 else 0
        avg_val_loss = val_loss / len(val_loader) if len(val_loader) > 0 else 0

        print(f"  Epoch {epoch + 1}/{config['max_epochs']} Summary for {model_name}:")
        print(f"    Train Loss: {avg_train_loss:.4f}, Train Acc: {train_acc:.4f}")
        print(f"    Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.4f}")
        print(f"    Time: {epoch_time:.1f}s")
        print(f"üìç CHECKPOINT: End of Epoch {epoch+1} for {model_name}. Val Acc: {val_acc:.4f}.")

        if hasattr(model, 'get_connection_stats'):
            stats = model.get_connection_stats()
            print(f"    Connection: scale={stats['connection_scale']:.3f}, "
                  f"spectral_radius={stats['spectral_radius']:.3f}, "
                  f"condition_number={stats['condition_number']:.2f}")
        
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            torch.save(model.state_dict(), f'best_model_{model_name.replace(" ", "_")}.pt')
            print(f"    üíæ New best model saved with Val Acc: {best_val_acc:.4f}")
        
        print("-" * 30)
    
    if training_unstable:
        print(f"‚ö†Ô∏è {model_name} training was unstable. Best Val Acc during attempted training: {best_val_acc:.4f}")
    else:
        print(f"‚úÖ {model_name} training completed. Best Val Acc: {best_val_acc:.4f}")
    print(f"üìç CHECKPOINT: Finished training for {model_name}.")
    return best_val_acc

def print_comparison_results(results_dict):
    """Î™®Îì† Î™®Îç∏ Í≤∞Í≥º ÎπÑÍµê Ï∂úÎ†•"""
    print("\n" + "üéØ COMPREHENSIVE MODEL COMPARISON" + "\n")
    print("=" * 70)
    
    if not results_dict:
        print("  No results to compare.")
        return

    print("üèÜ Performance Ranking:")
    # Handle cases where accuracy might be 0 or None if training failed
    valid_results = {k: v for k, v in results_dict.items() if v is not None}
    if not valid_results:
        print("  No valid results for ranking.")
        print("=" * 70)
        return

    sorted_results = sorted(valid_results.items(), key=lambda x: x[1], reverse=True)
    
    for i, (model_name, acc) in enumerate(sorted_results, 1):
        emoji = "ü•á" if i == 1 else "ü•à" if i == 2 else "ü•â" if i == 3 else "üìä"
        print(f"  {emoji} {model_name:<25}: {acc:.4f}")
    
    print(f"\nüìä Performance Gaps:")
    best_acc = sorted_results[0][1]
    best_model = sorted_results[0][0]
    
    print(f"  üèÜ Champion: {best_model} ({best_acc:.4f})")
    
    for model_name, acc in sorted_results[1:]:
        gap = best_acc - acc
        gap_pct = (gap / best_acc) * 100 if best_acc > 0 else 0
        print(f"  üìâ {model_name}: -{gap:.4f} (-{gap_pct:.1f}%)")
    
    conn_pure = results_dict.get('Pure Conn-Trans', 0)
    conn_ffn = results_dict.get('Conn-Trans + FFN', 0)
    standard = results_dict.get('Standard Transformer', 0)
    
    print(f"\nüß† Architecture Analysis:")
    
    if conn_pure is not None and standard is not None and standard > 0: # Check for None and standard > 0
        pure_vs_standard = ((conn_pure - standard) / standard) * 100
        print(f"  üîπ Pure vs Standard: {pure_vs_standard:+.1f}%")
        if pure_vs_standard >= -5: print(f"    ‚úÖ Pure Connection competitive! Novel mechanism validated.")
        elif pure_vs_standard >= -15: print(f"    üìà Pure Connection promising. Acceptable gap.")
        else: print(f"    ü§î Pure Connection needs improvement.")
    elif conn_pure is None or standard is None:
        print(f"  üîπ Pure vs Standard: N/A (one or both models did not complete training)")

    if conn_ffn is not None and conn_pure is not None and conn_pure > 0: # Check for None and conn_pure > 0
        ffn_improvement = ((conn_ffn - conn_pure) / conn_pure) * 100
        print(f"  üî∏ FFN Effect: +{ffn_improvement:.1f}%")
        if ffn_improvement > 10: print(f"    üöÄ FFN provides significant boost!")
        elif ffn_improvement > 3: print(f"    ‚úÖ FFN helps moderately.")
        else: print(f"    ü§∑ FFN effect minimal.")
    elif conn_ffn is None or conn_pure is None:
         print(f"  üî∏ FFN Effect: N/A (one or both models did not complete training)")


    if conn_ffn is not None and standard is not None and standard > 0: # Check for None and standard > 0
        ffn_vs_standard = ((conn_ffn - standard) / standard) * 100
        print(f"  üî∏ FFN vs Standard: {ffn_vs_standard:+.1f}%")
    elif conn_ffn is None or standard is None:
        print(f"  üî∏ FFN vs Standard: N/A (one or both models did not complete training)")
    
    print(f"\n‚ö° Parameter Efficiency (Approximate - see model init for exact):") # Clarified approximation
    print(f"  Pure Conn-Trans: ~20M params") # These are example numbers
    print(f"  Conn-Trans + FFN: ~30M params") 
    print(f"  Standard Transformer: ~25M params")
    
    if conn_pure is not None and standard is not None and standard > 0:
        # Parameter ratio is an example. Actual ratio should be calculated from model param counts.
        param_ratio_pure_vs_std = (20/25) # Example ratio
        eff_score_pure = (conn_pure / standard) / param_ratio_pure_vs_std if standard > 0 and param_ratio_pure_vs_std > 0 else 0
        print(f"  üìä Pure Efficiency Score (vs Standard, illustrative): {eff_score_pure:.2f}")

    print(f"\nüéØ Key Insights:")
    if conn_pure is not None and standard is not None and conn_pure >= standard * 0.95:
        print(f"  üéâ Pure Connection mechanism successfully validated!")
    if conn_ffn is not None and conn_pure is not None and standard is not None and conn_ffn > max(conn_pure, standard):
        print(f"  üèÜ Connection + FFN achieves best performance (among completed models)")
    if conn_pure is not None and standard is not None and conn_pure < standard * 0.85:
         print(f"  üìö Standard Transformer shows superiority (if both completed)")
    
    print(f"\nüöÄ Research Contributions:")
    print(f"  (Based on the design and intent of the experiment)")
    print(f"  üìê Novel interpretable reasoning mechanism")
    print(f"  üõ°Ô∏è Numerical stability considerations addressed")
    print("=" * 70)

def visualize_connection_matrix(model, save_path="connection_matrix.png", title_suffix=""):
    if not hasattr(model, 'C'):
        print("Model doesn't have Connection Matrix")
        return
    
    C_param = model.C
    if hasattr(model, 'connection_scale'):
        C_numpy = (model.connection_scale.detach() * C_param.detach()).cpu().numpy()
        scale_info = f" (scale: {model.connection_scale.item():.3f})"
    else:
        C_numpy = C_param.detach().cpu().numpy()
        scale_info = ""
    
    plt.figure(figsize=(12, 10))
    sns.heatmap(C_numpy, cmap='RdBu_r', center=0, cbar=True, 
                square=True, linewidths=0.01, cbar_kws={"shrink": .8})
    plt.title(f'Connection Matrix (C){title_suffix}{scale_info}\nLearned Reasoning Patterns')
    plt.xlabel('IR Node Index')
    plt.ylabel('IR Node Index')
    plt.tight_layout()
    try:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"üíæ Connection Matrix saved to {save_path}")
    except Exception as e:
        print(f"‚ö†Ô∏è Could not save Connection Matrix: {e}")
    plt.close()
    
    print(f"Matrix stats: min={C_numpy.min():.3f}, max={C_numpy.max():.3f}, "
          f"norm={np.linalg.norm(C_numpy):.3f}, mean={C_numpy.mean():.3f}")
    try:
        if C_numpy.shape[0] == C_numpy.shape[1]: # Eigenvalues only for square matrices
            eigenvals = np.linalg.eigvals(C_numpy)
            spectral_radius = np.abs(eigenvals).max()
            print(f"Spectral radius: {spectral_radius:.3f}")
            print(f"Eigenvalue range (real part): [{eigenvals.real.min():.3f}, {eigenvals.real.max():.3f}]")
    except np.linalg.LinAlgError:
        print("‚ö†Ô∏è Could not compute eigenvalues for Connection Matrix (possibly singular or ill-conditioned).")
    except Exception as e:
        print(f"‚ö†Ô∏è Error computing eigenvalues: {e}")


def analyze_reasoning_evolution(model, sample_input, save_path="reasoning_evolution.png"):
    if not hasattr(model, 'get_reasoning_trace'):
        print("Model doesn't support reasoning trace")
        return
    
    model.eval()
    with torch.no_grad():
        # Ensure sample_input tensors are on the same device as the model
        device = next(model.parameters()).device
        input_ids = sample_input['input_ids'].unsqueeze(0).to(device)
        attention_mask = sample_input['attention_mask'].unsqueeze(0).to(device)
        trace, norms = model.get_reasoning_trace(input_ids, attention_mask)
    
    plt.figure(figsize=(10, 6))
    plt.plot(range(len(norms)), norms, 'o-', linewidth=2, markersize=8)
    plt.xlabel('Reasoning Step')
    plt.ylabel('Average Activation Norm')
    plt.title('Reasoning State Evolution')
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    try:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"üíæ Reasoning evolution saved to {save_path}")
    except Exception as e:
        print(f"‚ö†Ô∏è Could not save reasoning evolution plot: {e}")
    plt.close()
    
    print(f"Norm evolution: {' ‚Üí '.join([f'{n:.2f}' for n in norms])}")
    return trace, norms

def create_dummy_babi_dataset(size, task_id, config):
    """bAbI Îç∞Ïù¥ÌÑ∞ Î°úÎî© Ïã§Ìå®Ïãú ÎçîÎØ∏ Îç∞Ïù¥ÌÑ∞ÏÖã ÏÉùÏÑ±"""
    class DummyBabiDataset(Dataset): # Changed to inner class to capture config
        def __init__(self, size, task_id, vocab_ref=None): # vocab_ref for consistency if needed
            self.data = []
            self.vocab = ['<PAD>', '<UNK>', '<SEP>', 'if', 'then', 'is', 'what', 'where', 
                         'john', 'mary', 'kitchen', 'garden', 'green', 'frog', 'color', 'yes', 'no',
                         'apple', 'football', 'bedroom', 'office', 'journeyed', 'travelled', 'moved',
                         'got', 'took', 'discarded', 'put', 'down', 'picked', 'up', 'left', 'the', 'a',
                         'to', 'in', 'red', 'blue', 'yellow', 'bill', 'sandra', 'daniel', 'julie'] # Expanded vocab
            
            self.word_to_id = {word: i for i, word in enumerate(self.vocab)}
            self.vocab_size = len(self.vocab)
            self.max_seq_len = config["max_seq_len"] # Use from outer scope config
            
            templates = [
                ("mary moved to the bathroom", "john went to the hallway", "where is mary", "bathroom"),
                ("daniel was in the kitchen", "sandra picked up the milk", "where is daniel", "kitchen"),
                ("john travelled to the office", "john took the football there", "what did john take", "football"),
                ("julie is in the bedroom", "bill is in the garden", "is julie in the garden", "no"),
                ("frogs are green", "mice are grey", "what color is a frog", "green"),
            ]
            
            for i in range(size):
                template = templates[i % len(templates)]
                self.data.append({
                    'story': [template[0], template[1]],
                    'question': template[2],
                    'answer': template[3],
                    'task': task_id
                })
        
        def _tokenize(self, text):
            words = text.lower().split() # Simple split for dummy
            return [self.word_to_id.get(word, self.word_to_id['<UNK>']) for word in words]
        
        def __len__(self):
            return len(self.data)
        
        def __getitem__(self, idx):
            example = self.data[idx]
            story_text = ' '.join(example['story'])
            input_text = f"{story_text} <SEP> {example['question']}"
            
            input_ids = self._tokenize(input_text)
            answer_ids = self._tokenize(example['answer'])
            
            input_ids = input_ids[:self.max_seq_len -1] # Truncate
            input_length = len(input_ids)
            padded_input_ids = input_ids + [self.word_to_id['<PAD>']] * (self.max_seq_len - input_length)
            attention_mask = [1] * input_length + [0] * (self.max_seq_len - input_length)

            if not answer_ids: # Handle empty answer
                answer_ids = [self.word_to_id['<PAD>']]

            return {
                'input_ids': torch.tensor(padded_input_ids, dtype=torch.long),
                'attention_mask': torch.tensor(attention_mask, dtype=torch.bool),
                'answer_ids': torch.tensor(answer_ids, dtype=torch.long),
                'answer_text': example['answer']
            }
    
    print(f"üìç CHECKPOINT: Creating DummyBabiDataset with size {size}, task {task_id}, max_seq_len {config['max_seq_len']}.")
    return DummyBabiDataset(size, task_id)


def main():
    """Î©îÏù∏ Ïã§Ìóò - ÏàòÏπò ÏïàÏ†ïÏÑ± Í∞ïÌôî Î∞è Îç∞Ïù¥ÌÑ∞ Î°úÎî© ÏµúÏã†Ìôî Î≤ÑÏ†Ñ"""
    print("üìç CHECKPOINT: main() function started.")
    print("üöÄ CONN-TRANS vs STANDARD TRANSFORMER")
    print("üî¨ Comprehensive Comparison with Numerical Stability")
    print("=" * 70)
    # ... (initial prints)
    print("Hardware: RTX 4090 (24GB) - Target") # Clarify target
    # ...
    print("=" * 70)
    
    print("üìç CHECKPOINT: Setting up CUDA optimizations.")
    if torch.cuda.is_available():
        torch.backends.cudnn.benchmark = True
        torch.backends.cudnn.deterministic = False # False for benchmark=True usually
    
    print("\nüì¶ Data Loading (Updated 2024)...")
    print("üìç CHECKPOINT: Attempting to load bAbI dataset.")
    train_dataset, val_dataset = None, None
    data_source_type = "Unknown"

    try:
        # For bAbI, vocab should be built on train and applied to val/test
        # Simplification: each dataset object builds its own vocab.
        # For more rigorous setup, pass train_dataset.word_to_id and vocab_size to val_dataset
        train_dataset = BabiDataset(task_id=16, split='train', max_seq_len=CONFIG["max_seq_len"])
        # Use 'test' split for validation as 'validation' split is often not available for bAbI in HF datasets
        val_dataset = BabiDataset(task_id=16, split='test', max_seq_len=CONFIG["max_seq_len"]) #, vocab=train_dataset.vocab, word_to_id=train_dataset.word_to_id)
        
        # To ensure consistent vocab:
        # val_dataset.vocab = train_dataset.vocab
        # val_dataset.word_to_id = train_dataset.word_to_id
        # val_dataset.vocab_size = train_dataset.vocab_size
        # This would require BabiDataset to accept vocab parameters. For now, keeping it simple.

        print("‚úÖ Îç∞Ïù¥ÌÑ∞ Î°úÎî© ÏÑ±Í≥µ (real data)")
        data_source_type = "Real bAbI Dataset"
        print("üìç CHECKPOINT: Successfully loaded real bAbI dataset.")
        
    except Exception as e:
        print(f"‚ùå Îç∞Ïù¥ÌÑ∞ Î°úÎî© Ïã§Ìå®: {e}")
        # ... (error messages)
        print("\n‚ö†Ô∏è ÎçîÎØ∏ Îç∞Ïù¥ÌÑ∞Î°ú ÏïÑÌÇ§ÌÖçÏ≤ò ÌÖåÏä§Ìä∏ Í≥ÑÏÜç ÏßÑÌñâ")
        print("üìç CHECKPOINT: Real dataset loading failed. Falling back to dummy dataset.")
        # Pass CONFIG to dummy dataset creator
        train_dataset = create_dummy_babi_dataset(1000, 16, CONFIG)
        val_dataset = create_dummy_babi_dataset(200, 16, CONFIG)
        # Ensure dummy val_dataset uses the same vocab as dummy train_dataset
        val_dataset.vocab = train_dataset.vocab
        val_dataset.word_to_id = train_dataset.word_to_id
        val_dataset.vocab_size = train_dataset.vocab_size
        print("üîß ÎçîÎØ∏ Îç∞Ïù¥ÌÑ∞ÏÖã ÏÉùÏÑ± ÏôÑÎ£å")
        data_source_type = "Dummy Fallback Dataset"
        print("üìç CHECKPOINT: Dummy dataset created and being used.")

    if train_dataset is None or val_dataset is None:
        print("‚ùå CRITICAL: Dataset not loaded. Exiting.")
        return {} # Return empty if no data

    print("üìç CHECKPOINT: Creating DataLoaders.")
    train_loader = DataLoader(
        train_dataset, batch_size=CONFIG["batch_size"], shuffle=True,
        num_workers=min(4, CONFIG["batch_size"] // 2 if CONFIG["batch_size"] > 1 else 0), # Adjust num_workers
        pin_memory=torch.cuda.is_available() # pin_memory only if CUDA is used
    )
    val_loader = DataLoader(
        val_dataset, batch_size=CONFIG["batch_size"], shuffle=False,
        num_workers=min(4, CONFIG["batch_size"] // 2 if CONFIG["batch_size"] > 1 else 0),
        pin_memory=torch.cuda.is_available()
    )
    
    vocab_size = train_dataset.vocab_size
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    
    print(f"  ‚úÖ Device: {device}")
    print(f"  üìö Vocabulary: {vocab_size:,} tokens (from {'train' if data_source_type.startswith('Real') else 'dummy'} data)")
    print(f"  üî¢ Train samples: {len(train_dataset):,}")
    print(f"  üî¢ Val samples: {len(val_dataset):,}")
    print(f"  üì¶ Batch size: {CONFIG['batch_size']}")
    print(f"  üìä Data Source: {data_source_type}")
    print("üìç CHECKPOINT: Data loading and setup complete.")
    
    results = {}
    model_stats = {}
    
    # 1. Pure Conn-Trans Ïã§Ìóò
    print("\n" + "="*60)
    print("üîπ EXPERIMENT 1: Pure Connection Transformer")
    # ... (experiment prints)
    print("üìç CHECKPOINT: Starting Experiment 1: Pure Conn-Trans.")
    pure_model = PureConnTrans(vocab_size, CONFIG)
    pure_acc = train_model(pure_model, train_loader, val_loader, CONFIG, device, "Pure Conn-Trans")
    results['Pure Conn-Trans'] = pure_acc
    print("üìç CHECKPOINT: Finished Experiment 1: Pure Conn-Trans.")
    
    print(f"\nüìä Pure Model Analysis:")
    # ... (analysis prints)
    if pure_acc is not None and pure_acc > 0 :
        try:
            pure_stats = pure_model.get_connection_stats()
            model_stats['Pure Conn-Trans'] = pure_stats
            # ... (stats prints)
            visualize_connection_matrix(pure_model, "pure_connection_matrix.png", " (Pure)")
            sample_data = val_dataset[0]
            analyze_reasoning_evolution(pure_model, sample_data, "pure_reasoning_evolution.png")
        except Exception as e_viz:
            print(f"‚ö†Ô∏è Error during Pure Conn-Trans visualization/analysis: {e_viz}")

    del pure_model
    if torch.cuda.is_available(): torch.cuda.empty_cache()
    print("üìç CHECKPOINT: Pure Conn-Trans model deleted and cache cleared.")
    
    # 2. Standard Transformer Ïã§Ìóò  
    print("\n" + "="*60)
    print("üî∂ EXPERIMENT 2: Standard Transformer")
    # ... (experiment prints)
    print("üìç CHECKPOINT: Starting Experiment 2: Standard Transformer.")
    standard_model = StandardTransformer(vocab_size, CONFIG)
    standard_acc = train_model(standard_model, train_loader, val_loader, CONFIG, device, "Standard Transformer")
    results['Standard Transformer'] = standard_acc
    print("üìç CHECKPOINT: Finished Experiment 2: Standard Transformer.")
    
    print(f"\nüìä Standard Model Analysis:")
    # ... (analysis prints)
    del standard_model
    if torch.cuda.is_available(): torch.cuda.empty_cache()
    print("üìç CHECKPOINT: Standard Transformer model deleted and cache cleared.")
    
    # 3. Conn-Trans with FFN Ïã§Ìóò
    print("\n" + "="*60)
    print("üî∏ EXPERIMENT 3: Connection Transformer + FFN")
    # ... (experiment prints)
    print("üìç CHECKPOINT: Starting Experiment 3: Conn-Trans + FFN.")
    ffn_model = ConnTransWithFFN(vocab_size, CONFIG)
    ffn_acc = train_model(ffn_model, train_loader, val_loader, CONFIG, device, "Conn-Trans + FFN")
    results['Conn-Trans + FFN'] = ffn_acc
    print("üìç CHECKPOINT: Finished Experiment 3: Conn-Trans + FFN.")

    print(f"\nüìä FFN Model Analysis:")
    # ... (analysis prints)
    if ffn_acc is not None and ffn_acc > 0:
        try:
            ffn_stats = ffn_model.get_connection_stats()
            model_stats['Conn-Trans + FFN'] = ffn_stats
            # ... (stats prints)
            visualize_connection_matrix(ffn_model, "ffn_connection_matrix.png", " (FFN)")
            sample_data = val_dataset[0] # Re-fetch sample data in case val_dataset was modified
            analyze_reasoning_evolution(ffn_model, sample_data, "ffn_reasoning_evolution.png")
        except Exception as e_viz_ffn:
            print(f"‚ö†Ô∏è Error during Conn-Trans + FFN visualization/analysis: {e_viz_ffn}")
            
    del ffn_model
    if torch.cuda.is_available(): torch.cuda.empty_cache()
    print("üìç CHECKPOINT: Conn-Trans + FFN model deleted and cache cleared.")
    
    print("üìç CHECKPOINT: Starting comprehensive analysis and results comparison.")
    print_comparison_results(results)
    print("üìç CHECKPOINT: Finished comprehensive analysis.")
    
    if len(model_stats) >= 1: # Changed to >=1 to print if any model has stats
        print(f"\nüîç Connection Matrix Comparison (for completed models):")
        # ... (stats comparison prints)

    print("üìç CHECKPOINT: Preparing to save experimental results.")
    experiment_results = {
        "experiment_type": "comprehensive_comparison_stable_2024",
        "task": "babi_task16_basic_induction", 
        "hardware_target": "RTX_4090_24GB", # Added target
        "actual_device": device,
        "data_source": data_source_type,
        "config": CONFIG,
        "results": results,
        "model_stats": model_stats,
        # ... (other result fields)
        "timestamp": time.strftime("%Y%m%d_%H%M%S")
    }
    
    results_filename = f"stable_comparison_2024_{experiment_results['timestamp']}.json"
    try:
        with open(results_filename, "w") as f:
            json.dump(experiment_results, f, indent=2, cls=NpEncoder) # Added NpEncoder for numpy types
        print(f"  üìÑ Results successfully saved to: {results_filename}")
    except Exception as e_json:
        print(f"‚ö†Ô∏è Error saving JSON results: {e_json}")
    print("üìç CHECKPOINT: Finished saving experimental results.")
    
    # ... (Final conclusions, Future research)
    print("üìç CHECKPOINT: Printing final conclusions and future work.")
    
    if results:
        # ... (summary prints)
        pass # Already very verbose
    
    print(f"\n‚ú® Experiment completed!") # Simplified message
    print(f"   All models (attempted) trained with numerical stability measures.")
    print(f"   Data loading system validated (used {data_source_type}).")
    print(f"   Results and analysis (if successful) saved for future reference.")
    print(f"   Safety mechanisms were active.")
    print("üìç CHECKPOINT: main() function finished.")
    return results

# Custom JSON encoder to handle NumPy types if they appear
class NpEncoder(json.JSONEncoder):
    def default(self, obj):
        if isinstance(obj, np.integer):
            return int(obj)
        if isinstance(obj, np.floating):
            return float(obj)
        if isinstance(obj, np.ndarray):
            return obj.tolist()
        if isinstance(obj, torch.Tensor): # Handle torch tensors if they sneak in
            return obj.cpu().numpy().tolist()
        return super(NpEncoder, self).default(obj)

if __name__ == "__main__":
    print("üìç CHECKPOINT: Script execution started (__name__ == '__main__').")
    print("üîß Environment Check:")
    # ... (env check prints)
    
    warnings.filterwarnings("ignore", category=UserWarning, module="torch.nn.modules.lazy")
    warnings.filterwarnings("ignore", category=UserWarning, message="TypedStorage is deprecated")


    try:
        final_results = main()
        print(f"\nüéâ All experiments sequence completed!")
        if final_results:
             print(f"Final Results Summary: {final_results}")
             if all(acc is not None and acc > 0.01 for acc in final_results.values()): # check for some success
                 print(f"‚úÖ All reported models achieved some minimal performance.")
             else:
                 print(f"‚ö†Ô∏è Some models may have had training issues or yielded low/no accuracy.")
        else:
            print(f"‚ö†Ô∏è No results returned from main experiment function.")
        
    except KeyboardInterrupt:
        print(f"\nüõë Experiment interrupted by user.")
    except Exception as e:
        print(f"\n‚ùå CRITICAL EXPERIMENT FAILURE: {e}")
        import traceback
        traceback.print_exc()
        # ... (debugging tips)
    print("üìç CHECKPOINT: Script execution finished.")